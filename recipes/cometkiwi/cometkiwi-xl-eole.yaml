#override_opts: true
seed: 1234
share_vocab: true
#overwrite: true
save_data: "/media/vincent/Crucial X6/NMT_work/en-de/runs/36-0-32-2560-10240/"
src_vocab: "/mnt/InternalCrucial4/LLM_work/xlm-roberta-xl-eole/dict2.txt"
src_vocab_size: 250880
tgt_vocab_size: 250880
vocab_size_multiple: 1
report_every: 50
skip_empty_level: silent
default_specials: ['<unk>', '<blank>', '<s>', '</s>']
data_task: "encoder"

# transforms config
transforms: [sentencepiece, filtertoolong]
transforms_configs:
  sentencepiece:
    #### Subword
    src_subword_model: "/mnt/InternalCrucial4/LLM_work/xlm-roberta-xl-eole/sentencepiece.bpe.model"
    tgt_subword_model: "/mnt/InternalCrucial4/LLM_work/xlm-roberta-xl-eole/sentencepiece.bpe.model"

  filtertoolong:
    src_seq_length: 94
    tgt_seq_length: 94


# Corpus opts:
data:
    1720-da-mlqe:
        path_src: "/mnt/InternalCrucial4/data/en-de/1720-da-mlqe/1720-da.mlqe.src"
        path_tgt: "/mnt/InternalCrucial4/data/en-de/1720-da-mlqe/1720-da.mlqe.tgt"
        path_sco: "/mnt/InternalCrucial4/data/en-de/1720-da-mlqe/1720-da.mlqe.sco"
        transforms: [sentencepiece, filtertoolong]
        weight: 1

    valid:
        path_src: "/mnt/InternalCrucial4/data/en-de/testsets/newstest2014-origlang-en.en"
        path_tgt: "/mnt/InternalCrucial4/data/en-de/testsets/newstest2014-origlang-en.de"
        transforms: [sentencepiece]

log_file: "/media/vincent/Crucial X6/NMT_work/en-de/runs/36-0-32-2560-10240-with-estim/36-0-32-2560-10240-with-estim.log"

training:
    # General opts
    #torch_compile: true
    self_attn_backend: "pytorch"
    #update_vocab: true
    
    #train_from: "/mnt/InternalCrucial4/LLM_work/xlm-roberta-xl-eole/"
    train_from: "/media/vincent/Crucial X6/NMT_work/en-de/runs/36-0-32-2560-10240-with-estim2/step_10000_pretrain"
    reset_optim: all #keep_states
    model_path: "/media/vincent/Crucial X6/NMT_work/en-de/runs/36-0-32-2560-10240-with-estim2"
    keep_checkpoint: 50
    save_checkpoint_steps: 10000
    average_decay: 0.0
    train_steps: 40000
    valid_steps: 1000

    # Batching
    bucket_size: 262144
    num_workers: 2
    prefetch_factor: 400
    world_size: 1
    gpu_ranks: [0]
    batch_type: "sents"
    batch_size: 16
    valid_batch_size: 1
    batch_size_multiple: 1
    accum_count: [4]
    accum_steps: [0]

    # Optimization
    model_dtype: "fp16"
    #apex_opt_level: "O2"
    optim: "fusedadam"
    learning_rate: 0.000015
    decay_method: "none"
    start_decay_steps: 1000000
    decay_steps: 100000
    learning_rate_decay: 1
    adam_beta2: 0.998
    max_grad_norm: 1
    label_smoothing: 0.1
    param_init: 0
    param_init_glorot: true
    normalization: "tokens"

    dropout_steps: [0]
    dropout: [0.1]
    attention_dropout: [0.0]
    estim_loss_lambda_steps: [0]
    estim_loss_lambda: [1.0]
    score_threshold: 0.0 #0.68
    freeze_encoder: false
    # LoRa
    lora_layers: ['linear_values', 'linear_query', 'linear_keys', 'final_linear', 'gate_up_proj', 'down_proj', '0', '3', '6']
    lora_rank: 2
    lora_dropout: 0.05
    lora_alpha: 8
    lora_embedding: false

model:
    architecture: "transformer_encoder"
    layers: 36
    heads: 32
    hidden_size: 2560
    transformer_ff: 10240
    add_qkvbias: true
    add_ffnbias: true
    mlp_activation_fn: gelu
    #parallel_residual: true
    #shared_layer_norm: true
    add_estimator: true
    share_decoder_embeddings: true
    share_embeddings: true
    max_relative_positions: 0
    rotary_interleave: false
    layer_norm: standard
    norm_eps: 1e-5
    embeddings:
        freeze_word_vecs_enc: true
        word_vec_size: 2560
        position_encoding: true
        position_encoding_type: Learned
        n_positions: 514
        position_shift: 2
