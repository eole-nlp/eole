transforms: [onmt_tokenize]
transforms_configs:
  onmt_tokenize:
    src_subword_type: bpe
    src_subword_model: ${EOLE_MODEL_DIR}/openai_gpt2/bpe.model
    gpt2_pretok: true

world_size: 1
gpu_ranks: [0]
gpu: 0

model_path: ${EOLE_MODEL_DIR}/openai_gpt2

src: lm_input.txt
output: lm_pred.txt
beam_size: 5
# random_sampling_topp: 0.5
random_sampling_temp: 1.0
random_sampling_topk: 50
random_sampling_topp: 1
n_best: 5

seed: 42
self_attn_backend: "pytorch"
precision: fp32

max_length: 30

verbose: true