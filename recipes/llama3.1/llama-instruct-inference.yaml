transforms: [onmt_tokenize]

transforms_configs:
  onmt_tokenize:
    src_subword_type: bpe
    src_subword_model: "${EOLE_MODEL_DIR}/llama3.1-8b-instruct/bpe.model"
    src_onmttok_kwargs: {"mode": "space", "spacer_annotate": True, "preserve_placeholders": True}
    tgt_subword_type: bpe
    tgt_subword_model: "${EOLE_MODEL_DIR}/llama3.1-8b-instruct/bpe.model"
    tgt_onmttok_kwargs: {"mode": "space", "spacer_annotate": True, "preserve_placeholders": True}
    gpt2_pretok: true
    mapped_tokens: [['<|start_header_id|>', '｟start_header_id｠'], ['<|end_header_id|>', '｟end_header_id｠'], ['<|eot_id|>', '｟eot_id｠']]

optional_eos: ['<|eot_id|>']

# Model info
model_path: "${EOLE_MODEL_DIR}/llama3.1-8b-instruct"

# Inference
seed: 42
max_length: 256
# max_length: 1
# gpu: 0
batch_type: tokens
batch_size: 8192
#world_size: 1
#gpu_ranks: [0]
world_size: 2
gpu_ranks: [0, 1]
parallel_mode: "tensor_parallel"
#quant_layers: ['gate_up_proj', 'down_proj', 'up_proj', 'linear_values', 'linear_query', 'linear_keys', 'final_linear']
#quant_type: "bnb_NF4"
compute_dtype: fp16
#random_sampling_topk: 1
#random_sampling_topp: 0.0
#random_sampling_temp: 0.9
beam_size: 1
n_best: 1
report_time: true
src: None

