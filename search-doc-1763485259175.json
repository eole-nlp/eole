{"searchDocs":[{"title":"Command Line","type":0,"sectionRef":"#","url":"/eole/docs/concepts/command_line","content":"","keywords":"","version":"Next"},{"title":"Categories of Tools​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#categories-of-tools","content":" ","version":"Next","tagName":"h2"},{"title":"Main Entrypoints​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#main-entrypoints","content":" build_vocabtraintranslate  ","version":"Next","tagName":"h3"},{"title":"Model Conversion Tools​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#model-conversion-tools","content":" convert Flavors: xgen, redpajama, llama_legacy, falcon, mpt, HF, T5  ","version":"Next","tagName":"h3"},{"title":"Model Management Tools​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#model-management-tools","content":" model Subcommands: lora, release, extract_embeddings, average  ","version":"Next","tagName":"h3"},{"title":"Miscellaneous Tools (Mostly Legacy)​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#miscellaneous-tools-mostly-legacy","content":" tools Subcommands: LM_scoring, oracle_comet, run_mmlu, spm_to_vocab, mbr_bleu, embeddings_to_torch, oracle_bleu  ","version":"Next","tagName":"h3"},{"title":"Usage​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#usage","content":" The main entrypoints are typically used with a yaml configuration file. Most parameters can also be overridden via corresponding command line flags if needed.  ","version":"Next","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#examples","content":" eole build_vocab -c your_config.yaml eole train -c your_config.yaml eole predict -c your_config.yaml   ","version":"Next","tagName":"h3"},{"title":"Additional Tools​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#additional-tools","content":" All other tools have specific arguments that can be inspected via the command helpstring -h.  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#example","content":" eole tool_name -h  ","version":"Next","tagName":"h3"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/eole/docs/concepts/config","content":"Configuration One of the core principles around Eole is the structured configuration logic via Pydantic models. This allows for centralized validation of numerous parameters, as well as proper nesting of various objects and scopes. It can be a lot at first, but that's a necessary evil for proper structure and modularity. Here is an example configuration to train a GPT-style language model: # General data/vocab/run related settings seed: 42 save_data: test_save_data src_vocab_size: 60000 tgt_vocab_size: 60000 share_vocab: true src_vocab: my_vocab.txt report_every: 100 # report stats every 100 steps # datasets data: # training sets can be numerous, and named anything corpus_1: path_src: my_training_set.txt # single validation set, always named &quot;valid&quot; valid: path_src: my_validation_set.txt # default transforms, in application order transforms: [onmt_tokenize, filtertoolong] # transforms configuration transforms_configs: onmt_tokenize: src_subword_type: bpe src_subword_model: my_subwords_model.bpe src_onmttok_kwargs: {&quot;mode&quot;: &quot;aggressive&quot;, &quot;joiner_annotate&quot;: True, &quot;preserve_placeholders&quot;: True, &quot;case_markup&quot;: True, &quot;soft_case_regions&quot;: True, &quot;preserve_segmented_tokens&quot;: True} filtertoolong: src_seq_length: 512 tgt_seq_length: 512 # model architecture configuration model: architecture: &quot;transformer_lm&quot; layers: 6 heads: 8 hidden_size: 512 transformer_ff: 2048 embeddings: word_vec_size: 512 position_encoding: true # training routine configuration training: # Train on a single GPU world_size: 1 gpu_ranks: [0] # Batching batch_size: 2048 batch_type: tokens # Optimizer model_dtype: &quot;fp32&quot; optim: &quot;adam&quot; learning_rate: 2 warmup_steps: 8000 decay_method: &quot;noam&quot; adam_beta2: 0.998 # Hyperparams dropout_steps: [0] dropout: [0.1] attention_dropout: [0.1] max_grad_norm: 0 label_smoothing: 0.1 param_init: 0 param_init_glorot: true normalization: &quot;tokens&quot; # Where to save the checkpoints (creates a directory) model_path: my_model # Steps intervals save_checkpoint_steps: 10 train_steps: 50 valid_steps: 500 ","keywords":"","version":"Next"},{"title":"Dataset Weighting","type":0,"sectionRef":"#","url":"/eole/docs/concepts/weighting","content":"","keywords":"","version":"Next"},{"title":"Example​","type":1,"pageTitle":"Dataset Weighting","url":"/eole/docs/concepts/weighting#example","content":" In the following example, we will sequentially sample 7 examples from corpus_1, and 3 examples from corpus_2, and so on:  # &lt;your_config&gt;.yaml ... # Corpus opts: data: corpus_1: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt weight: 7 corpus_2: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt weight: 3 valid: path_src: toy-ende/src-val.txt path_tgt: toy-ende/tgt-val.txt ...  ","version":"Next","tagName":"h3"},{"title":"EOLE","type":0,"sectionRef":"#","url":"/eole/docs/","content":"","keywords":"","version":"Next"},{"title":"HF Models supported​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#hf-models-supported","content":" deepseek-ai/DeepSeek-OCR For now takes any image and rescales to 1024x1024 before processing - Gundam mode not implemented yet)tencent/Hunyuan-MT-7B SOTA NMT at WMT25, better than Towerplus-9B and EuroLLM-9BQwen/Qwen2/3 Non VL family. Includes Qwen3-30B-A3Bgoogle/gemma-3-27b-it All Gemma3 family - supports text and image inputMistral-3.1-24B-instruct supports all Mistral AI models (text and image input) - includes Ministral, Mixtral, Mathstralmeta-llama/Llama-3.X modelsmicrosoft/Phi-2/3 models  Of course you can train your own architecture (Decoder only, Encoder Only, or EncoderDecoder Model)  ","version":"Next","tagName":"h2"},{"title":"Latest developments​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#latest-developments","content":" high inference speed using Flash Attention and Vllm RMSNorm kernel.prefixLM + split prompt/answer in src/tgt optional method to feed your dataPure-BF16 Training thanks to Kahan Summation implemented hereWeb-based (Google translator-like) interface featuring the latest Hunyuan-MT-7B or EuroLLM-8B-Instruct LLMEstimator layer which enables to rescore multiple beams in the same model. Read article here and hereSupport Hugging Face Tokenizers for better compatiblityReplicate CometKiwi(XL/XXL) Encoder+Estimator models    ","version":"Next","tagName":"h2"},{"title":"Key Features​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#key-features","content":" Versatile Training and Inference: Train from scratch, finetune, and infer models of various architectures including Transformer Encoder/Decoder/EncoderDecoder and RNN EncoderDecoder.Dynamic Data Transforms: Apply on-the-fly transformations in the dataloading logic for both training and inference.Comprehensive LLM Support: Includes converters for Llama, Mistral, Phi, Gemma ...Advanced Quantization: Support for 8-bit and 4-bit quantization, along with LoRA adapters, with or without checkpointing, as well as mixed precision (FP16).Efficient Finetuning: Finetune 7B and 13B models on a single RTX 24GB GPU using 4-bit quantization.Flexible Inference: Perform inference in 4-bit or 8-bit using the same layer quantization methods as in finetuning.Tensor Parallelism: Enable tensor parallelism for both training and inference when models exceed the memory capacity of a single GPU.  ","version":"Next","tagName":"h2"},{"title":"Work completed​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#work-completed","content":" We have made significant progress in several areas:  Configuration Management: Streamlined through pydantic models.Command Line Entry Points: Improved using structured subparsers for better organization.Reproducible Recipes: Provided for widely used models and tasks, ensuring consistency and reliability.Core API Simplification: Refined around the new configuration objects for ease of use.Revamped Fast API based server: see above example with EuroLLM-9B-Instruct    ","version":"Next","tagName":"h2"},{"title":"Future Directions​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#future-directions","content":" There are still several exciting avenues to explore:  Further Simplification and Refactoring: Continue enhancing the codebase for clarity and efficiency.Documentation: Enhance and expand the documentation for better user guidance.Test Coverage: Improve testing to ensure code reliability and performance.Logging Enhancements: Implement more sophisticated logging mechanisms.Broader Model Support: Extend support to include a wider range of open models, potentially multi-modal.  ","version":"Next","tagName":"h3"},{"title":"Setup​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#setup","content":" ","version":"Next","tagName":"h2"},{"title":"Using Docker​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#using-docker","content":" To facilitate setup and reproducibility, we provide Docker images via the GitHub Container Registry: EOLE Docker Images.  You can customize the workflow and build your own images based on specific needs using build.sh and Dockerfile in the docker directory of the repository.  To pull the Docker image:  docker pull ghcr.io/eole-nlp/eole:0.4.0-torch2.9.1-ubuntu22.04-cuda12.8   Example one-liner to run a container and open a bash shell within it:  docker run --rm -it --runtime=nvidia ghcr.io/eole-nlp/eole:0.4.0-torch2.9.1-ubuntu22.04-cuda12.8   Note: Ensure you have the Nvidia Container Toolkit (formerly nvidia-docker) installed to take advantage of CUDA/GPU features.  Depending on your needs, you can add various flags:  -p 5000:5000: Forward an exposed port from your container to your host.-v /some/local/directory:/some/container/directory: Mount a local directory to a container directory.--entrypoint some_command: Run a specific command as the container entry point (instead of the default bash shell).  ","version":"Next","tagName":"h3"},{"title":"Installing Locally​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#installing-locally","content":" Requirements​  Python &gt;= 3.10PyTorch &gt;= 2.8 &lt; 2.10  Installation from Source​  To install from source:  git clone https://github.com/eole-nlp/eole cd eole pip install -e .   Installation from PyPI​  Installation from PyPI will be available soon.  Notes​  If you encounter a MemoryError during installation, try using pip with the --no-cache-dir option.  (Optional) Some advanced features (e.g., pretrained models or specific transforms) require extra packages. Install them with:  pip install -r requirements.opt.txt   ","version":"Next","tagName":"h3"},{"title":"Manual Installation of Some Dependencies​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#manual-installation-of-some-dependencies","content":" Flash Attention​  To use Flash Attention, install it manually:  pip install flash-attn --no-build-isolation   AWQ​  For inference or quantizing an AWQ model, AutoAWQ is required. Install it with:  pip install autoawq   For more details, refer to AutoAWQ.  ","version":"Next","tagName":"h3"},{"title":"Notes on Mixed-precision or Low precision Training​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#notes-on-mixed-precision-or-low-precision-training","content":" Until Feb 25, we used torch optimizers with or without AMP (mixed precision) or &quot;fusedadam&quot; which was an old implementation of Apex/Nvidia using FP16 with dynamic loss scaling and without FP32 master weights. As of 0.2 &quot;fusedadam&quot; is deprecated and we implemented pure-BF16 training.  As a result, config flags are now:  For FP16-amp or BF16-amp training (using pytorch optimizers and amp implementation)  compute_dtype: fp16 or bf16 use_amp: true optim: adam or adamw   Special note: even though it may not be logical, we still use the torch GradScaler in BF16-AMP. Even if the BF16 range is similar to FP32, scaling prevents from underflowing. We tested BF16-AMP without the GradScaler and it does not give good results.  For pure-bf16 training (using torch-optimi and kahan summation)  compute_dtype: bf16 use_amp: false optim: adam or adamw   Pure-BF16 training is faster than AMP and the memory footprint is reduced (master weights are kept in BF16 vs FP32). However Kahan Summation is not magical, results are good but not as good as AMP. Use this feature mainly when memory footprint is an issue with LLMs.    ","version":"Next","tagName":"h2"},{"title":"Contributing​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#contributing","content":" We love contributions! Please look at issues marked with the contributions welcome tag.  Before raising an issue, make sure you read the requirements and the Full Documentation. You can also check if a Recipe fits your use case.  Unless there is a bug, please use the Discussions tab to ask questions or propose new topics/features. ","version":"Next","tagName":"h2"},{"title":"Contributors","type":0,"sectionRef":"#","url":"/eole/docs/contributing","content":"","keywords":"","version":"Next"},{"title":"Guidelines​","type":1,"pageTitle":"Contributors","url":"/eole/docs/contributing#guidelines","content":" Before sending a PR, please do this checklist first:  Please run eole/tests/pull_request_check.sh and fix any errors. When adding new functionality, also add tests to this script. Included checks: black and flake8 check for coding style;unittest;continuous integration tests listed in .github/workflows/push.yml. When adding/modifying class constructor, please make the arguments as same naming style as its superclass in PyTorch.If your change is based on a paper, please include a clear comment and reference in the code (more on that below).  ","version":"Next","tagName":"h2"},{"title":"Docstrings​","type":1,"pageTitle":"Contributors","url":"/eole/docs/contributing#docstrings","content":" Above all, try to follow the Google docstring format (Napoleon example,Google styleguide). This makes it easy to include your contributions in the Sphinx documentation. And, do feel free to autodoc your contributions in the API .rst files in the docs/source folder! If you do, check that your additions look right.  cd docs # install some dependencies if necessary: pip install -r requirements.txt make html firefox build/html/main.html # or your browser of choice   Some particular advice:  Try to follow Python 3 typing module conventions when documenting types. Exception: use &quot;or&quot; instead of unions for more readabilityFor external types, use the full &quot;import name&quot;. Common abbreviations (e.g. np) are acceptable. For torch.Tensor types, the torch. is optional.Please don't use tics like (`str`) or rst directives like (:obj:`str`). Napoleon handles types very well without additional help, so avoid the clutter. Google docstrings don't support multiple returns. For multiple returns, the following works well with Sphinx and is still very readable. def foo(a, b): &quot;&quot;&quot;This is my docstring. Args: a (object): Something. b (class): Another thing. Returns: (object, class): * a: Something or rather with a long description that spills over. * b: And another thing. &quot;&quot;&quot; return a, b When citing a paper, avoid directly linking in the docstring! Add a Bibtex entry to docs/source/refs.bib. E.g., to cite &quot;Attention Is All You Need&quot;, visit arXiv, choose thebibtext link, search docs/source/refs.bibusing CTRL-F for DBLP:journals/corr/VaswaniSPUJGKP17, and if you do not find it then copy-paste the citation into refs.bib. Then, in your docstring, use :cite:`DBLP:journals/corr/VaswaniSPUJGKP17` . However, a link is better than nothing. Please document tensor shapes. Prefer the format``(a, b, c)``. This style is easy to read, allows using x for multplication, and is common (PyTorch uses a few variations on the parentheses format, AllenNLP uses exactly this format, Fairseq uses the parentheses format with single ticks). Again, a different style is better than no shape documentation. Please avoid unnecessary space characters, try to capitalize, and try to punctuate. For multi-line docstrings, add a blank line after the closing &quot;&quot;&quot;. Don't use a blank line before the closing quotes. &quot;&quot;&quot; not this &quot;&quot;&quot; &quot;&quot;&quot;This.&quot;&quot;&quot; &quot;&quot;&quot; Not this. &quot;&quot;&quot; &quot;&quot;&quot;This.&quot;&quot;&quot; This note is the least important. Focus on content first, but remember that consistent docs look good. Be sensible about the first line. Generally, one stand-alone summary line (per the Google guidelines) is good. Sometimes, it's better to cut directly to the args or an extended description. It's always acceptable to have a &quot;trailing&quot; citation. ","version":"Next","tagName":"h3"},{"title":"How can I ensemble Models at inference?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/ensemble_decoding","content":"How can I ensemble Models at inference? You can specify several models in the onmt_translate command line: -model model1_seed1 model2_seed2Bear in mind that your models must share the same target vocabulary.","keywords":"","version":"Next"},{"title":"Do you support multi-gpu?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/distributed","content":"Do you support multi-gpu? First you need to make sure you export CUDA_VISIBLE_DEVICES=0,1,2,3. If you want to use GPU id 1 and 3 of your OS, you will need to export CUDA_VISIBLE_DEVICES=1,3 Both -world_size and -gpu_ranks need to be set. E.g. -world_size 4 -gpu_ranks 0 1 2 3 will use 4 GPU on this node only. Warning - Deprecated Multi-node distributed training has not been properly re-implemented since OpenNMT-py 2.0. If you want to use 2 nodes with 2 GPU each, you need to set -master_ip and -master_port, and -world_size 4 -gpu_ranks 0 1: on the first node-world_size 4 -gpu_ranks 2 3: on the second node-accum_count 2: This will accumulate over 2 batches before updating parameters. If you use a regular network card (1 Gbps) then we suggest to use a higher -accum_count to minimize the inter-node communication. Note: In the legacy version, when training on several GPUs, you couldn't have them in 'Exclusive' compute mode (nvidia-smi -c 3). The multi-gpu setup relied on a Producer/Consumer setup. This setup means there will be 2&lt;n_gpu&gt; + 1 processes spawned, with 2 processes per GPU, one for model training and one (Consumer) that hosts a Queue of batches that will be processed next. The additional process is the Producer, creating batches and sending them to the Consumers. This setup is beneficial for both wall time and memory, since it loads data shards 'in advance', and does not require to load it for each GPU process. The new codebase allows GPUs to be in exclusive mode, because batches are moved to the device later in the process. Hence, there is no 'producer' process on each GPU.","keywords":"","version":"Next"},{"title":"How to use gradient checkpointing when dealing with a big model ?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/gradient_checkpointing","content":"How to use gradient checkpointing when dealing with a big model ? use_ckpting: [&quot;ffn&quot;, &quot;mha&quot;, &quot;lora&quot;] Be carefull, the module that you use checkpointing needs to have gradients.","keywords":"","version":"Next"},{"title":"How to use LoRa and 8bit loading to finetune a big model ?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/lora","content":"How to use LoRa and 8bit loading to finetune a big model ? Cf paper: LoRa LoRa is a mechanism that helps to finetune bigger model on a single GPU card by limiting the anmount of VRAM needed. The principle is to make only a few layers trainable (hence reducing the amount of required memory especially for the Adam optimizer) You need to train_from a model (for instance NLLB-200 3.3B) and use the following options: lora_layers: ['linear_values', 'linear_query'] these are the two layers of the Self-Attention module the paper recommend to make trainable.lora_rank: 2lora_dropout: 0.1 or any value you can testlora_alpha: 1 or any value you can testlora_embedding: true makes Embeddings LoRa compatible, hence trainable in the case you use update_vocab: true or if you want to finetune Embeddings as well. Bitsandbytes enables quantization of Linear layers. For more information: https://github.com/TimDettmers/bitsandbytesAlso you can read the blog post here: https://huggingface.co/blog/hf-bitsandbytes-integration You need to add the following option: quant_layers: ['up_proj', 'down_proj', 'linear_values', 'linear_query']quant_type: ['bnb_NF4'] You can for instane quantize the layers of the PositionWise Feed-Forward from the Encoder/Decoder and the key/query/values/final from the Multi-head attention. Choices for quantization are [&quot;bnb_8bit&quot;, &quot;bnb_FP4&quot;, &quot;bnb_NF4&quot;]","keywords":"","version":"Next"},{"title":"How to switch from OpenNMT-py to EOLE?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/opennmt","content":"","keywords":"","version":"Next"},{"title":"Configuration conversion​","type":1,"pageTitle":"How to switch from OpenNMT-py to EOLE?","url":"/eole/docs/FAQ/opennmt#configuration-conversion","content":" One of the main pillars of EOLE is the full revamping of the configuration structure and validation logic. That means OpenNMT-py configuration files are not supported by default. That being said, a conversion tool has been created to facilitate the transition: eole convert onmt_config  There are a few key things to know:  what was previous fully &quot;flat&quot; in OpenNMT-py configurations is now mostly nested in nested sections with specific scope such as training, model, transforms_configs;some parameters were renamed, removed, or replaced by other logics, which makes the conversion script not 100% exhaustive;the conversion script will log the remaining &quot;unmapped settings&quot;, to facilitate fixing the last issues manually.  ","version":"Next","tagName":"h2"},{"title":"Model conversion​","type":1,"pageTitle":"How to switch from OpenNMT-py to EOLE?","url":"/eole/docs/FAQ/opennmt#model-conversion","content":" Models trained with OpenNMT-py can technically be converted to be used with EOLE, but there is no automated tool for now. Feel free to get in touch via Issues or Discussions if that is a blocker. ","version":"Next","tagName":"h2"},{"title":"How can I create custom on-the-fly data transforms?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/custom_transforms","content":"How can I create custom on-the-fly data transforms? The code is easily extendable with custom transforms inheriting from the Transform base class. You can for instance have a look at the FilterTooLongTransform and the corresponding FilterTooLongConfig classes as a template: class FilterTooLongConfig(TransformConfig): src_seq_length: int | None = Field( default=192, description=&quot;Maximum source sequence length.&quot; ) tgt_seq_length: int | None = Field( default=192, description=&quot;Maximum target sequence length.&quot; ) ... @register_transform(name=&quot;filtertoolong&quot;) class FilterTooLongTransform(Transform): &quot;&quot;&quot;Filter out sentence that are too long.&quot;&quot;&quot; config_model = FilterTooLongConfig def __init__(self, config): super().__init__(config) def _parse_config(self): self.src_seq_length = self.config.src_seq_length self.tgt_seq_length = self.config.tgt_seq_length def apply(self, example, is_train=False, stats=None, **kwargs): &quot;&quot;&quot;Return None if too long else return as is.&quot;&quot;&quot; if len(example[&quot;src&quot;]) &gt; self.src_seq_length or ( example[&quot;tgt&quot;] is not None and len(example[&quot;tgt&quot;]) &gt; self.tgt_seq_length - 2 ): if stats is not None: stats.update(FilterTooLongStats()) return None else: return example def _repr_args(self): &quot;&quot;&quot;Return str represent key arguments for class.&quot;&quot;&quot; return &quot;{}={}, {}={}&quot;.format( &quot;src_seq_length&quot;, self.src_seq_length, &quot;tgt_seq_length&quot;, self.tgt_seq_length ) Methods: _parse_opts allows to parse options from the config_model;apply is where the transform happens;_repr_args is for clean logging purposes. As you can see, there is the @register_transform wrapper before the class definition. This will allow for the class to be automatically detected (if put in the proper transforms folder) and usable in your training configurations through its name argument. You could also collect statistics for your custom transform by creating a class inheriting ObservableStats: class FilterTooLongStats(ObservableStats): &quot;&quot;&quot;Runing statistics for FilterTooLongTransform.&quot;&quot;&quot; __slots__ = [&quot;filtered&quot;] def __init__(self): self.filtered = 1 def update(self, other: &quot;FilterTooLongStats&quot;): self.filtered += other.filtered NOTE: Add elements to keep track in the __init__ and also __slot__ to make it lightweight;Supply update logic in update method;(Optional) override __str__ to change default log message format;Instantiate and passing the statistic object in the apply method of the corresponding transform class;statistics will be gathered per corpus per worker, but only first worker will report for its shard by default. The example argument of apply is a dict of the form: { &quot;src&quot;: &lt;source string&gt;, &quot;tgt&quot;: &lt;target string&gt;, &quot;align&quot;: &lt;alignment pharaoh string&gt; # optional } This is defined in eole.inputters.text_corpus.ParallelCorpus.load. This class is not easily extendable for now but it can be considered for future developments. For instance, we could create some CustomParallelCorpus class that would handle other kind of inputs.","keywords":"","version":"Next"},{"title":"Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/position_encoding","content":"Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi The basic feature is absolute position encoding stemming from the original Transformer Paper. However, even with this, we can use SinusoidalInterleaved (default OpenNMT-py) or SinusoidalConcat (default Fairseq imported models) position_encoding_type: 'SinusoidalInterleaved'Do not forget to set also param_init_glorot: true If you prefer to use relative position encoding, we support 3 modes: &quot;Shaw&quot;: https://arxiv.org/abs/1803.02155 - you need to set position_encoding_type: 'Relative' and n_positions: N where N &gt; 1 (use 16, 20, 32) see paper.&quot;Rope&quot; Rotary Embeddings: https://arxiv.org/abs/2104.09864 - you need to set position_encoding_type: 'Rotary'&quot;Alibi&quot; (used by MPT-7B for example) https://arxiv.org/abs/2108.12409 - you need to set position_encoding_type: 'Alibi' In a nutshell, at the time if this writing (v3.1) absolute position encoding is managed in the Embeddings module, whereas the relative position encoding is managed directly in the multi-head self-attention module.","keywords":"","version":"Next"},{"title":"Data Transforms","type":0,"sectionRef":"#","url":"/eole/docs/concepts/transforms","content":"","keywords":"","version":"Next"},{"title":"Transform Types​","type":1,"pageTitle":"Data Transforms","url":"/eole/docs/concepts/transforms#transform-types","content":" The concept of TransformType was introduced to facilitate transparent configuration management. The underlying issue at stake is that all transforms are not meant to be used in the same concept. For instance, the filtertoolong transform is meant as a &quot;safeguard&quot; to limit the size of training batches. Enabling this transform when predicting can introduce some unwanted behaviours and poor results. For now, the possible transform types are:  Default // &quot;any&quot;: usable in any context (default unless specified otherwise in the transform class definition);Train // &quot;train&quot;: usable only in training context;Predict // &quot;predict&quot;: usable only in prediction context.  This concept might be extended later for various needs, such as different data types, etc.  ","version":"Next","tagName":"h3"},{"title":"General purpose​","type":1,"pageTitle":"Data Transforms","url":"/eole/docs/concepts/transforms#general-purpose","content":" Filter examples by length​  Transform name: filtertoolong  Class: eole.transforms.misc.FilterTooLongTransform  The following options can be added to the configuration :  src_seq_length: maximum source sequence length;tgt_seq_length: maximum target sequence length.  Add custom prefix to examples​  Transform name: prefix  Class: eole.transforms.misc.PrefixTransform  For each dataset that the prefix transform is applied to, you can set the additional src_prefix and tgt_prefix parameters in its data configuration:  data: corpus_1: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt transforms: [prefix] weight: 1 src_prefix: __some_src_prefix__ tgt_prefix: __some_tgt_prefix__   At inference if you want to use the target prefix feature to prefix your target segment with a unique prefix (as opposed to a target prefix coming from a line-by-line file) you need to set your yaml file as follow (example given with a target language as in the NLLB-200 case):  tgt_prefix: &quot;spa_Latn&quot; tgt_file_prefix: true   Add custom suffix to examples​  Transform name: suffix  Class: eole.transforms.misc.SuffixTransform  For each dataset that the suffix transform is applied to, you can set the additional src_suffix and tgt_suffix parameters in its data configuration:  data: corpus_1: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt transforms: [suffix] weight: 1 src_suffix: __some_src_suffix__ tgt_suffix: __some_tgt_suffix__   Convert examples to uppercase​  Transform name: uppercase  Class: eole.transforms.uppercase.UpperCaseTransform  Converts source and target (if present) examples to uppercase so the model can learn better to translate sentences in all caps. This transform normalizes the examples so the uppercased strings are stripped from any diacritics and accents. Usually this is desirable for most languages, although there are few exceptions.  The following option can be added to the main configuration (same ratio for all dataset with this transform):  upper_corpus_ratio: ratio of the corpus that will be transformed to uppercase (default: 0.01);  Normalize punctuation​  Transform name: normalize  Class: eole.transforms.normalize.NormalizeTransform  Normalizes source and target (if present) examples using the same rules as Moses punctuation normalizer.  The following options can be added to the configuration of each dataset:  src_lang: en, de, cz/cs, fr (default='')tgt_lang: en, de, cz/cs, fr (default='')penn: Penn substitution (default=True)norm_quote_commas: Normalize quotations and commas (default=True)norm_numbers: Normalize numbers (default=True)pre_replace_unicode_punct: Replace unicode punct (default=False)post_remove_control_chars: Remove control chars (default=False)  Clean dataset​  Transform name: clean  Class: eole.transforms.clean.CleanTransform  Cleans source and target (if present) examples using a set of rules.  The following options can be added to the configuration of each dataset:  src_eq_tgt: Remove example when source=target (default=True)same_char: Remove example if the same char is repeated 4 times (default=True)same_word: Remove example if the same word is repeated 3 times (default=True)script_ok: Remove example which contains chars that do not belong to these scripts (default=['Latin', 'Common'])script_nok: Remove example which contains chars that belong to these scripts (default=[])src_tgt_ratio: Remove example for which src/tgt ration is &lt;1/ratio or &gt;ratio (default=2)avg_tok_min: Remove example for which the average token length is &lt; X (default=3)avg_tok_max: Remove example for which the average token length is &gt; X (default=20)lang_id: Remove example for which detected language is not in [] (default=['en', 'fr'])  Context / Doc aware transform​  Transform name: docify  Class: eole.transforms.docify.DocifyTransform  Concatenates several segments into one, separated with a delimiter.  Pre-requisite:  Dataset must be &quot;Docs&quot; separated by an empty line which will make clear a story ends at this empty line.  The following options can be added to the main configuration (same options for all dataset with this transform):  doc_length: max token to be concatenated (default=200)max_context: number of delimiter (default=1 , ie 2 segments concatenated)  When working with several workers, this require some precaution in order to make sure &quot;doc&quot; are read linearly.  max_context + 1 needs to be a multiple of stride = Number of gpu x num_workers  Example: max_context=1 and 1 GPU, then num_workers must be 2 or 4.  Augment source segments with fuzzy matches for Neural Fuzzy Repair​  Transform name: fuzzymatch  Class: eole.transforms.fuzzymatch.FuzzyMatchTransform  Augments source segments with fuzzy matches for Neural Fuzzy Repair, as described in Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation. Currently, the transform augments source segments with only a single fuzzy match. The Translation Memory (TM) format should be a flat text file, with each line containing the source and the target segment separated by a delimiter. As fuzzy matching during training is computational intensive, we offer some advice to achieve good performance and minimize overhead:  Depending on your system's specs, you may have to experiment with the options bucket_size, bucket_size_init, and bucket_size_increment;You should increase the num_workers and prefetch_factor so your GPU does not have to wait for the batches to be augmented with fuzzy matches;Try to use a sensible Translation Memory size. 200k-250k translation units should be enough for yielding a sufficient number of matches;Although the transform performs some basic filtering both in the TM and in the corpus for very short or very long segments, some examples may still be long enough, so you should increase a bit the src_seq_length;Currently, when using n_sample, examples are always processed one by one and not in batches.  The following options can be added to the main configuration (valid for all datasets using this transform):  tm_path: The path to the Translation Memory text file;fuzzy_corpus_ratio: Ratio of corpus to augment with fuzzy matches (default: 0.1);fuzzy_threshold: The fuzzy matching threshold (default: 70);tm_delimiter: The delimiter used in the flat text TM (default: &quot;\\t&quot;);fuzzy_token: The fuzzy token to be added with the matches (default: &quot;｟fuzzy｠&quot;);fuzzymatch_min_length: Min length for TM entries and examples to match (default: 4);fuzzymatch_max_length: Max length for TM entries and examples to match (default: 70).  Augment source and target segments with inline tags​  Transform name: inlinetags  Class: eole.transforms.inlinetags.InlineTagsTransform  Augments source and target segments with inline tags (placeholders). The transform adds 2 kind of tags, paired tags (an opening and a closing tag) and isolated (standalone) tags, and requires a tab-delimited dictionary text file with source and target terms and phrases. A dictionary with 20-30k entries is recommended. User-defined tags must include the number placeholder #, e.g. &quot;｟user_start_tag_#｠&quot;.  The following options can be added to the main configuration (valid for all datasets using this transform):  tags_dictionary_path: The path to the dictionary text file;tags_corpus_ratio: Ratio of corpus to augment with inline tags (default: 0.1);max_tags: Maximum number of tags that can be added to a single sentence. (default: 12);paired_stag: The format of an opening paired inline tag. Must include the character # (default: &quot;｟ph_#_beg｠&quot;);paired_etag: The format of a closing paired inline tag. Must include the character # (default: &quot;｟ph_#_end｠&quot;);isolated_tag: The format of an isolated inline tag. Must include the character # (default: &quot;｟ph_#_std｠&quot;);src_delimiter: Any special token used for augmented src sentences (default: &quot;｟fuzzy｠&quot;);  Make the model learn to use terminology​  Transform name: terminology  Class: eole.transforms.terminology.TerminologyTransform  Augments source segments with terms so the model can learn to use user-provided terms at inference. It requires a dictionary with source and target terms, delimited with a tab. The transform uses Spacy's lemmatization facilities in order to a) solve the word inflection problem when searching for terms in any form, and b) make the model inflect correctly most target terms at inference. The lemmatization is applied at the dictionary entries and also at the source and target examples, and the term searches during training are performed on the lemmatized examples. The format of a processed segment augmented with terms is as follows:This is an ｟src_term_start｠ augmented ｟tgt_term_start｠ target_lemma_for_augmented ｟tgt_term_end｠ example.The following options can be added to the main configuration (valid for all datasets using this transform):  termbase_path: The path to the dictionary text file;src_spacy_language_model: Name of the spacy language model for the source corpus;tgt_spacy_language_model: Name of the spacy language model for the target corpus;term_corpus_ratio: Ratio of corpus to augment with terms # (default: 0.3);term_example_ratio: Max terms allowed in an example # (default: 0.2);src_term_stoken: The source term start token # (default: &quot;｟src_term_start｠&quot;);tgt_term_stoken: The target term start token # (default: &quot;｟tgt_term_start｠&quot;);tgt_term_etoken: The target term end token # (default: &quot;｟tgt_term_end｠&quot;);term_source_delimiter: Any special token used for augmented src sentences. The default is the fuzzy token used in the FuzzyMatch transform # (default: &quot;｟fuzzy｠&quot;);  ","version":"Next","tagName":"h3"},{"title":"Tokenization​","type":1,"pageTitle":"Data Transforms","url":"/eole/docs/concepts/transforms#tokenization","content":" Common options for the tokenization transforms are the following:  src_subword_model: path of source side (or both if shared) subword model;tgt_subword_model: path of target side subword model;src_subword_nbest: number of candidates for subword regularization (sentencepiece), source side;tgt_subword_nbest: number of candidates for subword regularization (sentencepiece), target_side;src_subword_alpha: smoothing parameter for sentencepiece regularization / dropout probability for BPE, source side;tgt_subword_alpha: smoothing parameter for sentencepiece regularization / dropout probability for BPE, target side.  OpenNMT Tokenizer​  Transform name: onmt_tokenize  Class: eole.transforms.tokenize.OnmtTokenizerTransform  Additional options are available:  src_subword_type: type of subword model for source side (from [&quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot;]);tgt_subword_type: type of subword model for target side (from [&quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot;]);src_onmttok_kwargs: additional kwargs for pyonmttok Tokenizer class, source side;tgt_onmttok_kwargs: additional kwargs for pyonmttok Tokenizer class, target side.  SentencePiece​  Transform name: sentencepiece  Class: eole.transforms.tokenize.SentencePieceTransform  The src_subword_model and tgt_subword_model should be valid sentencepiece models.  BPE subword-nmt​  Transform name: bpe  Class: eole.transforms.tokenize.BPETransform  The src_subword_model and tgt_subword_model should be valid BPE models.  ","version":"Next","tagName":"h3"},{"title":"BART-style noise​","type":1,"pageTitle":"Data Transforms","url":"/eole/docs/concepts/transforms#bart-style-noise","content":" BART-style noise is composed of several parts, as described in BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.  These different types of noise can be controlled with the following options:  permute_sent_ratio: proportion of sentences to permute (default boundaries are &quot;.&quot;, &quot;?&quot; and &quot;!&quot;);rotate_ratio: proportion of inputs to permute;insert_ratio: proportion of additional random tokens to insert;random_ratio: proportion of tokens to replace with random;mask_ratio: proportion of words/subwords to mask;mask_length: length of masking window (from [&quot;subword&quot;, &quot;word&quot;, &quot;span-poisson&quot;]);poisson_lambda: $\\lambda$ value for Poisson distribution to sample span length (in the case of mask_length set to span-poisson);replace_length: when masking N tokens, replace with 0, 1, &quot; &quot;or N tokens. (set to -1 for N).  ","version":"Next","tagName":"h3"},{"title":"SwitchOut and sampling​","type":1,"pageTitle":"Data Transforms","url":"/eole/docs/concepts/transforms#switchout-and-sampling","content":" SwitchOut​  Transform name: switchout  Class: eole.transforms.sampling.SwitchOutTransform  Options:  switchout_temperature: sampling temperature for SwitchOut.  Drop some tokens​  Transform name: tokendrop  Class: eole.transforms.sampling.TokenDropTransform  Options:  tokendrop_temperature: sampling temperature for token deletion.  Mask some tokens​  Transform name: tokenmask  Class: eole.transforms.sampling.TokenMaskTransform  Options:  tokenmask_temperature: sampling temperature for token masking. ","version":"Next","tagName":"h3"},{"title":"Compute dtype (precision) and storage dtype","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/precision","content":"","keywords":"","version":"Next"},{"title":"How to configure​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#how-to-configure","content":" It's important to note that compute precision does not necessarily reflect model parameters dtype. With this considered, compute precision can be configured by setting the compute_dtype field. From that, and other optimization settings (or specicic cases), the storage_dtype computed field is deduced. This is different from the specific quantization logic configured via quant_layers and quant_type. If such quantization is enabled, precision is still taken into account for non quantized components.  Note: the compute_dtype field can take both str and torch.dtype input types. An str input is validated to the corresponding torch.dtype via a custom mapping (see eole.config.common.RunningConfig.compute_dtype).  ","version":"Next","tagName":"h2"},{"title":"Available modes​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#available-modes","content":" ","version":"Next","tagName":"h2"},{"title":"Full precision​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#full-precision","content":" compute_dtype: {fp32, torch.float32}Standard float precision.  Note: flash attention is not compatible with float32 precision.  ","version":"Next","tagName":"h3"},{"title":"Half precision​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#half-precision","content":" compute_dtype: {fp16, torch.float16}  In most cases, the main model storage_dtype will be torch.float32, and some parameters will be automatically casted to torch.float16 with torch Automatic Mixed Precision.  Note: this means that checkpoints will be stored in torch.float32 in the amp case.  ","version":"Next","tagName":"h3"},{"title":"BFloat16​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#bfloat16","content":" compute_dtype: {bf16, torch.bfloat16}  See bfloat16 floating-point format for specificities.  When using the flag use_amp=True, behavior will be the same as above, ie torch AMP (mixed precision)  When using use_amp=False, we switch to torch-optimi which enables pure BF16 training using Kahan summation. see Optimi  ","version":"Next","tagName":"h3"},{"title":"Int8​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#int8","content":" compute_dtype: {int8, torch.int8}  This specific setting is only valid for CPU prediction, to enable Dynamic Quantization.  In that case, storage_dtype will initially be torch.float32, and the model will then be quantized to torch.qint8 with torch.quantization.quantize_dynamic. ","version":"Next","tagName":"h3"},{"title":"Performance tips","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/performance","content":"Performance tips use fp16use batch_size_multiple 8use vocab_size_multiple 8Depending on the number of GPU use num_workers 4 (for 1 GPU) or 2 (for multiple GPU)To avoid averaging checkpoints you can use the &quot;during training&quot; average decay system.for very fast inference convert your model to CTranslate2 format.","keywords":"","version":"Next"},{"title":"How do I use Pretrained embeddings (e.g. GloVe)?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/pretrained_embeddings","content":"","keywords":"","version":"Next"},{"title":"Example​","type":1,"pageTitle":"How do I use Pretrained embeddings (e.g. GloVe)?","url":"/eole/docs/FAQ/pretrained_embeddings#example","content":" Get GloVe files:  mkdir &quot;glove_dir&quot; wget http://nlp.stanford.edu/data/glove.6B.zip unzip glove.6B.zip -d &quot;glove_dir&quot;   Adapt the configuration:  # &lt;your_config&gt;.yaml &lt;Your data config...&gt; ... # this means embeddings will be used for both encoder and decoder sides both_embeddings: glove_dir/glove.6B.100d.txt # to set src and tgt embeddings separately: # src_embeddings: ... # tgt_embeddings: ... # supported types: GloVe, word2vec embeddings_type: &quot;GloVe&quot; # word_vec_size need to match with the pretrained embeddings dimensions word_vec_size: 100   Train:  eole train -config &lt;your_config&gt;.yaml   Notes:  the matched embeddings will be saved at &lt;save_data&gt;.enc_embeddings.pt and &lt;save_data&gt;.dec_embeddings.pt;additional flags freeze_word_vecs_enc and freeze_word_vecs_dec are available to freeze the embeddings. ","version":"Next","tagName":"h3"},{"title":"How can I update a checkpoint's vocabulary?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/update_vocab","content":"How can I update a checkpoint's vocabulary? New vocabulary can be used to continue training from a checkpoint. Existing vocabulary embeddings will be mapped to the new vocabulary, and new vocabulary tokens will be initialized as usual. Run eole build_vocab as usual with the new dataset. New vocabulary files will be created. Training options to perform vocabulary update are: -update_vocab: set this option-reset_optim: set the value to &quot;states&quot;-train_from: checkpoint path","keywords":"","version":"Next"},{"title":"How can I apply on-the-fly tokenization and subword regularization when training?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/tokenization","content":"","keywords":"","version":"Next"},{"title":"Example​","type":1,"pageTitle":"How can I apply on-the-fly tokenization and subword regularization when training?","url":"/eole/docs/FAQ/tokenization#example","content":" This example applies sentencepiece tokenization with pyonmttok, with nbest=20 and alpha=0.1.  # &lt;your_config&gt;.yaml ... transforms_configs: onmt_tokenize: # Tokenization options src_subword_type: sentencepiece src_subword_model: examples/subword.spm.model tgt_subword_type: sentencepiece tgt_subword_model: examples/subword.spm.model # Number of candidates for SentencePiece sampling subword_nbest: 20 # Smoothing parameter for SentencePiece sampling subword_alpha: 0.1 # Specific arguments for pyonmttok src_onmttok_kwargs: &quot;{'mode': 'none', 'spacer_annotate': True}&quot; tgt_onmttok_kwargs: &quot;{'mode': 'none', 'spacer_annotate': True}&quot; # transforms: [onmt_tokenize] # if you don't need to specify at dataset level # Corpus opts: data: corpus_1: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt transforms: [onmt_tokenize] weight: 1 valid: path_src: toy-ende/src-val.txt path_tgt: toy-ende/tgt-val.txt transforms: [onmt_tokenize] ...   Other tokenization methods and transforms are readily available. See the dedicated docs for more details. ","version":"Next","tagName":"h3"},{"title":"Can I get word alignments while translating?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/word_alignments","content":"","keywords":"","version":"Next"},{"title":"Raw alignments from averaging Transformer attention heads​","type":1,"pageTitle":"Can I get word alignments while translating?","url":"/eole/docs/FAQ/word_alignments#raw-alignments-from-averaging-transformer-attention-heads","content":" Currently, we support producing word alignment while translating for Transformer based models. Using -report_align when calling translate.py will output the inferred alignments in Pharaoh format. Those alignments are computed from an argmax on the average of the attention heads of the second to last decoder layer. The resulting alignment src-tgt (Pharaoh) will be pasted to the translation sentence, separated by |||. Note: The second to last default behaviour was empirically determined. It is not the same as the paper (they take the penultimate layer), probably because of slight differences in the architecture.  alignments use the standard &quot;Pharaoh format&quot;, where a pair i-j indicates the ith word of source language is aligned to jth word of target language.Example: {'src': 'das stimmt nicht !'; 'output': 'that is not true ! ||| 0-0 0-1 1-2 2-3 1-4 1-5 3-6'}Using -tgt and -gold_align options when calling translate.py, we output alignments between the source and the gold target rather than the inferred target, assuming we're doing evaluation.To convert subword alignments to word alignments, or symetrize bidirectional alignments, please refer to the lilt scripts.  ","version":"Next","tagName":"h3"},{"title":"Supervised learning on a specific head​","type":1,"pageTitle":"Can I get word alignments while translating?","url":"/eole/docs/FAQ/word_alignments#supervised-learning-on-a-specific-head","content":" The quality of output alignments can be further improved by providing reference alignments while training. This will invoke multi-task learning on translation and alignment. This is an implementation based on the paper Jointly Learning to Align and Translate with Transformer Models.  The data need to be preprocessed with the reference alignments in order to learn the supervised task. The reference alignment file(s) can for instance be generated by GIZA++ or fast_align.  In order to learn the supervised task, you can set for each dataset the path of its alignment file in the YAML configuration file:  &lt;your_config&gt;.yaml ... # Corpus opts: data: corpus_1: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt # src - tgt alignments in pharaoh format path_align: toy-ende/src-tgt.align transforms: [] weight: 1 valid: path_src: toy-ende/src-val.txt path_tgt: toy-ende/tgt-val.txt transforms: [] ...   Notes:  Most of the transforms are for now incompatible with the joint alignment learning pipeline, because most of them make modifications at the token level, hence alignments would be made invalid.There should be no blank lines in the alignment files provided.  Training options to learn such alignments are:  -lambda_align: set the value &gt; 0.0 to enable joint align training, the paper suggests 0.05;-alignment_layer: indicate the index of the decoder layer;-alignment_heads: number of alignment heads for the alignment task - should be set to 1 for the supervised task, and preferably kept to default (or same as num_heads) for the average task;-full_context_alignment: do full context decoder pass (no future mask) when computing alignments. This will slow down the training (~12% in terms of tok/s) but will be beneficial to generate better alignment. ","version":"Next","tagName":"h3"},{"title":"What special tokens are used?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/special_tokens","content":"","keywords":"","version":"Next"},{"title":"Special tokens actually used​","type":1,"pageTitle":"What special tokens are used?","url":"/eole/docs/FAQ/special_tokens#special-tokens-actually-used","content":" Depending on the context, these tokens can take various values:  Default behaviour, training from scratch  Some default values are defined as constants for the project:  class DefaultTokens(object): PAD = &quot;&lt;blank&gt;&quot; BOS = &quot;&lt;s&gt;&quot; EOS = &quot;&lt;/s&gt;&quot; UNK = &quot;&lt;unk&gt;&quot;   Retrieving a pretrained model from HF  The special tokens will be retrieved and configured from the special_tokens_map.json configuration file from the HF model files.  Custom behaviour  In any case, these tokens can be overriden via the ad-hoc configuration settings:  bos_tokenpad_tokeneos_tokenunk_token  ","version":"Next","tagName":"h2"},{"title":"Special tokens behaviour in Eole​","type":1,"pageTitle":"What special tokens are used?","url":"/eole/docs/FAQ/special_tokens#special-tokens-behaviour-in-eole","content":" When we train a SEQ2SEQ model we use: SRC: srctok1 srctok2 srctok3 .... srctokn TGT: BOS tgttok1 tgttok2 ..... tgttokm EOS But when training a LM SRC: BOS srctok1 srctok2 srctok3 .... srctokn TGT: srctok1 srctok2 srctok3 .... srctokn EOS  Having said that, sometimes we need to finetune models (eg: NLLB-200, Llama, ...) with existing vocab and special tokens are not the same.  ex with NLLB-200 BOS id=0 PAD id=1 EOS id=2 UNK id=3 And the decoder start token is EOS () which means in fact that the BOS is never used. At training, TGT needs to start with EOS instead of BOS in the default OpenNMT-py config.  Example of Llama UNK id=0 BOS id=1 EOS id=2 There was no PAD but to avoid conflicts we forced PAD id=3 (which was token '&lt;0x00&gt;' in the original llama tokenizer) ","version":"Next","tagName":"h2"},{"title":"Overview","type":0,"sectionRef":"#","url":"/eole/docs/main","content":"","keywords":"","version":"Next"},{"title":"Installation​","type":1,"pageTitle":"Overview","url":"/eole/docs/main#installation","content":" Install OpenNMT-py from pip:  pip install OpenNMT-py   or from the sources:  git clone https://github.com/OpenNMT/OpenNMT-py.git cd OpenNMT-py python setup.py install   (Optional) some advanced features (e.g. working pretrained models or specific transforms) requires extra packages, you can install it with:  pip install -r requirements.opt.txt   And you are ready to go!  Take a look at the quickstart to familiarize yourself with the main training workflow.  ","version":"Next","tagName":"h2"},{"title":"Citation​","type":1,"pageTitle":"Overview","url":"/eole/docs/main#citation","content":" When using OpenNMT-py for research please cite ourOpenNMT technical report  @inproceedings{opennmt, author = {Guillaume Klein and Yoon Kim and Yuntian Deng and Jean Senellart and Alexander M. Rush}, title = {OpenNMT: Open-Source Toolkit for Neural Machine Translation}, booktitle = {Proc. ACL}, year = {2017}, url = {https://doi.org/10.18653/v1/P17-4012}, doi = {10.18653/v1/P17-4012} }   ","version":"Next","tagName":"h2"},{"title":"Additional resources​","type":1,"pageTitle":"Overview","url":"/eole/docs/main#additional-resources","content":" You can find additional help or tutorials in the following resources:  Forum Gitter channel ","version":"Next","tagName":"h2"},{"title":"Recipes","type":0,"sectionRef":"#","url":"/eole/docs/recipes/","content":"","keywords":"","version":"Next"},{"title":"Available Recipes​","type":1,"pageTitle":"Recipes","url":"/eole/docs/recipes/#available-recipes","content":" Explore our collection of recipes designed to help you get started with various language models and tasks:  Llama2 Inference and Fine-tuning (Vicuna-style)Llama3 InferenceMMLU BenchmarkGPT-2 Style Language Model (WikiText-103)Transformer Translation Model (WMT17 en-de)  ","version":"Next","tagName":"h2"},{"title":"Contributing New Recipes​","type":1,"pageTitle":"Recipes","url":"/eole/docs/recipes/#contributing-new-recipes","content":" We welcome contributions to expand our library of recipes. Please follow these guidelines:  Respect Licenses: Ensure that you do not violate any licenses.Clear Documentation: Document your recipe thoroughly and ensure all scripts/commands are executable.Add Tests: Include tests if they are relevant to your recipe.Single Documentation File: Document your recipe in a single README.md file so it can be properly used in the documentation stack. ","version":"Next","tagName":"h2"},{"title":"fineweb10B","type":0,"sectionRef":"#","url":"/eole/docs/recipes/fineweb10B/","content":"fineweb10B This recipe is inspired from karpathy/llm.c#481 Disclaimer: it might not reproduce the results strictly, but should be a good starting point for whomever wanting to dive in such a topic.","keywords":"","version":"Next"},{"title":"CometKiwi","type":0,"sectionRef":"#","url":"/eole/docs/recipes/cometkiwi/","content":"","keywords":"","version":"Next"},{"title":"To make your life easier, run these commands from the recipe directory (here recipes/cometkiwi).​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#to-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipescometkiwi","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert the base model​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#download-and-convert-the-base-model","content":" eole convert HF --model_dir facebook/xlm-roberta-xxl --output $EOLE_MODEL_DIR/xlm-roberta-xxl-eole   NOTEThe facebook original model is stored in FP32 but we convert it to FP16 at conversion.  XXL is a 10.7G params model hence will save a 21.4GB file on disk (safetensors format) XL is a 3.5GB params model hence will save a 7.0GB file on disk  After conversion in $EOLE_MODEL_DIR you will get the following files: config.json model.00.safetensors sentencepiece.bpe.model vocab.json vocab.txt  The vocab.txt file contains 250000 entries (from sentencepiece) but the model was trained with extra tokens You need to concat the added_vocab.txt file to the vocab.txt file resulting in 250880 tokens  ","version":"Next","tagName":"h3"},{"title":"Training cometkiwi​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#training-cometkiwi","content":" Training will happen in two steps (see cometkiwi-xxl-eole.yaml file) FYI the trained model can be downloaded here: https://huggingface.co/eole-nlp/cometkiwi-xxl-eole/  STEP 1: We train from the converted xlm-roberta-xxl model but we keep everything frozen. We just add an Estimator layer that will be trained during 4000 steps To make sure we do not lose anything we will rename the pre-trained subfolder into step_4000_pretrainIn this step we do NOT use LoRA but we use 4bit quant to make things easier and fit in a smaller cards  STEP 2: We release the encoder to make weights trainable hence we need to use LoRA since the model is big and gradients would not fit in consumer grade cards. We train during 40000 steps  For the two steps above, the training command is the same but make sure the yaml file is modified according to the instructionseole train --config cometkiwi-xxl-eole.yaml  After this step we need to merge the LoRA weights into the original model with the following command:  eole model lora --base_model &quot;./cometkiwi-xxl-eole/step_4000_pretrain/&quot; --lora_weights &quot;./cometkiwi-xxl-eole/&quot; --output &quot;./cometkiwi-xxl-eole/merged&quot;  ","version":"Next","tagName":"h2"},{"title":"Cometkiwi Inference​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#cometkiwi-inference","content":" Format the source / target file you want to score in the Comet format:  ./combine.sh newstest2014.en newstest2014.de newstest2014.comb  Score the .comb file:  eole predict --config cometkiwi-xxl-inference.yaml --src newstest2014.comb --output newstest2014.scores --with_score  For now the scores are in the third column, so you can cut -f3 the output file ","version":"Next","tagName":"h2"},{"title":"Serving Eurollm in a gradio interface with Eole","type":0,"sectionRef":"#","url":"/eole/docs/recipes/eurollm/","content":"","keywords":"","version":"Next"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#download-and-convert-model","content":" eole convert HF --model_dir utter-project/EuroLLM-9B-Instruct --output $EOLE_MODEL_DIR/EuroLLM-9B-Instruct --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Run server with the config file from this folder (you can add options according to your needs)​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#run-server-with-the-config-file-from-this-folder-you-can-add-options-according-to-your-needs","content":" eole serve -c serve.yaml   ","version":"Next","tagName":"h2"},{"title":"Start the gradio based translator​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#start-the-gradio-based-translator","content":" python eole-translator.py   You can access the Web based translator from the url given by Gradio (either local or from Gradio proxy with share=True turned on)  ","version":"Next","tagName":"h2"},{"title":"Alternatively you can also play with the API​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#alternatively-you-can-also-play-with-the-api","content":" FastAPI exposes a swagger UI by default. It should be accessible via your browser at http://localhost:5000/docs. ","version":"Next","tagName":"h2"},{"title":"OpenAI GPT2","type":0,"sectionRef":"#","url":"/eole/docs/recipes/gpt2/","content":"","keywords":"","version":"Next"},{"title":"Convert​","type":1,"pageTitle":"OpenAI GPT2","url":"/eole/docs/recipes/gpt2/#convert","content":" eole convert HF --model_dir openai-community/gpt2 --output $EOLE_MODEL_DIR/openai_gpt2 --token $HF_TOKEN   ","version":"Next","tagName":"h2"},{"title":"Infer​","type":1,"pageTitle":"OpenAI GPT2","url":"/eole/docs/recipes/gpt2/#infer","content":" echo -e &quot;The European Union was created in&quot; &gt; lm_input.txt eole predict -c inference.yaml   ","version":"Next","tagName":"h2"},{"title":"HellaSwag benchmark​","type":1,"pageTitle":"OpenAI GPT2","url":"/eole/docs/recipes/gpt2/#hellaswag-benchmark","content":" eole tools eval_hellaswag -c inference.yaml   Eole results, marginally different due to slight implementation differences (nn.Linear vs nn.Conv1D):  ... 10040 acc: 0.2865 acc_norm: 2959/10040=0.2947 10041 acc: 0.2864 acc_norm: 2959/10041=0.2947 10042 acc: 0.2864 acc_norm: 2960/10042=0.2948   Comparable results from llm.c script using official huggingface implementation:  ... 10040 acc: 0.2862 acc_norm: 2966/10040=0.2954 10041 acc: 0.2861 acc_norm: 2966/10041=0.2954 10042 acc: 0.2861 acc_norm: 2967/10042=0.2955  ","version":"Next","tagName":"h2"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/eole/docs/quickstart","content":"","keywords":"","version":"Next"},{"title":"How to train a model from scratch​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#how-to-train-a-model-from-scratch","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Prepare the data​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-1-prepare-the-data","content":" To get started, we propose to download a toy English-German dataset for machine translation containing 10k tokenized sentences:  wget https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz tar xf toy-ende.tar.gz cd toy-ende   The data consists of parallel source (src) and target (tgt) data containing one sentence per line with tokens separated by a space:  src-train.txttgt-train.txtsrc-val.txttgt-val.txt  Validation files are used to evaluate the convergence of the training. It usually contains no more than 5k sentences.  $ head -n 2 toy-ende/src-train.txt It is not acceptable that , with the help of the national bureaucracies , Parliament &amp;apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance . Federal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .   We need to build a YAML configuration file to specify the data that will be used:  # toy_en_de.yaml ## Where the samples will be written save_data: toy-ende/run/example ## Where the vocab(s) will be written src_vocab: toy-ende/run/example.vocab.src tgt_vocab: toy-ende/run/example.vocab.tgt # Prevent overwriting existing files in the folder overwrite: False # Corpus opts: data: corpus_1: path_src: toy-ende/src-train.txt path_tgt: toy-ende/tgt-train.txt valid: path_src: toy-ende/src-val.txt path_tgt: toy-ende/tgt-val.txt   From this configuration, we can build the vocab(s), that will be necessary to train the model:  eole build_vocab -config toy_en_de.yaml -n_sample 10000   Notes:  -n_sample is advised here – it represents the number of lines sampled from each corpus to build the vocab.This configuration is the simplest possible, without any tokenization or other transforms. See recipes for more complex pipelines.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Train the model​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-2-train-the-model","content":" To train a model, we need to add the following to the YAML configuration file:  the vocabulary path(s) that will be used: can be that generated by eole build_vocab;training specific parameters.  # toy_en_de.yaml # Model architecture model: architecture: transformer # Train on a single GPU training: world_size: 1 gpu_ranks: [0] model_path: toy-ende/run/model save_checkpoint_steps: 500 train_steps: 1000 valid_steps: 500 # adapt dataloading defaults to very small dataset bucket_size: 1000   Then you can simply run:  eole train -config toy_en_de.yaml   This configuration will run a default transformer model. It will run on a single GPU (world_size 1 &amp; gpu_ranks [0]).  Before the training process actually starts, it is possible to generate transformed samples to simplify any potentially required visual inspection. The number of sample lines to dump per corpus is set with the -n_sample flag.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Translate​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-3-translate","content":" eole predict -model_path toy-ende/run/model -src toy-ende/src-test.txt -output toy-ende/pred_1000.txt -gpu 0 -verbose   Now you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into toy-ende/pred_1000.txt.  Note:  The predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets!  For example you can download millions of parallel sentences for translation or summarization.  ","version":"Next","tagName":"h3"},{"title":"How to generate with a pretrained LLM​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#how-to-generate-with-a-pretrained-llm","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Convert a model from Hugging Face Hub​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-1-convert-a-model-from-hugging-face-hub","content":" Several converters are provided for models 1) from Hugging Face hub: T5, Falcon, MPT, Openllama, Redpajama, Xgen or 2) the legacy Llama from Meta.  See here for conversion command line.  T5 (and variant Flan-T5), Llama and Openllama use Sentencepiece. Other models uses BPE, we had to reconstruct the BPE model and vocab file:  MPT bpe model  MPT vocab  Redpajama bpe model  Redpajama vocab  Falcon bpe model  Falcon vocab  The command line to convert a model to OpenNMT-py is:  Note: providing a HuggingFace repo id is supported in most conversion tools.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Prepare an inference.yaml config file​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-2-prepare-an-inferenceyaml-config-file","content":" Even though it is not mandatory, the best way to run inference is to use a config file; here is an example:  transforms: [sentencepiece] #### Subword src_subword_model: &quot;/path_to/llama7B/tokenizer.model&quot; tgt_subword_model: &quot;/path_to/llama7B/tokenizer.model&quot; # Model info model_path: &quot;/path_to/llama7B/llama7B-eole.pt&quot; # Inference seed: 42 max_length: 256 gpu: 0 batch_type: sents batch_size: 1 compute_dtype: fp16 #random_sampling_topk: 40 #random_sampling_topp: 0.75 #random_sampling_temp: 0.1 beam_size: 1 n_best: 1 report_time: true   or similarly for a model using BPE:  transforms: [onmt_tokenize] #### Subword src_subword_type: bpe src_subword_model: &quot;/path_to/mpt7B/mpt-model.bpe&quot; src_onmttok_kwargs: '{&quot;mode&quot;: &quot;conservative&quot;}' tgt_subword_type: bpe tgt_subword_model: &quot;/path_to/mpt7B/mpt-model.bpe&quot; tgt_onmttok_kwargs: '{&quot;mode&quot;: &quot;conservative&quot;}' gpt2_pretok: true # Model info model_path: &quot;/path_to/mpt7B/mpt-eole.pt&quot; # Inference seed: 42 max_length: 1 gpu: 0 batch_type: sents batch_size: 1 compute_dtype: fp16 #random_sampling_topk: 40 #random_sampling_topp: 0.75 #random_sampling_temp: 0.8 beam_size: 1 report_time: true src: None tgt: None   In this second example, we used max_length: 1 and src: None tgt: None which is typically the configuration to be used in a scoring script like MMLU where it expects only 1 token as the answer.  WARNINGFor inhomogeneous batches with many examples, the potentially high number of tokens inserted in the shortest examples leads to degraded results when attention layer quantization and flash attention are activated. In practice, in the inference configuration file, when batch_size is greater than 1, delete ‘linear_values’, ‘linear_query’, ‘linear_keys’, ‘final_linear’ from quant_layers and specify self_attn_type: scaled-dot.  You can run this script with the following command line:  eole tools run_mmlu --config myinference.yaml   ","version":"Next","tagName":"h3"},{"title":"Step 3: Generate text​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-3-generate-text","content":" Generating text is also easier with an inference config file (in which you can set max_length or ramdom sampling settings):  eole predict --config /path_to_config/llama7B/llama-inference.yaml --src /path_to_source/input.txt --output /path_to_target/output.txt   ","version":"Next","tagName":"h3"},{"title":"How to finetune a pretrained LLM​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#how-to-finetune-a-pretrained-llm","content":" See Llama2 recipe for an end-to-end example.  Note:  If you want to enable the “zero-out prompt loss” mechanism to ignore the prompt when calculating the loss, you can add the insert_mask_before_placeholder transform as well as the zero_out_prompt_loss flag:  transforms: [insert_mask_before_placeholder, sentencepiece, filtertoolong] zero_out_prompt_loss: true   The default value for the response response_pattern used to locate the end of the prompt is “Response : ｟newline｠”, but you can choose another to align it with your training data. ","version":"Next","tagName":"h2"},{"title":"Llama2","type":0,"sectionRef":"#","url":"/eole/docs/recipes/llama2/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/llama2).​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama2","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#download-and-convert-model","content":" eole convert HF --model_dir meta-llama/Llama-2-7b-chat-hf --output $EOLE_MODEL_DIR/llama2-7b-chat-hf --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Write test prompt to text file​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#write-test-prompt-to-text-file","content":" echo -e &quot;&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt; You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. &lt;&lt;/SYS&gt;&gt; What are some nice places to visit in France? [/INST]&quot; | sed ':a;N;$!ba;s/\\n/｟newline｠/g' &gt; test_prompt.txt   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#run-inference","content":" Single GPU  eole predict -c llama-inference.yaml -src test_prompt.txt -output test_output.txt   Dual GPU (tensor parallelism)  eole predict -c llama-inference-tp-2gpu.yaml -src test_prompt.txt -output test_output.txt   ","version":"Next","tagName":"h3"},{"title":"Finetuning​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#finetuning","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve data​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#retrieve-data","content":" [ ! -d ./data ] &amp;&amp; mkdir ./data # Alpaca wget -P ./data https://opennmt-models.s3.amazonaws.com/llama/alpaca_clean.txt # Vicuna wget -P ./data https://opennmt-models.s3.amazonaws.com/llama/sharegpt.txt # Open Assisstant wget -P ./data https://opennmt-models.s3.amazonaws.com/llama/osst1.flattened.txt   ","version":"Next","tagName":"h3"},{"title":"Finetune​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#finetune","content":" eole train -c llama-finetune.yaml   ","version":"Next","tagName":"h3"},{"title":"Merge LoRa weights​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#merge-lora-weights","content":" eole model lora --action merge --base_model ${EOLE_MODEL_DIR}/llama2-7b-chat-hf --lora_weights ./finetune/llama2-7b-chat-hf-finetune --output ./finetune/merged   Then you can just update your inference setup to use the newly finetuned &amp; merged model. ","version":"Next","tagName":"h3"},{"title":"Mistral","type":0,"sectionRef":"#","url":"/eole/docs/recipes/mistral/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/mistral).​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesmistral","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#download-and-convert-model","content":" eole convert HF --model_dir mistralai/Mistral-7B-v0.3 --output ${EOLE_MODEL_DIR}/mistral-7b-v0.3 --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Write test prompt to text file​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#write-test-prompt-to-text-file","content":" echo -e &quot;What are some nice places to visit in France?&quot; | sed ':a;N;$!ba;s/\\n/｟newline｠/g' &gt; test_prompt.txt   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#run-inference","content":" eole predict -c mistral-7b-awq-gemm-inference.yaml -src test_prompt.txt -output test_output.txt  ","version":"Next","tagName":"h3"},{"title":"Llama3","type":0,"sectionRef":"#","url":"/eole/docs/recipes/llama3/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/llama3).​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama3","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#download-and-convert-model","content":" eole convert HF --model_dir meta-llama/Meta-Llama-3-8B-Instruct --output $EOLE_MODEL_DIR/llama3-8b-instruct --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Write test prompt to text file​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#write-test-prompt-to-text-file","content":" echo -e &quot;What are some nice places to visit in France?&quot; | sed ':a;N;$!ba;s/\\n/｟newline｠/g' &gt; test_prompt.txt   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#run-inference","content":" eole predict -c llama-inference.yaml -src test_prompt.txt -output test_output.txt  ","version":"Next","tagName":"h3"},{"title":"Model Validator","type":0,"sectionRef":"#","url":"/eole/docs/recipes/model-validator/","content":"","keywords":"","version":"Next"},{"title":"1. Edit the models you want to run in the run.sh script:​","type":1,"pageTitle":"Model Validator","url":"/eole/docs/recipes/model-validator/#1-edit-the-models-you-want-to-run-in-the-runsh-script","content":" #!/bin/bash SCRIPT_DIR=$( cd -- &quot;$( dirname -- &quot;${BASH_SOURCE[0]}&quot; )&quot; &amp;&gt; /dev/null &amp;&amp; pwd ) # Define the models table models=( &quot;mistralai/Ministral-8B-Instruct-2410&quot; &quot;mistralai/Mistral-7B-v0.3&quot; &quot;mistralai/Mathstral-7B-v0.1&quot; &quot;meta-llama/Llama-3.2-1B&quot; ... ) ...   ","version":"Next","tagName":"h2"},{"title":"2. Run the script​","type":1,"pageTitle":"Model Validator","url":"/eole/docs/recipes/model-validator/#2-run-the-script","content":" nohup ./run.sh &gt; &lt;custom_logfile_name&gt;.log  ","version":"Next","tagName":"h2"},{"title":"Mixtral","type":0,"sectionRef":"#","url":"/eole/docs/recipes/mixtral/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/mixtral).​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesmixtral","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#download-and-convert-model","content":" eole convert HF --model_dir TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ --output ${EOLE_MODEL_DIR}/mixtral-8x7b-instruct-v0.1-awq --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Write test prompt to text file​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#write-test-prompt-to-text-file","content":" echo -e &quot;What are some nice places to visit in France?&quot; | sed ':a;N;$!ba;s/\\n/｟newline｠/g' &gt; test_prompt.txt   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#run-inference","content":" eole predict -c mixtral-inference-awq.yaml -src test_prompt.txt -output test_output.txt  ","version":"Next","tagName":"h3"},{"title":"Example of using Hugging Face streaming dataset","type":0,"sectionRef":"#","url":"/eole/docs/recipes/NewsPalm-synthetic/","content":"","keywords":"","version":"Next"},{"title":"Based on: https://arxiv.org/pdf/2408.06537​","type":1,"pageTitle":"Example of using Hugging Face streaming dataset","url":"/eole/docs/recipes/NewsPalm-synthetic/#based-on-httpsarxivorgpdf240806537","content":" Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality Parallel Data Outperforms Traditional Web-Crawled Data  ","version":"Next","tagName":"h2"},{"title":"Get the vocab and BPE model on HF​","type":1,"pageTitle":"Example of using Hugging Face streaming dataset","url":"/eole/docs/recipes/NewsPalm-synthetic/#get-the-vocab-and-bpe-model-on-hf","content":" https://huggingface.co/eole-nlp/NewsPalmSynthetic-ENDE  copy files:  ende.vocab2subwords.en_de.bpe  ","version":"Next","tagName":"h3"},{"title":"Optionally you can get the trained model to test it.​","type":1,"pageTitle":"Example of using Hugging Face streaming dataset","url":"/eole/docs/recipes/NewsPalm-synthetic/#optionally-you-can-get-the-trained-model-to-test-it","content":" config.jsonvocab.jsonmodel.00.safetensors  ","version":"Next","tagName":"h3"},{"title":"Train with the yaml config file​","type":1,"pageTitle":"Example of using Hugging Face streaming dataset","url":"/eole/docs/recipes/NewsPalm-synthetic/#train-with-the-yaml-config-file","content":" eole train -c newspalm-synthetic-hfstreaming.yaml   ","version":"Next","tagName":"h2"},{"title":"Run inference​","type":1,"pageTitle":"Example of using Hugging Face streaming dataset","url":"/eole/docs/recipes/NewsPalm-synthetic/#run-inference","content":" eole predict -c inference.yaml --src newstest2023-src.en --output newstest2023-hyp.de   Then you can score with sacrebleu and/or comet  Scoring with Unbabel/wmt22-comet-da gives: 81.90  You can compare to table 5 lines 2a) to 2d) of the paper https://arxiv.org/pdf/2408.06537 ","version":"Next","tagName":"h2"},{"title":"Llama3","type":0,"sectionRef":"#","url":"/eole/docs/recipes/llama3.1/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/llama3.1).​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama31","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#download-and-convert-model","content":" eole convert HF --model_dir meta-llama/Meta-Llama-3.1-8B --output $EOLE_MODEL_DIR/llama3.1-8b --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Write test prompt to text file​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#write-test-prompt-to-text-file","content":" (Example prompt inspired from this HF PR: https://github.com/huggingface/transformers/pull/24653)  echo -e &quot;You are given this machine learning research paper, please read it carefully and answer the follow up question. === BEGIN === 2306.15595v2 [cs.CL] 28 Jun 2023 arXiv EXTENDING CONTEXT WINDOW OF LARGE LAN- GUAGE MODELS VIA POSITION INTERPOLATION Shouyuan Chen Sherman Wong Liangjian Chen Yuandong Tian Meta Platforms Inc. {chenshouyuan, shermanwong, cli, yuandong}@meta . com 1 INTRODUCTION Large language models (LLMs) typically come with a pre-defined context window size. For exam- ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set context window limit is frequently exceeded in applications such as conducting long conversations, summarizing long documents, or executing long-term planning. For these applications, LLMs with longer context windows are preferred. However, training an LLM from scratch with long context windows requires significant investments. This naturally leads to a question: Can we extend the context window of an existing pre-trained LLM? One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con- text window. However, empirically, we found that models trained this way adapt to long context windows very slowly. After training for more than 10000 batches, the effective context window saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is inefficient for extending to substantially longer context windows. While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length extrapolation of Transformers, i.e. train on short context windows and inference on longer ones, many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability of these techniques for extending the context window sizes of such LLMs remains limited. In this work, we introduce Position Interpolation to enable context window extensions for certain existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly down-scale the position indices so that the maximum position index matches the previous context window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom- modate more input tokens, we interpolate the position encodings at neighboring integer positions, utilizing the fact that position encodings can be applied on non-integer positions, as opposed to extrapolating outside the trained positions, which may lead to catastrophic values. We verify our approach theoretically, by showing that the interpolated attention score has a much smaller upper bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more stable. Therefore, interpolated position encodings are easier for the model to adapt. Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a very short period of fine-tuning for the model to fully adapt to greatly extended context windows. We present experimental results for extending the context window to up to 32768 from the initial 2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that 1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality. The cost of fine-tuning is negligible compared to the pre-training costs. This confirms our hypothesis that it is relatively easy for the models to adapt to interpolated position encodings. 2. Position Interpolation generates strong models that can effectively make use of much ex- tended context window. We show that models extended by Position Interpolation enjoy significant perplexity gains from greatly extended context windows for text modeling, and we show that the perplexity reduces graceful with the enlargement of context windows. We also applied Position Interpolation in a long text summarization task, and demonstrate competitive performances. 3. Position Interpolation preserves model quality relatively well for tasks within its original context window sizes. We present a variety of evaluation results for the extended LLaMA models on the original LLaMA benchmark. Compared with original LLaMA models, the extended LLLaM A models saw a minor degradation on several standard benchmarks within a 2048 token limit. Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex- trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct extrapolation of positional encodings and it can be largely mitigated by interpolating position en- codings instead. Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super- HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context window from 2K to 8K. Recently, open source community picks it up in Reddit post ! and Github Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we also give theoretical explanations why interpolation achieves much more stable results than extrap- olation, by showing that the upper bound of interplated attention score is much lower than that of extrapolated ones. 2 METHOD 2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE) Transformer models require explicit positional information to be injected, typically in the form of positional encodings, to represent the order of inputs. We consider Rotary Position Embedding (ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al., 2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as follows Using RoPE, the self-attention score is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the query and key vector for a specific attention head. At each layer, RoPE is applied on both query and key embeddings for computing attention scores. 2.2 DIRECT EXTRAPOLATION While the attention score in RoPE only depends on the relative positions, which is what we want, its extrapolation performance is not great . In particular, when directly extending to larger context windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., &gt; 10%), comparable to untrained models. Ideally, we want to see the model trained on a context window of size L = 2048 to still work reasonably well on longer context window, but may not have the capability to leverage information that appears beyond L. For example, to answer a question located at 3000, the model trained on maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are located at location 2900. What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3 of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and think about Eqn. 2 as basis expansion as the following: where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1) are complex coefficients depending on q and k (here the definition of h; is exactly the same as the definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but much larger in regions beyond. 2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI) In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s &gt; L, how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window? Formally, we replace RoPE f by {’ defined as follows We call this transformation on the position encoding Position Interpolation. In this step, we reduce position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE. Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been reduced from I’ to L. Since we align the ranges of position indices and relative distances before and after extension, we mitigate the effect on attention score computation due to context window extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the following theorem, we show that the interpolated attention score is well-behaved: While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d (check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x smaller than the extrapolation bound, and thus the interpolated attention score is much more stable than extrapolated one. Notably, our method of rescaling of position indices does not introduce extra weight, or modify the model architecture in any way. This makes it attractive in practical applications, since most infrastructure and optimization for the original model can be reused after the extension. Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task with interpolated position encodings on the extended context window size using a pre-training cor- pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning is not sensitive to the choice of examples. The reason may be that the model is only adapting to the new context window during the fine-tuning phase, starting from a good initialization, as opposed to acquiring new knowledge. Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola- tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal magnitude of query/key products. If we enforce a regularization on || during LLM training, it is possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo- lated a(s) when s &gt; L can be comparable to that within [0, L]. To our knowledge, we are not aware of existing LLM pre-training techniques that leverage this regularization and will leave it for future work. 3 EXPERIMENTS We show Position Interpolation can effectively extend context window up to 32 times of the original size, and such extension can be done with only several hundreds of training steps. We show the resulting models are strong LLMs with fully effective long context windows. We demonstrate its performance in a number of tasks including language modeling, passkey retrieval, and long doc- ument summarization. We also present benchmark results of the extended models on the original LLaMA evaluation benchmarks. 3.1 SETUP Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or Position Interpoloation method. Except for rescaling the position indices for models extended with Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any ways. Training Procedure. We fine-tune all model variants using the next token prediction objective. We use AdamW (Loshchilov &amp; Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models, we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and 128 global batch size. We note that the main need of using more GPUs is memory limitation during fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch (Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao et al., 2022). If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000 steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the RedPajama dataset (Computer, 2023). 3.2 LONG SEQUENCE LANGUAGE MODELING We evaluate the long sequence language modeling performance of our extended models and base- lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile dataset (Azerbayev et al., 2022). We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19, we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random subsample of 128 documents with at least 32768 SentencePiece (Kudo &amp; Richardson, 2018) tokens and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various context window size by using a sliding window approach following Press et al. (2022) with stride S = 256. In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets. From the results, we found that models extended with our method enjoy a significantly improved perplexity from longer context window sizes. By increasing the context window size from 2048 to 16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42 reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and -0.3 reductions of perplexity by extending to the 8192 context window size. In general, we observed a consistent trend of our models achieving better perplexity with longer context windows. This indicates our models can effectively make use of the longer context windows to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to 32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This indicates that our method may enable extension to even longer context windows. In contrast, we observed that models extended via the direct fine-tuning method has shown regres- sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows. This indicates that models extended this way have limited capability of making use of context win- dows longer than their pre-trained settings. We saw a minor degradation of the perplexity on the original context window of 2048 for our ex- tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation of performance within original evaluation context window is expected since Position Interpolation forces position encodings in original context window to reside in a much narrower region, which may negatively affect the language model’s performance. We present more benchmark results on the original context window size in Section 3.4. In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit certain language modeling capability, as indicated by &lt; 20 perplexity for extending to 8192 context window (in contrast, the direct extrapolation method leads to &gt; 10% perplexity). With fine-tuning, we observed that the perplexity improves quickly. At 200 steps the models surpassed the original model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can see the models have improved steadily and achieve a significantly better perplexity. 3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL We study the effective context window size, i.e. the maximum distance of a token can effectively attend to during inference, of our models after extension. To measure this, we follow a synthetic evaluation task of passkey retrieval proposed by Mohtashami &amp; Jaggi (2023). In this task, the models are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of the document. Given a language model, we estimate the upper and lower bounds of effective context windows as follows. Suppose the random passkey is k tokens away from the end of the input. When a model persistently fails to retrieve the correct passkey value across several independent attempts, it suggests that the effective context window size of the model is less than k. Conversely, if a model consistently succeeds in retrieving the correct passkey value, we deduce that the effective context window size of the model is at least k. We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or direct fine-tuning. For each model, we use 32 different &amp;£ uniformly spaced in the targeted context window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5 random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps, We can see that models extended via Position Interpolation all successfully attain their desired ex- tension objectives in terms of effective context window sizes, indicating by the effective context window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context win- dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size. 3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE We evaluate the models extended by Position Interpolation on several standard benchmark tasks within the original context window size of 2048. The evaluation results are listed in Table 5. From the results, we saw that models extended to 8192 produce comparable results on the original bench- mark which is designed for a much smaller context window, with a degradation of up to 2% on the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win- dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark performances, which may be due to the limited number of fine-tuning steps used in our method. The regression on benchmark tasks is consistent with our observation on perplexity regression in Section 3.2. 3.5 LONG DOCUMENT SUMMARIZATION In this task, we evaluate our models’ performance on the long document summarization task. In particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents for training and 972 documents for evaluation. Each document comes with a human generated summary. We truncate all input documents to their first 15000 tokens. We fine-tune the LL.aMA models extended with Position Interpolation with a context window of 16384. Note the rescaling of position indices are still required during this fine-tuning step. We first Model Size Context Window Fine-tune on BoolQ PIQA Race-M Race-H WinoGrande format the raw document using the prompt template in Figure 4, and then concatenate the prompt with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine- tune the model using the next token prediction task with the above setup for 10 epochs. The losses from the input prompt proportion of training examples are excluded during our fine-tuning. We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a summarization of each document in the test set. The final output is truncated at 1000 tokens. We used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate the models’ outputs vs the ground-truth summaries. In Table 6 we report our evaluation results. We have also included results from two baselines in existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This result suggests our models with 16384 context window can effectively handle the long document summarization task. === END OF FILE === Question: What is the paper about? Answer: &quot; | sed ':a;N;$!ba;s/\\n/｟newline｠/g' &gt; test_prompt.txt   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#run-inference","content":" eole predict -c llama-inference.yaml -src test_prompt.txt -output test_output.txt  ","version":"Next","tagName":"h3"},{"title":"Pixtral","type":0,"sectionRef":"#","url":"/eole/docs/recipes/pixtral/","content":"","keywords":"","version":"Next"},{"title":"Convert the model​","type":1,"pageTitle":"Pixtral","url":"/eole/docs/recipes/pixtral/#convert-the-model","content":" eole convert HF --model_dir mistral-community/pixtral-12b --output ./pixtral-12b --token $HF_TOKEN   ","version":"Next","tagName":"h2"},{"title":"Run the test script​","type":1,"pageTitle":"Pixtral","url":"/eole/docs/recipes/pixtral/#run-the-test-script","content":" python3 test_inference.py   There are several examples in the test script (taken from pixtral blog posts). A single one is activated by default, but you can uncomment the others to test the various cases.  ","version":"Next","tagName":"h2"},{"title":"Finetuning​","type":1,"pageTitle":"Pixtral","url":"/eole/docs/recipes/pixtral/#finetuning","content":" Finetuning is untested for now. Feel free to try it out and fix any arising issues. ","version":"Next","tagName":"h2"},{"title":"NLLB","type":0,"sectionRef":"#","url":"/eole/docs/recipes/nllb/","content":"","keywords":"","version":"Next"},{"title":"Conversion​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#conversion","content":" ","version":"Next","tagName":"h2"},{"title":"1. Sentencepiece with OpenNMT Tokenizer​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#1-sentencepiece-with-opennmt-tokenizer","content":" eole convert HF --model_dir facebook/nllb-200-1.3B --output ./nllb-1.3b --token $HF_TOKEN --tokenizer onmt   ","version":"Next","tagName":"h3"},{"title":"2. HuggingFace Tokenizer​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#2-huggingface-tokenizer","content":" eole convert HF --model_dir facebook/nllb-200-1.3B --output ./nllb-1.3b --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#inference","content":" echo &quot;What is the weather like in Tahiti?&quot; &gt; test.en   ","version":"Next","tagName":"h2"},{"title":"1. Sentencepiece with OpenNMT Tokenizer​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#1-sentencepiece-with-opennmt-tokenizer-1","content":" eole predict -c inference-pyonmttok.yaml   ","version":"Next","tagName":"h3"},{"title":"2. HuggingFace Tokenizer​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#2-huggingface-tokenizer-1","content":" eole predict -c inference-hf.yaml  ","version":"Next","tagName":"h3"},{"title":"Evaluation with MMLU dataset","type":0,"sectionRef":"#","url":"/eole/docs/recipes/mmlu/","content":"","keywords":"","version":"Next"},{"title":"How to run​","type":1,"pageTitle":"Evaluation with MMLU dataset","url":"/eole/docs/recipes/mmlu/#how-to-run","content":" To run the MMLU benchmark with a specific model, execute the following command from the recipe directory (recipes/mmlu):  eole tools run_mmlu -c &lt;model_inference_config&gt;   For instance, following the llama3 recipe:  eole tools run_mmlu -c ../llama3/llama-mmlu.yaml   ","version":"Next","tagName":"h2"},{"title":"Results​","type":1,"pageTitle":"Evaluation with MMLU dataset","url":"/eole/docs/recipes/mmlu/#results","content":" Note: Below are the legacy OpenNMT-py results. We might re-run and update everything at some point.  All evaluations below have been computed with the OpenNMT-py converted models.  The evaluation script is taken from the https://github.com/FranxYao/chain-of-thought-hub repo and modified to use the OpenNMT-py models  There is a difference compared to the original MMLU Hendrycks script.  We do not compare the logprobs of A, B, C, D to determine the answer, we actually decode the next token after the prompt.  When the model is Sentencepiece based the next token can be 'A', 'B', 'C', 'D' or any other token. When the model is BPE based the tokens will be ' A', ' B', ' C', ' D' because the leading space is encoded with the letter, We strip that space to compute the metric.  For 7B params models:  Llama7B score (35.25) matches both the Llama paper and the score reported by chain-of-thought-hub Falcon7B is a little higher then the score reported by chain-of-thought-hub (0.2641) I ran MPT7B with chain-of-thought-hub and found 28.46, again ours is a little higher. There are major discrepancies between those scores and Open LLM leaderboard of HF for MPT, Falcon, Redpajama that are way higher on the leaderboard.  For 13B, 33B, 40B models, we score with the 4-bit loading option, hence for Llama13B a score slightly under the paper (46.9), same for 33B (paper is 57.8)  \tMPT7B\tRedpajama7B\tOpen Llama7B\tFalcon7B\txgen7B\tFlan-T5-3B\tLlama7B\tLlama-2-7B\tLlama-2-chat-7B\tOpen Llama13B\tLlama13B\tLlama-2-13B\tLlama-2-chat-13B\tFalcon40B\tLlama33B\tLlama-2-70BACC-all\t0.2958\t0.2745\t0.3007\t0.2765\t0.3468\t0.4929\t0.3525\t0.4587\t0.4569\t0.4148\t0.4472\t0.5429\t0.5217\t0.5499\t0.5701\t0.6875 ACC-abstract_algebra\t0.2200\t0.2500\t0.3000\t0.2400\t0.2900\t0.2700\t0.2500\t0.3000\t0.3100\t0.3200\t0.2800\t0.3100\t0.3500\t0.3200\t0.3700\t0.3900 ACC-anatomy\t0.2963\t0.2667\t0.3333\t0.2444\t0.3185\t0.4296\t0.3852\t0.4815\t0.4222\t0.4667\t0.4889\t0.5037\t0.5037\t0.5111\t0.5185\t0.6296 ACC-astronomy\t0.2961\t0.2763\t0.2500\t0.2434\t0.3355\t0.4737\t0.3487\t0.4079\t0.4803\t0.4737\t0.4671\t0.5263\t0.5461\t0.5658\t0.6118\t0.7895 ACC-business_ethics\t0.2900\t0.2900\t0.3200\t0.1900\t0.3200\t0.6800\t0.4100\t0.5300\t0.4200\t0.4100\t0.4300\t0.5500\t0.5000\t0.5500\t0.5800\t0.6900 ACC-clinical_knowledge\t0.2943\t0.3208\t0.3887\t0.3019\t0.3057\t0.5245\t0.3585\t0.4604\t0.5208\t0.4113\t0.4189\t0.5811\t0.5698\t0.6113\t0.5547\t0.7019 ACC-college_biology\t0.3056\t0.3125\t0.3264\t0.2153\t0.3958\t0.4444\t0.3819\t0.4722\t0.5417\t0.4167\t0.4722\t0.5694\t0.5347\t0.6319\t0.5833\t0.8333 ACC-college_chemistry\t0.2800\t0.2700\t0.2400\t0.2300\t0.2500\t0.3400\t0.2900\t0.3400\t0.2500\t0.2800\t0.2400\t0.3900\t0.3600\t0.4100\t0.3800\t0.5200 ACC-college_computer_science\t0.3100\t0.3100\t0.3100\t0.3000\t0.3300\t0.3600\t0.2900\t0.3400\t0.3600\t0.4000\t0.3700\t0.4600\t0.5100\t0.4700\t0.4400\t0.6000 ACC-college_mathematics\t0.2900\t0.2500\t0.2800\t0.2900\t0.3200\t0.2900\t0.3400\t0.3800\t0.3400\t0.3200\t0.2500\t0.3000\t0.2800\t0.3500\t0.3600\t0.3700 ACC-college_medicine\t0.2890\t0.2659\t0.3179\t0.2659\t0.3410\t0.4277\t0.3237\t0.4220\t0.4104\t0.3699\t0.4220\t0.5318\t0.4451\t0.4798\t0.5376\t0.6532 ACC-college_physics\t0.2157\t0.2451\t0.1863\t0.2157\t0.2353\t0.2941\t0.2451\t0.2255\t0.2451\t0.2549\t0.1863\t0.2647\t0.3137\t0.3333\t0.3137\t0.3333 ACC-computer_security\t0.3100\t0.3600\t0.3800\t0.2800\t0.3900\t0.6400\t0.4500\t0.6200\t0.5400\t0.5400\t0.6300\t0.6900\t0.6700\t0.6500\t0.6800\t0.8100 ACC-conceptual_physics\t0.3362\t0.2723\t0.3064\t0.3149\t0.3489\t0.4085\t0.3702\t0.4170\t0.3872\t0.3574\t0.3915\t0.4511\t0.3787\t0.4170\t0.4723\t0.6723 ACC-econometrics\t0.2895\t0.2368\t0.2895\t0.2632\t0.2632\t0.2807\t0.2632\t0.2632\t0.3333\t0.3070\t0.2719\t0.2895\t0.3158\t0.3246\t0.3333\t0.4123 ACC-electrical_engineering\t0.2897\t0.3034\t0.3034\t0.2828\t0.3862\t0.4552\t0.2483\t0.4759\t0.4345\t0.4966\t0.3862\t0.5172\t0.5103\t0.5034\t0.4690\t0.6276 ACC-elementary_mathematics\t0.2698\t0.2646\t0.2698\t0.2593\t0.2725\t0.3148\t0.2646\t0.2672\t0.2857\t0.2487\t0.2487\t0.3360\t0.3333\t0.3413\t0.3413\t0.4180 ACC-formal_logic\t0.2540\t0.4048\t0.2381\t0.1905\t0.2619\t0.3333\t0.2619\t0.2698\t0.2381\t0.3016\t0.3889\t0.3492\t0.2857\t0.3413\t0.3571\t0.5000 ACC-global_facts\t0.2700\t0.3200\t0.3200\t0.3100\t0.3300\t0.3600\t0.3000\t0.3200\t0.3100\t0.2900\t0.3400\t0.3200\t0.2900\t0.3300\t0.3900\t0.4500 ACC-high_school_biology\t0.3097\t0.2484\t0.2968\t0.2645\t0.3290\t0.5645\t0.3387\t0.5065\t0.5258\t0.4290\t0.5065\t0.6742\t0.6194\t0.6516\t0.6419\t0.8194 ACC-high_school_chemistry\t0.2020\t0.2660\t0.2512\t0.2512\t0.2611\t0.3300\t0.2956\t0.3744\t0.3547\t0.3350\t0.2660\t0.4286\t0.4138\t0.4187\t0.3793\t0.5468 ACC-high_school_computer_science\t0.3400\t0.2700\t0.2800\t0.3200\t0.3200\t0.5100\t0.3300\t0.4000\t0.4500\t0.2700\t0.4500\t0.5500\t0.5800\t0.6000\t0.5800\t0.7700 ACC-high_school_european_history\t0.3455\t0.2848\t0.3455\t0.2909\t0.3879\t0.7333\t0.4667\t0.6121\t0.5818\t0.4727\t0.6121\t0.6545\t0.6667\t0.6667\t0.7152\t0.8121 ACC-high_school_geography\t0.3737\t0.3283\t0.3333\t0.1667\t0.3636\t0.6414\t0.3333\t0.4899\t0.5960\t0.4899\t0.5000\t0.6616\t0.6616\t0.7121\t0.7273\t0.8636 ACC-high_school_government_and_politics\t0.3782\t0.2124\t0.3575\t0.2591\t0.4352\t0.6632\t0.4611\t0.6736\t0.6632\t0.5959\t0.6425\t0.8135\t0.7617\t0.7927\t0.8187\t0.9430 ACC-high_school_macroeconomics\t0.3821\t0.2718\t0.3564\t0.2615\t0.3359\t0.5359\t0.3410\t0.4513\t0.4103\t0.4282\t0.4256\t0.4923\t0.4744\t0.5641\t0.5590\t0.7308 ACC-high_school_mathematics\t0.2778\t0.2667\t0.2407\t0.2481\t0.2333\t0.3074\t0.2630\t0.2963\t0.2556\t0.2667\t0.2593\t0.2889\t0.3037\t0.3111\t0.2741\t0.3630 ACC-high_school_microeconomics\t0.2941\t0.3067\t0.2941\t0.2899\t0.3697\t0.5168\t0.3319\t0.4412\t0.4328\t0.4370\t0.4454\t0.5630\t0.5042\t0.5504\t0.5588\t0.7605 ACC-high_school_physics\t0.2583\t0.2649\t0.2517\t0.3179\t0.2450\t0.2980\t0.2649\t0.3179\t0.3046\t0.2980\t0.2517\t0.3444\t0.3245\t0.2914\t0.3311\t0.3907 ACC-high_school_psychology\t0.2844\t0.3229\t0.3505\t0.2440\t0.4752\t0.6771\t0.4789\t0.6312\t0.6477\t0.5486\t0.5835\t0.7413\t0.7229\t0.7541\t0.7596\t0.8752 ACC-high_school_statistics\t0.4028\t0.2454\t0.3981\t0.1852\t0.1620\t0.3657\t0.3241\t0.2778\t0.3241\t0.2546\t0.2685\t0.4722\t0.3611\t0.4630\t0.4676\t0.6157 ACC-high_school_us_history\t0.2892\t0.2255\t0.3137\t0.2892\t0.4167\t0.6863\t0.3284\t0.5245\t0.6765\t0.5490\t0.5343\t0.7108\t0.6863\t0.7108\t0.7696\t0.9069 ACC-high_school_world_history\t0.2489\t0.2785\t0.2869\t0.2996\t0.3966\t0.6667\t0.4262\t0.6245\t0.6667\t0.5105\t0.6287\t0.7089\t0.7215\t0.6835\t0.7637\t0.8608 ACC-human_aging\t0.3274\t0.1659\t0.2870\t0.4215\t0.4260\t0.5650\t0.3991\t0.5695\t0.5695\t0.5157\t0.5112\t0.6502\t0.6816\t0.7130\t0.6861\t0.7848 ACC-human_sexuality\t0.3511\t0.2519\t0.2748\t0.2901\t0.3359\t0.5802\t0.3435\t0.5649\t0.4885\t0.4962\t0.5649\t0.6031\t0.5878\t0.6794\t0.6718\t0.8550 ACC-international_law\t0.3802\t0.2231\t0.3636\t0.2479\t0.5041\t0.6860\t0.5207\t0.6529\t0.5620\t0.5207\t0.6860\t0.6860\t0.7851\t0.6612\t0.7603\t0.8595 ACC-jurisprudence\t0.3704\t0.2315\t0.3426\t0.3426\t0.4074\t0.6204\t0.4167\t0.5370\t0.5833\t0.4444\t0.4722\t0.6852\t0.7037\t0.6667\t0.6574\t0.8148 ACC-logical_fallacies\t0.2945\t0.2638\t0.2883\t0.2638\t0.3558\t0.6319\t0.4172\t0.5092\t0.5399\t0.4847\t0.5031\t0.6564\t0.6319\t0.6503\t0.6994\t0.7975 ACC-machine_learning\t0.3125\t0.2232\t0.2321\t0.3750\t0.2589\t0.3571\t0.2768\t0.3839\t0.3393\t0.3571\t0.3304\t0.3036\t0.3482\t0.3036\t0.3750\t0.5089 ACC-management\t0.3301\t0.2816\t0.2524\t0.2816\t0.3010\t0.6796\t0.3301\t0.5631\t0.6699\t0.5243\t0.6311\t0.7379\t0.7184\t0.7184\t0.7573\t0.8252 ACC-marketing\t0.3120\t0.2735\t0.3761\t0.2949\t0.5385\t0.7906\t0.4615\t0.6795\t0.7265\t0.5897\t0.7094\t0.8077\t0.7821\t0.7949\t0.8333\t0.8932 ACC-medical_genetics\t0.3100\t0.2400\t0.2700\t0.2800\t0.3600\t0.4800\t0.3700\t0.5500\t0.5000\t0.5100\t0.5100\t0.5500\t0.5700\t0.6200\t0.6100\t0.7400 ACC-miscellaneous\t0.3001\t0.2899\t0.3678\t0.2976\t0.5326\t0.6782\t0.4278\t0.6450\t0.6692\t0.5900\t0.6296\t0.7407\t0.7458\t0.7471\t0.7752\t0.8557 ACC-moral_disputes\t0.2977\t0.2659\t0.3295\t0.3092\t0.3613\t0.5983\t0.4133\t0.5116\t0.5145\t0.4798\t0.4566\t0.6272\t0.5809\t0.6503\t0.6503\t0.7572 ACC-moral_scenarios\t0.2436\t0.2469\t0.2469\t0.2492\t0.2425\t0.2436\t0.2425\t0.2380\t0.2145\t0.2715\t0.2480\t0.3464\t0.2927\t0.2615\t0.3855\t0.4413 ACC-nutrition\t0.2810\t0.2908\t0.3301\t0.2582\t0.3431\t0.4804\t0.3922\t0.4902\t0.5098\t0.3758\t0.5163\t0.6144\t0.5980\t0.6405\t0.6471\t0.7778 ACC-philosophy\t0.3183\t0.2830\t0.2830\t0.2830\t0.3151\t0.5177\t0.4051\t0.6013\t0.5659\t0.4662\t0.5145\t0.6656\t0.6077\t0.6399\t0.6656\t0.7781 ACC-prehistory\t0.3056\t0.3210\t0.3210\t0.3117\t0.3488\t0.5216\t0.3519\t0.4907\t0.5679\t0.5216\t0.5093\t0.6451\t0.5926\t0.5988\t0.6667\t0.8272 ACC-professional_accounting\t0.2447\t0.2872\t0.2553\t0.2979\t0.3050\t0.3723\t0.2730\t0.3582\t0.3475\t0.3050\t0.3227\t0.3830\t0.3759\t0.4255\t0.4326\t0.5780 ACC-professional_law\t0.2784\t0.2705\t0.2523\t0.2497\t0.2647\t0.3990\t0.2973\t0.3553\t0.3266\t0.3064\t0.3566\t0.4068\t0.3722\t0.4296\t0.4342\t0.5404 ACC-professional_medicine\t0.2206\t0.2059\t0.2500\t0.3125\t0.4375\t0.4412\t0.4265\t0.5184\t0.3529\t0.3860\t0.5000\t0.5221\t0.4706\t0.6176\t0.5441\t0.7390 ACC-professional_psychology\t0.2876\t0.2925\t0.2696\t0.2647\t0.3203\t0.4526\t0.3546\t0.4428\t0.4739\t0.3693\t0.4575\t0.5392\t0.5065\t0.5539\t0.6144\t0.7500 ACC-public_relations\t0.3455\t0.3182\t0.4091\t0.3364\t0.4182\t0.5909\t0.4091\t0.5273\t0.5182\t0.5273\t0.5545\t0.6364\t0.6091\t0.6364\t0.6818\t0.7273 ACC-security_studies\t0.3796\t0.2816\t0.2939\t0.3102\t0.2531\t0.6531\t0.3306\t0.4980\t0.4571\t0.4245\t0.5224\t0.6122\t0.6531\t0.6735\t0.6367\t0.8082 ACC-sociology\t0.2239\t0.2587\t0.2488\t0.3532\t0.4826\t0.7363\t0.4726\t0.6318\t0.5771\t0.5473\t0.6418\t0.7264\t0.7214\t0.7761\t0.7761\t0.8955 ACC-us_foreign_policy\t0.3500\t0.3200\t0.3900\t0.4200\t0.5100\t0.6600\t0.4300\t0.6500\t0.6700\t0.6100\t0.7200\t0.8500\t0.7700\t0.8000\t0.8300\t0.9100 ACC-virology\t0.3494\t0.2530\t0.3494\t0.3554\t0.3735\t0.4819\t0.3253\t0.4217\t0.4277\t0.4398\t0.4096\t0.4458\t0.4940\t0.4639\t0.5000\t0.5361 ACC-world_religions\t0.3158\t0.3041\t0.4035\t0.3333\t0.6140\t0.5614\t0.4912\t0.6842\t0.6842\t0.6550\t0.6491\t0.7602\t0.7427\t0.7719\t0.7953\t0.8538 0.3022\t0.2747\t0.3053\t0.2818\t0.3515\t0.5018\t0.3569\t0.4682\t0.4662\t0.4258\t0.4559\t0.5482\t0.5340\t0.5580\t0.5741\t0.6932 ","version":"Next","tagName":"h2"},{"title":"Serving models with Eole","type":0,"sectionRef":"#","url":"/eole/docs/recipes/server/","content":"","keywords":"","version":"Next"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Option 1 - Download and convert model​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#option-1---download-and-convert-model","content":" The first example &quot;llama3-8b-instruct&quot; requires you to manually convert the model in your desired $EOLE_MODEL_DIR.  eole convert HF --model_dir meta-llama/Meta-Llama-3-8B-Instruct --output $EOLE_MODEL_DIR/llama3-8b-instruct --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Option 2 - Retrieve an already converted model from HF​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#option-2---retrieve-an-already-converted-model-from-hf","content":" The second example &quot;llama3-8b-instruct-hf&quot; downloads a model that has already been converted, for the sake of this example.  ","version":"Next","tagName":"h3"},{"title":"Run server​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#run-server","content":" eole serve -c serve.example.yaml   ","version":"Next","tagName":"h2"},{"title":"Play with the API​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#play-with-the-api","content":" FastAPI exposes a swagger UI by default. It should be accessible via your browser at http://localhost:5000/docs. ","version":"Next","tagName":"h2"},{"title":"Language Model Wiki-103","type":0,"sectionRef":"#","url":"/eole/docs/recipes/wiki_103/","content":"","keywords":"","version":"Next"},{"title":"Step 1: Download and clean the data, prepare subword model​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#step-1-download-and-clean-the-data-prepare-subword-model","content":" Preliminary steps are defined in the prepare_wikitext-103_data.sh script.  The following command will download the WikiText103 dataset, remove empty lines and shuffle the training corpus. It will also call the learn_bpe.py script to train a bpe of 40000 symbols on the training dataset using pyonmttok. The bpe model will be stored in data/wikitext/wikitext-103-raw-v1/subwords.bpe.  chmod u+x prepare_wikitext-103_data.sh ./prepare_wikitext-103_data.sh   ","version":"Next","tagName":"h2"},{"title":"Step 2: Build the vocabulary​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#step-2-build-the-vocabulary","content":" An example of yaml configuration for language modeling task is available in wiki_103.yaml. This configuration will be used for building the vocabulary and training the model. BPE and language modeling specificities are explained in the following sections.  ","version":"Next","tagName":"h2"},{"title":"Language Model specificities​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#language-model-specificities","content":" In LM tasks we expect a single source, therefore path_tgt is not required for LM tasks.  data: corpus_1: path_src: data/wikitext-103-raw/wiki.train.raw   ","version":"Next","tagName":"h3"},{"title":"BPE specificities​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#bpe-specificities","content":" To use BPE tokenization on the fly, the following parameters must be in the config file. Slight differences between on the fly tokenization and outputed tokenized files from step 1 can be observed.  transforms: [onmt_tokenize, filtertoolong] transforms_configs: onmt_tokenize: src_subword_type: bpe src_subword_model: data/wikitext-103-raw/subwords.bpe src_onmttok_kwargs: {&quot;mode&quot;: &quot;aggressive&quot;, &quot;joiner_annotate&quot;: True, &quot;preserve_placeholders&quot;: True, &quot;case_markup&quot;: True, &quot;soft_case_regions&quot;: True, &quot;preserve_segmented_tokens&quot;: True}   ","version":"Next","tagName":"h3"},{"title":"Build vocabulary command​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#build-vocabulary-command","content":" The vocabulary is built using:  eole build_vocab -config wiki_103.yaml -n_sample -1 # -num_threads 4   ","version":"Next","tagName":"h3"},{"title":"Step 3: Train the model​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#step-3-train-the-model","content":" The training is launched using:  eole train -config wiki_103.yaml   Tensorboard can be used to monitor the training.  Expected results: perplexity of 20-22 on the validation set.  ","version":"Next","tagName":"h2"},{"title":"Step 4: Generate output​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#step-4-generate-output","content":" Options contained in the loaded model will trigger language modeling specific inference.  input.txt must contain already tokenized examples, with the same method as the training data. Here, part of validation data will be used:  head data/wikitext-103-raw/wiki.valid.bpe | cut -d&quot; &quot; -f-15 &gt; data/wikitext/wikitext-103-raw-v1/test_input.txt   To proceed with LM inference, sampling methods such as top-k sampling or nucleus sampling are usually applied. Details and options about inference methods can be found in eole/config/inference.py.  The following command will provide inference with nucleus sampling of p=0.9 and return the 3 sequences with the lowest perplexity out of the 10 generated sequences (see inference.yaml):  eole predict -config inference.yaml -model_path data/wikitext/wikitext-103-raw-v1/run/model-lm/step_1000000 -src data/wikitext/wikitext-103-raw-v1/test_input.txt -output data/wikitext/wikitext-103-raw-v1/test_pred.txt   Note: main transform-related settings are now stored within the model and its configuration, which makes the (rather complex) inference.yaml config not strictly necessary anymore. The above command can be converted to a simple command line with the desired settings:  eole predict -model_path data/wikitext/wikitext-103-raw-v1/run/model-lm/step_1000000 -src data/wikitext/wikitext-103-raw-v1/test_input.txt -output data/wikitext/wikitext-103-raw-v1/test_pred.txt -world_size 1 -gpu_ranks 0 -n_best 3 -top_p 0.9 -beam_size 10  ","version":"Next","tagName":"h2"},{"title":"Translation WMT17 en-de","type":0,"sectionRef":"#","url":"/eole/docs/recipes/wmt17/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/wmt17).​","type":1,"pageTitle":"Translation WMT17 en-de","url":"/eole/docs/recipes/wmt17/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipeswmt17","content":" ","version":"Next","tagName":"h2"},{"title":"Tokenization methods​","type":1,"pageTitle":"Translation WMT17 en-de","url":"/eole/docs/recipes/wmt17/#tokenization-methods","content":" The following configurations as provided as example:  wmt17_ende_yaml: &quot;legacy&quot; configuration, using already tokenized data;wmt17_ende_bpe.yaml: on-the-fly bpe tokenization, using the &quot;official&quot; subword-nmt based bpe transform;wmt17_ende_bpe_onmt_tokenize.yaml: on-the-fly bpe tokenization, using the pyonmttok based onmt_tokenize transform;wmt17_ende_spm.yaml: on-the-fly sentencepiece tokenization, using the official sentencepiece based sentencepiece transform;wmt17_ende_spm_onmt_tokenize.yaml: on-the-fly sentencepiece tokenization, using the pyonmttok based onmt_tokenize transform;  ","version":"Next","tagName":"h3"},{"title":"Get Data and prepare​","type":1,"pageTitle":"Translation WMT17 en-de","url":"/eole/docs/recipes/wmt17/#get-data-and-prepare","content":" WMT17 English-German data set:  cd recipes/wmt17 bash prepare_wmt_ende_data.sh   Options:  --method: bpe/sentencepiece (subwords method to use)--encode: true/false (tokenize all datasets, not necessary if using on the fly transforms)  If you want to use one of the aforementioned configurations with on-the-fly transforms, set --encode false, and either of --method bpe/--method sentecepiece.  ","version":"Next","tagName":"h3"},{"title":"Train​","type":1,"pageTitle":"Translation WMT17 en-de","url":"/eole/docs/recipes/wmt17/#train","content":" Choose the config you want to run:  export CONFIG=&quot;wmt17_ende_bpe.yaml&quot;   Training the following big transformer for 50K steps takes less than 10 hours on a single RTX 4090  eole build_vocab --config $CONFIG --n_sample -1 # --num_threads 4 eole train --config $CONFIG   Note: if you need to perform some visual checks on the &quot;transformed&quot; data, you can enable the dump_samples flag at the build_vocab stage (and specify a smaller -n_sample for efficiency).  Translate test sets with various settings on local GPU and CPUs.  Notes:  the exact model path depends on the config you chose. You can check your logs for the exact path.the &quot;root&quot; model links to the last saved step, but you can choose any step subfolder if needed (e.g. --model_path wmt17_en_de/transformer_big_bpe/step_10000)  eole predict --src wmt17_en_de/test.src.bpe --model_path wmt17_en_de/transformer_big_bpe --beam_size 5 --batch_size 4096 --batch_type tokens --output wmt17_en_de/pred.trg.bpe --gpu 0 sed -re 's/@@( |$)//g' &lt; wmt17_en_de/pred.trg.bpe &gt; wmt17_en_de/pred.trg.tok sacrebleu -tok none wmt17_en_de/test.trg &lt; wmt17_en_de/pred.trg.tok   BLEU scored at 40K, 45K, 50K steps on the test set (Newstest2016)  { &quot;name&quot;: &quot;BLEU&quot;, &quot;score&quot;: 35.4, &quot;signature&quot;: &quot;nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.0.0&quot;, &quot;verbose_score&quot;: &quot;66.2/41.3/28.5/20.3 (BP = 0.998 ratio = 0.998 hyp_len = 64244 ref_len = 64379)&quot;, &quot;nrefs&quot;: &quot;1&quot;, &quot;case&quot;: &quot;mixed&quot;, &quot;eff&quot;: &quot;no&quot;, &quot;tok&quot;: &quot;none&quot;, &quot;smooth&quot;: &quot;exp&quot;, &quot;version&quot;: &quot;2.0.0&quot; } { &quot;name&quot;: &quot;BLEU&quot;, &quot;score&quot;: 35.2, &quot;signature&quot;: &quot;nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.0.0&quot;, &quot;verbose_score&quot;: &quot;65.9/41.0/28.3/20.2 (BP = 1.000 ratio = 1.000 hyp_len = 64357 ref_len = 64379)&quot;, &quot;nrefs&quot;: &quot;1&quot;, &quot;case&quot;: &quot;mixed&quot;, &quot;eff&quot;: &quot;no&quot;, &quot;tok&quot;: &quot;none&quot;, &quot;smooth&quot;: &quot;exp&quot;, &quot;version&quot;: &quot;2.0.0&quot; } { &quot;name&quot;: &quot;BLEU&quot;, &quot;score&quot;: 35.1, &quot;signature&quot;: &quot;nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.0.0&quot;, &quot;verbose_score&quot;: &quot;66.2/41.2/28.4/20.3 (BP = 0.992 ratio = 0.992 hyp_len = 63885 ref_len = 64379)&quot;, &quot;nrefs&quot;: &quot;1&quot;, &quot;case&quot;: &quot;mixed&quot;, &quot;eff&quot;: &quot;no&quot;, &quot;tok&quot;: &quot;none&quot;, &quot;smooth&quot;: &quot;exp&quot;, &quot;version&quot;: &quot;2.0.0&quot; }  ","version":"Next","tagName":"h3"},{"title":"TowerInstruct-(Llama2)","type":0,"sectionRef":"#","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/wmt22_with_TowerInstruct-llama2).​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipeswmt22_with_towerinstruct-llama2","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#download-and-convert-model","content":" eole convert HF --model_dir Unbabel/TowerInstruct-7B-v0.2 --output $EOLE_MODEL_DIR/TowerInstruct-7b-v0.2 --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Build the prompt for translation of newstest2022-src.en​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#build-the-prompt-for-translation-of-newstest2022-srcen","content":" python promptize_llama2.py   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#run-inference","content":" eole predict -c tower-inference.yaml -src newstest2022-src-prompt.en -output newstest2022-hyp.de   Then you can score newstest2022-hyp.de against newstest2022-ref.de with a scorer (sacrebleu or comet) or just use cometkiwi for reference-less score. ","version":"Next","tagName":"h3"},{"title":"Llama3","type":0,"sectionRef":"#","url":"/eole/docs/recipes/wmt22_with_llama3.1/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/wmt22_with_llama3.1).​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipeswmt22_with_llama31","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#download-and-convert-model","content":" eole convert HF --model_dir meta-llama/Meta-Llama-3.1-8B --output $EOLE_MODEL_DIR/llama3.1-8b --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Build the prompt for translation of newstest2022-src.en​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#build-the-prompt-for-translation-of-newstest2022-srcen","content":" python promptize_llama3.py   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#run-inference","content":" eole predict -c llama-instruct-inference.yaml -src newstest2022-src-prompt.en -output newstest2022-hyp.de   Then you can score newstest2022-hyp.de against newstest2022-ref.de with a scorer (sacrebleu or comet) or just use cometkiwi for reference-less score. ","version":"Next","tagName":"h3"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/","content":"","keywords":"","version":"Next"},{"title":"TODO: add some example yaml configs here​","type":1,"pageTitle":"Configuration","url":"/eole/docs/reference/Config/#todo-add-some-example-yaml-configs-here","content":"","version":"Next","tagName":"h2"},{"title":"TowerInstruct (Mistral)","type":0,"sectionRef":"#","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/wmt22_with_TowerInstruct-Mistral).​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipeswmt22_with_towerinstruct-mistral","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#download-and-convert-model","content":" eole convert HF --model_dir Unbabel/TowerInstruct-Mistral-7B-v0.2 --output $EOLE_MODEL_DIR/TowerInstruct-Mistral-7b-v0.2 --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Build the prompt for translation of newstest2022-src.en​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#build-the-prompt-for-translation-of-newstest2022-srcen","content":" python promptize_mistral.py   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#run-inference","content":" eole predict -c tower-inference.yaml -src newstest2022-src-prompt.en -output newstest2022-hyp.de   Then you can score newstest2022-hyp.de against newstest2022-ref.de with a scorer (sacrebleu or comet) or just use cometkiwi for reference-less score. ","version":"Next","tagName":"h3"},{"title":"Bibliography","type":0,"sectionRef":"#","url":"/eole/docs/reference/bibliography","content":"Bibliography Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation By Jointly Learning To Align and Translate.” In ICLR, 1–15.https://doi.org/10.1146/annurev.neuro.26.041002.131047. Bowman, Samuel R., Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Józefowicz, and Samy Bengio. 2016. “Generating Sentences from a Continuous Space.” In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, 10–21.http://aclweb.org/anthology/K/K16/K16-1002.pdf. Bulte, Bram, and Arda Tezcan. 2019. “Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 1800–1809. Florence, Italy: Association for Computational Linguistics.https://doi.org/10.18653/v1/P19-1175. Chan, William, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals. 2015. “Listen, Attend and Spell.” CoRR abs/1508.01211.http://arxiv.org/abs/1508.01211. Cho, Kyunghyun, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation.” In Proc of EMNLP. Chopra, Sumit, Michael Auli, Alexander M Rush, and SEAS Harvard. 2016. “Abstractive Sentence Summarization with Attentive Recurrent Neural Networks.” Proceedings of NAACL-HLT16, 93–98. Chung, Junyoung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.” arXiv Preprint arXiv:1412.3555. Crego, Josep, Jungi Kim, and Jean Senellart. 2016. “SYSTRAN’s Pure Neural Machine Translation System.” arXiv Preprint arXiv:1602.06023. Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, et al. 2012. “Large Scale Distributed Deep Networks.” In Advances in Neural Information Processing Systems, 1223–31. Deng, Yuntian, Anssi Kanervisto, and Alexander M. Rush. 2016. “What You Get Is What You See: A Visual Markup Decompiler.” CoRR abs/1609.04938.http://arxiv.org/abs/1609.04938. Dyer, Chris, Jonathan Weese, Hendra Setiawan, Adam Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitkevitch, Phil Blunsom, and Philip Resnik. 2010. “Cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models.” In Proc ACL, 7–12. Association for Computational Linguistics. Garg, Sarthak, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik. 2019. “Jointly Learning to Align and Translate with Transformer Models.” In Conference on Empirical Methods in Natural Language Processing (EMNLP). Hong Kong. https://arxiv.org/abs/1909.02074. Gehring, Jonas, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. “Convolutional Sequence to Sequence Learning.” CoRRabs/1705.03122. http://arxiv.org/abs/1705.03122. Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80. Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, et al. 2007. “Moses: Open Source Toolkit for Statistical Machine Translation.” In Proc ACL, 177–80. Association for Computational Linguistics. Lei, Tao, Yu Zhang, and Yoav Artzi. 2017. “Training RNNs as Fast as CNNs.” CoRR abs/1709.02755. http://arxiv.org/abs/1709.02755. Léonard, Nicholas, Sagar Waghmare, Yang Wang, and Jin-Hwa Kim. 2015. “Rnn : Recurrent Library for Torch.” CoRR abs/1511.07889.http://arxiv.org/abs/1511.07889. Li, Yujia, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. “Gated Graph Sequence Neural Networks.” In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. http://arxiv.org/abs/1511.05493. Liu, Yang, and Mirella Lapata. 2017. “Learning Structured Text Representations.” CoRR abs/1705.09207.http://arxiv.org/abs/1705.09207. Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” InProc of EMNLP. Luong, Minh-Thang, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba. 2015. “Addressing the Rare Word Problem in Neural Machine Translation.” In Proc of ACL. Martins, André F. T., and Ramón Fernández Astudillo. 2016. “From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification.” CoRR abs/1602.02068.http://arxiv.org/abs/1602.02068. Martins, André FT, and Ramón Fernandez Astudillo. 2016. “From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification.”arXiv Preprint arXiv:1602.02068. Neubig, G. 2017. “Neural Machine Translation and Sequence-to-sequence Models: A Tutorial.” ArXiv e-Prints, March. https://arxiv.org/abs/1703.01619. Neubig, Graham. 2013. “Travatar: A Forest-to-String Machine Translation Engine Based on Tree Transducers.” In Proc ACL. Sofia, Bulgaria. See, Abigail, Peter J. Liu, and Christopher D. Manning. 2017. “Get to the Point: Summarization with Pointer-Generator Networks.” CoRRabs/1704.04368. http://arxiv.org/abs/1704.04368. Sennrich, Rico, and Barry Haddow. 2016. “Linguistic Input Features Improve Neural Machine Translation.” arXiv Preprint arXiv:1606.02892. Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2015. “Neural Machine Translation of Rare Words with Subword Units.” CoRR abs/1508.07909.http://arxiv.org/abs/1508.07909. Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In NIPS, 9. http://arxiv.org/abs/1409.3215. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” CoRR abs/1706.03762.http://arxiv.org/abs/1706.03762. Vinyals, Oriol, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. 2016. “Matching Networks for One Shot Learning.” InAdvances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, 3630–38.http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning. Vinyals, Oriol, and Quoc Le. 2015. “A Neural Conversational Model.”arXiv Preprint arXiv:1506.05869. Wang, Qiang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. “Learning Deep Transformer Models for Machine Translation.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 1810–22. Florence, Italy: Association for Computational Linguistics.https://doi.org/10.18653/v1/P19-1176. Wang, Xinyi, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. “SwitchOut: An Efficient Data Augmentation Algorithm for Neural Machine Translation.” CoRR abs/1808.07512. http://arxiv.org/abs/1808.07512. Weston, Jason, Sumit Chopra, and Antoine Bordes. 2014. “Memory Networks.” CoRR abs/1410.3916. http://arxiv.org/abs/1410.3916. Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, et al. 2016. “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation.” arXiv Preprint arXiv:1609.08144. Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015a. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.”CoRR abs/1502.03044. http://arxiv.org/abs/1502.03044. Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015b. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” ICML, February.http://arxiv.org/abs/1502.03044. Yang, Zichao, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. “Hierarchical Attention Networks for Document Classification.” In Proc ACL. Zhang, Biao, Deyi Xiong, and Jinsong Su. 2018. “Accelerating Neural Transformer via an Average Attention Network.” CoRR abs/1805.00631.http://arxiv.org/abs/1805.00631.","keywords":"","version":"Next"},{"title":"Inference","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/inference","content":"","keywords":"","version":"Next"},{"title":"pydantic model eole.config.inference.DecodingConfig​","type":1,"pageTitle":"Inference","url":"/eole/docs/reference/Config/inference#pydantic-model-eoleconfiginferencedecodingconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;DecodingConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;beam_size&quot;: { &quot;default&quot;: 5, &quot;description&quot;: &quot;Beam size.&quot;, &quot;title&quot;: &quot;Beam Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;ratio&quot;: { &quot;default&quot;: -0.0, &quot;description&quot;: &quot;Ratio based beam stop condition.&quot;, &quot;title&quot;: &quot;Ratio&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;top_k&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Set this to -1 to do random sampling from full distribution. Set this to value k&gt;1 to do random sampling restricted to the k most likely next tokens. Set this to 1 to use argmax.&quot;, &quot;title&quot;: &quot;Top K&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;top_p&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Probability for top-p/nucleus sampling. Restrict tokens to the most likely until the cumulated probability is over p. In range [0,1]. (https://arxiv.org/abs/1904.09751)&quot;, &quot;lte&quot;: 1.0, &quot;minimum&quot;: 0.0, &quot;title&quot;: &quot;Top P&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;temperature&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;If doing random sampling, divide the logits by this before computing softmax during decoding.&quot;, &quot;title&quot;: &quot;Temperature&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;length_penalty&quot;: { &quot;default&quot;: &quot;avg&quot;, &quot;description&quot;: &quot;Length penalty to use.&quot;, &quot;enum&quot;: [ &quot;avg&quot;, &quot;wu&quot;, &quot;none&quot; ], &quot;title&quot;: &quot;Length Penalty&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;alpha&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Length penalty parameter (higher = longer generation)&quot;, &quot;title&quot;: &quot;Alpha&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;coverage_penalty&quot;: { &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Coverage penalty to use. Only available in beam search.&quot;, &quot;enum&quot;: [ &quot;none&quot;, &quot;wu&quot;, &quot;summary&quot; ], &quot;title&quot;: &quot;Coverage Penalty&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;beta&quot;: { &quot;default&quot;: -0.0, &quot;description&quot;: &quot;Coverage penalty parameter.&quot;, &quot;title&quot;: &quot;Beta&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;stepwise_penalty&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Apply coverage penalty at every decoding step. Helpful for summary penalty.&quot;, &quot;title&quot;: &quot;Stepwise Penalty&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;min_length&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Minimum prediction length.&quot;, &quot;minimum&quot;: 0, &quot;title&quot;: &quot;Min Length&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;max_length&quot;: { &quot;default&quot;: 250, &quot;description&quot;: &quot;Maximum prediction length.&quot;, &quot;title&quot;: &quot;Max Length&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;max_length_ratio&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Maximum prediction length ratio. For European languages, 2 is large enough, for target Asian charageters, need to increase to 2-3, for special languages (Burmese, Amharic) to 10.&quot;, &quot;minimum&quot;: 1, &quot;title&quot;: &quot;Max Length Ratio&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;block_ngram_repeat&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Block repetition of ngrams during decoding.&quot;, &quot;title&quot;: &quot;Block Ngram Repeat&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;ignore_when_blocking&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;Ignore these strings when blocking repeats. You want to block sentence delimiters.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Ignore When Blocking&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;replace_unk&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Replace the generated UNK tokens with the source token that had the highest attention weight. If phrase_table is provided, it will lok up the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table), then it will copy the source token.&quot;, &quot;title&quot;: &quot;Replace Unk&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ban_unk_token&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Prevent unk token generation by setting unk probability to 0.&quot;, &quot;title&quot;: &quot;Ban Unk Token&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;phrase_table&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;If phrase_table is provided (with replace_unk), it will look up the identified source token and give the corresponding target token.&quot;, &quot;title&quot;: &quot;Phrase Table&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;n_best&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Output the n_best decoded sentences.&quot;, &quot;title&quot;: &quot;N Best&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;dump_beam&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;File to dump beam information to.&quot;, &quot;title&quot;: &quot;Dump Beam&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;verbose&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print scores and predictions for each input.&quot;, &quot;title&quot;: &quot;Verbose&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;with_score&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add a tab separated score to each output.&quot;, &quot;title&quot;: &quot;With Score&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estim_only&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Process the input to estimator only (no decoder).&quot;, &quot;title&quot;: &quot;Estim Only&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;attn_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print best attn for each word.&quot;, &quot;title&quot;: &quot;Attn Debug&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;align_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print best align for each word.&quot;, &quot;title&quot;: &quot;Align Debug&quot;, &quot;type&quot;: &quot;boolean&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: align_debug (bool)alpha (float)attn_debug (bool)ban_unk_token (bool)beam_size (int)beta (float)block_ngram_repeat (int)coverage_penalty (Literal['none', 'wu', 'summary'])dump_beam (str)estim_only (bool)ignore_when_blocking (List[str])length_penalty (Literal['avg', 'wu', 'none'])max_length (int)max_length_ratio (float)min_length (int)n_best (int)phrase_table (str)ratio (float)replace_unk (bool)stepwise_penalty (bool)temperature (float)top_k (int)top_p (float)verbose (bool)with_score (bool)  field align_debug : bool = False​  Print best align for each word.  field alpha : float = 1.0​  Length penalty parameter (higher = longer generation)  field attn_debug : bool = False​  Print best attn for each word.  field ban_unk_token : bool = False​  Prevent unk token generation by setting unk probability to 0.  field beam_size : int = 5​  Beam size.  field beta : float = -0.0​  Coverage penalty parameter.  field block_ngram_repeat : int = 0​  Block repetition of ngrams during decoding.  field coverage_penalty : Literal['none', 'wu', 'summary'] = 'none'​  Coverage penalty to use. Only available in beam search.  field dump_beam : str = ''​  File to dump beam information to.  field estim_only : bool = False​  Process the input to estimator only (no decoder).  field ignore_when_blocking : List[str] = []​  Ignore these strings when blocking repeats. You want to block sentence delimiters.  field length_penalty : Literal['avg', 'wu', 'none'] = 'avg'​  Length penalty to use.  field max_length : int = 250​  Maximum prediction length.  field max_length_ratio : float = 2​  Maximum prediction length ratio. For European languages, 2 is large enough, for target Asian charageters, need to increase to 2-3, for special languages (Burmese, Amharic) to 10.  Constraints: ge = 1  field min_length : int = 0​  Minimum prediction length.  Constraints: ge = 0  field n_best : int = 1​  Output the n_best decoded sentences.  field phrase_table : str = ''​  If phrase_table is provided (with replace_unk), it will look up the identified source token and give the corresponding target token.  field ratio : float = -0.0​  Ratio based beam stop condition.  field replace_unk : bool = False​  Replace the generated UNK tokens with the source token that had the highest attention weight. If phrase_table is provided, it will lok up the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table), then it will copy the source token.  field stepwise_penalty : bool = False​  Apply coverage penalty at every decoding step. Helpful for summary penalty.  field temperature : float = 1.0​  If doing random sampling, divide the logits by this before computing softmax during decoding.  field top_k : int = 0​  Set this to -1 to do random sampling from full distribution. Set this to value k&gt;1 to do random sampling restricted to the k most likely next tokens. Set this to 1 to use argmax.  field top_p : float = 0.0​  Probability for top-p/nucleus sampling. Restrict tokens to the most likely until the cumulated probability is over p. In range [0,1]. (https://arxiv.org/abs/1904.09751)  Constraints: ge = 0.0  field verbose : bool = False​  Print scores and predictions for each input.  field with_score : bool = False​  Add a tab separated score to each output.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.inference.InferenceConfig​","type":1,"pageTitle":"Inference","url":"/eole/docs/reference/Config/inference#pydantic-model-eoleconfiginferenceinferenceconfig","content":" Bases: RunningConfig, DecodingConfig, LoRaConfig, QuantizeConfig   Show JSON schema { &quot;title&quot;: &quot;InferenceConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;quant_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be compressed in 4/8bit.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Quant Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;quant_type&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Type of compression.&quot;, &quot;enum&quot;: [ &quot;&quot;, &quot;bnb_8bit&quot;, &quot;bnb_FP4&quot;, &quot;bnb_NF4&quot;, &quot;awq_gemm&quot;, &quot;awq_gemv&quot; ], &quot;title&quot;: &quot;Quant Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;w_bit&quot;: { &quot;default&quot;: 4, &quot;description&quot;: &quot;W_bit quantization&quot;, &quot;title&quot;: &quot;W Bit&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;group_size&quot;: { &quot;default&quot;: 128, &quot;description&quot;: &quot;Group size quantization.&quot;, &quot;title&quot;: &quot;Group Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be replaced by LoRa layers. E.g. ['linear_values', 'linear_query'] (\\u00a74.2 in https://arxiv.org/abs/2106.09685)&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Lora Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;lora_embedding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Replace embeddings with LoRa Embeddings (\\u00a75.1)&quot;, &quot;title&quot;: &quot;Lora Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lora_rank&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;r=2 successfully tested with NLLB-200 3.3B&quot;, &quot;title&quot;: &quot;Lora Rank&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_alpha&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;\\u00a74.1 https://arxiv.org/abs/2106.09685&quot;, &quot;title&quot;: &quot;Lora Alpha&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_dropout&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rule of thumb: same value as in main model.&quot;, &quot;title&quot;: &quot;Lora Dropout&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;beam_size&quot;: { &quot;default&quot;: 5, &quot;description&quot;: &quot;Beam size.&quot;, &quot;title&quot;: &quot;Beam Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;ratio&quot;: { &quot;default&quot;: -0.0, &quot;description&quot;: &quot;Ratio based beam stop condition.&quot;, &quot;title&quot;: &quot;Ratio&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;top_k&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Set this to -1 to do random sampling from full distribution. Set this to value k&gt;1 to do random sampling restricted to the k most likely next tokens. Set this to 1 to use argmax.&quot;, &quot;title&quot;: &quot;Top K&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;top_p&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Probability for top-p/nucleus sampling. Restrict tokens to the most likely until the cumulated probability is over p. In range [0,1]. (https://arxiv.org/abs/1904.09751)&quot;, &quot;lte&quot;: 1.0, &quot;minimum&quot;: 0.0, &quot;title&quot;: &quot;Top P&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;temperature&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;If doing random sampling, divide the logits by this before computing softmax during decoding.&quot;, &quot;title&quot;: &quot;Temperature&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;length_penalty&quot;: { &quot;default&quot;: &quot;avg&quot;, &quot;description&quot;: &quot;Length penalty to use.&quot;, &quot;enum&quot;: [ &quot;avg&quot;, &quot;wu&quot;, &quot;none&quot; ], &quot;title&quot;: &quot;Length Penalty&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;alpha&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Length penalty parameter (higher = longer generation)&quot;, &quot;title&quot;: &quot;Alpha&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;coverage_penalty&quot;: { &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Coverage penalty to use. Only available in beam search.&quot;, &quot;enum&quot;: [ &quot;none&quot;, &quot;wu&quot;, &quot;summary&quot; ], &quot;title&quot;: &quot;Coverage Penalty&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;beta&quot;: { &quot;default&quot;: -0.0, &quot;description&quot;: &quot;Coverage penalty parameter.&quot;, &quot;title&quot;: &quot;Beta&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;stepwise_penalty&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Apply coverage penalty at every decoding step. Helpful for summary penalty.&quot;, &quot;title&quot;: &quot;Stepwise Penalty&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;min_length&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Minimum prediction length.&quot;, &quot;minimum&quot;: 0, &quot;title&quot;: &quot;Min Length&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;max_length&quot;: { &quot;default&quot;: 250, &quot;description&quot;: &quot;Maximum prediction length.&quot;, &quot;title&quot;: &quot;Max Length&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;max_length_ratio&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Maximum prediction length ratio. For European languages, 2 is large enough, for target Asian charageters, need to increase to 2-3, for special languages (Burmese, Amharic) to 10.&quot;, &quot;minimum&quot;: 1, &quot;title&quot;: &quot;Max Length Ratio&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;block_ngram_repeat&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Block repetition of ngrams during decoding.&quot;, &quot;title&quot;: &quot;Block Ngram Repeat&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;ignore_when_blocking&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;Ignore these strings when blocking repeats. You want to block sentence delimiters.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Ignore When Blocking&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;replace_unk&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Replace the generated UNK tokens with the source token that had the highest attention weight. If phrase_table is provided, it will lok up the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table), then it will copy the source token.&quot;, &quot;title&quot;: &quot;Replace Unk&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ban_unk_token&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Prevent unk token generation by setting unk probability to 0.&quot;, &quot;title&quot;: &quot;Ban Unk Token&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;phrase_table&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;If phrase_table is provided (with replace_unk), it will look up the identified source token and give the corresponding target token.&quot;, &quot;title&quot;: &quot;Phrase Table&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;n_best&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Output the n_best decoded sentences.&quot;, &quot;title&quot;: &quot;N Best&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;dump_beam&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;File to dump beam information to.&quot;, &quot;title&quot;: &quot;Dump Beam&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;verbose&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print scores and predictions for each input.&quot;, &quot;title&quot;: &quot;Verbose&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;with_score&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add a tab separated score to each output.&quot;, &quot;title&quot;: &quot;With Score&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estim_only&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Process the input to estimator only (no decoder).&quot;, &quot;title&quot;: &quot;Estim Only&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;attn_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print best attn for each word.&quot;, &quot;title&quot;: &quot;Attn Debug&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;align_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print best align for each word.&quot;, &quot;title&quot;: &quot;Align Debug&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;gpu_ranks&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of ranks for each process.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Gpu Ranks&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;world_size&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Total number of distributed processes.&quot;, &quot;title&quot;: &quot;World Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;parallel_mode&quot;: { &quot;default&quot;: &quot;data_parallel&quot;, &quot;description&quot;: &quot;Distributed mode.&quot;, &quot;enum&quot;: [ &quot;data_parallel&quot;, &quot;tensor_parallel&quot; ], &quot;title&quot;: &quot;Parallel Mode&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_backend&quot;: { &quot;default&quot;: &quot;nccl&quot;, &quot;description&quot;: &quot;Type of torch distributed backend.&quot;, &quot;title&quot;: &quot;Gpu Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_verbose_level&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Gives more info on each process per GPU.&quot;, &quot;title&quot;: &quot;Gpu Verbose Level&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;master_ip&quot;: { &quot;default&quot;: &quot;localhost&quot;, &quot;description&quot;: &quot;IP of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Ip&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;master_port&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Port of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Port&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;timeout&quot;: { &quot;default&quot;: 60, &quot;description&quot;: &quot;Timeout for one GPU to wait for the others.&quot;, &quot;title&quot;: &quot;Timeout&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;model_path&quot;: { &quot;default&quot;: &quot;model&quot;, &quot;description&quot;: &quot;Path to directory containing all model components.&quot;, &quot;title&quot;: &quot;Model Path&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;self_attn_backend&quot;: { &quot;default&quot;: &quot;flash&quot;, &quot;description&quot;: &quot;Self-attention backend.&quot;, &quot;enum&quot;: [ &quot;flash&quot;, &quot;pytorch&quot; ], &quot;title&quot;: &quot;Self Attn Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;compute_dtype&quot;: { &quot;description&quot;: &quot;Compute dtype (precision) to use for main compute. Some parameters might have other dtypes for specific cases (e.g. torch.amp -- See eole.config.training.TrainingConfig.storage_dtype) fp32 to force slow fp16 model on gtx1080, int8 to enable pytorch native 8-bit quantization (cpu only).&quot;, &quot;enum&quot;: [ &quot;fp32&quot;, &quot;fp16&quot;, &quot;int8&quot;, &quot;bf16&quot; ], &quot;title&quot;: &quot;Compute Dtype&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;torch_compile&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use torch.compile with dynamic=True.&quot;, &quot;title&quot;: &quot;Torch Compile&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;report_align&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report alignment for each translation.&quot;, &quot;title&quot;: &quot;Report Align&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;gold_align&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report alignment between source and gold target. Useful to test the performance of learnt alignments.&quot;, &quot;title&quot;: &quot;Gold Align&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;report_time&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report some translation time metrics.&quot;, &quot;title&quot;: &quot;Report Time&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;fuse_kvq&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Fuse K, V, Q Linear layers into a single KVQ in Self Attn.&quot;, &quot;title&quot;: &quot;Fuse Kvq&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;fuse_gate&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Fuse gate_up_proj and up_proj Linear layers into a single Linear.&quot;, &quot;title&quot;: &quot;Fuse Gate&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;profile&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report pytorch profiling stats.&quot;, &quot;title&quot;: &quot;Profile&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;batch_size&quot;: { &quot;default&quot;: 30, &quot;description&quot;: &quot;Batch size.&quot;, &quot;title&quot;: &quot;Batch Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;batch_type&quot;: { &quot;default&quot;: &quot;sents&quot;, &quot;description&quot;: &quot;Batch grouping for batch size.&quot;, &quot;enum&quot;: [ &quot;sents&quot;, &quot;tokens&quot; ], &quot;title&quot;: &quot;Batch Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;avg_raw_probs&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;If set, during ensembling scores from different models will be combined by averaging their raw probabilities and then taking the log. Otherwise, the log probabilities will be averaged directly. Necessary for models whose output layers can assign zero probability.&quot;, &quot;title&quot;: &quot;Avg Raw Probs&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;data_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;text&quot;, &quot;title&quot;: &quot;Data Type&quot; }, &quot;chat_template&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Chat Template&quot; }, &quot;optional_eos&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;Optional EOS tokens that would stop generation, e.g. &lt;|eot_id|&gt; for Llama3&quot;, &quot;title&quot;: &quot;Optional Eos&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = ()arbitrary_types_allowed: bool = True Fields: avg_raw_probs (bool)batch_size (int)batch_type (Literal['sents', 'tokens'])chat_template (str | None)data_type (str | None)fuse_gate (bool)fuse_kvq (bool)gold_align (bool)optional_eos (List[str] | None)profile (bool)report_align (bool)report_time (bool) Validators: _validate_model_path » model_path_validate_running_config » all fields  field avg_raw_probs : bool = False​  If set, during ensembling scores from different models will be combined by averaging their raw probabilities and then taking the log. Otherwise, the log probabilities will be averaged directly. Necessary for models whose output layers can assign zero probability.  Validated by: _validate_running_config  field batch_size : int = 30​  Batch size.  Validated by: _validate_running_config  field batch_type : Literal['sents', 'tokens'] = 'sents'​  Batch grouping for batch size.  Validated by: _validate_running_config  field chat_template : str | None = None​  Validated by: _validate_running_config  field data_type : str | None = 'text'​  Validated by: _validate_running_config  field fuse_gate : bool = False​  Fuse gate_up_proj and up_proj Linear layers into a single Linear.  Validated by: _validate_running_config  field fuse_kvq : bool = False​  Fuse K, V, Q Linear layers into a single KVQ in Self Attn.  Validated by: _validate_running_config  field gold_align : bool = False​  Report alignment between source and gold target. Useful to test the performance of learnt alignments.  Validated by: _validate_running_config  field optional_eos : List[str] | None = []​  Optional EOS tokens that would stop generation, e.g. &lt;  |eot_id|   for Llama3  Validated by: _validate_running_config  field profile : bool = False​  Report pytorch profiling stats.  Validated by: _validate_running_config  field report_align : bool = False​  Report alignment for each translation.  Validated by: _validate_running_config  field report_time : bool = False​  Report some translation time metrics.  Validated by: _validate_running_config  get_model_path()​  property storage_dtype : dtype​  Deduce which dtype to use for main model parameters. ","version":"Next","tagName":"h3"},{"title":"Training","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/training","content":"","keywords":"","version":"Next"},{"title":"pydantic model eole.config.training.OptimizerConfig​","type":1,"pageTitle":"Training","url":"/eole/docs/reference/Config/training#pydantic-model-eoleconfigtrainingoptimizerconfig","content":" Bases: Config  Everything related to optimizers. Might be split into multiple subclasses later. Note: not fully sufficient (yet) to replace full opt namespace in build_torch_optimizer. Some other parameters (hidden_size, compute_dtype, etc.) are accessed.   Show JSON schema { &quot;title&quot;: &quot;OptimizerConfig&quot;, &quot;description&quot;: &quot;Everything related to optimizers.\\nMight be split into multiple subclasses later.\\nNote: not fully sufficient (yet) to replace full opt namespace in build_torch_optimizer.\\nSome other parameters (hidden_size, compute_dtype, etc.) are accessed.&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;optim&quot;: { &quot;default&quot;: &quot;sgd&quot;, &quot;description&quot;: &quot;Optimization method.&quot;, &quot;enum&quot;: [ &quot;sgd&quot;, &quot;adagrad&quot;, &quot;adadelta&quot;, &quot;adam&quot;, &quot;adamw&quot;, &quot;sparseadam&quot;, &quot;adafactor&quot;, &quot;adamw8bit&quot;, &quot;pagedadamw8bit&quot;, &quot;pagedadamw32bit&quot; ], &quot;title&quot;: &quot;Optim&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;adagrad_accumulator_init&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Initialize the accumulator values in adagrad. Mirrors initial_accumulator_value flag from tensorflow adagrad implementation (default 0.1 there).&quot;, &quot;title&quot;: &quot;Adagrad Accumulator Init&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;adam_beta1&quot;: { &quot;default&quot;: 0.9, &quot;description&quot;: &quot;Beta1 parameter used by Adam. Almost without exception a value of 0.9 is used in the literature, seemingly giving good results, so we would discourage changing this value from the default without due consideration.&quot;, &quot;title&quot;: &quot;Adam Beta1&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;adam_beta2&quot;: { &quot;default&quot;: 0.999, &quot;description&quot;: &quot;Beta2 parameter used by Adam. Typically a value of 0.999 is recommended, as this is the value suggested by the original paper describing Adam, and is also the value adopted in other frameworks such as Tensorflow (https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and Keras (https://keras.io/optimizers/). Whereas recently the paper Attention is All You Need suggested a value of 0.98 for beta2, this parameter may not work well for normal models / default baselines.&quot;, &quot;title&quot;: &quot;Adam Beta2&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;adam_eps&quot;: { &quot;default&quot;: 1e-08, &quot;description&quot;: &quot;Adam epsilon to forward to torch Optimizer.&quot;, &quot;title&quot;: &quot;Adam Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;weight_decay&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Weight decay to forward to torch Optimizer.&quot;, &quot;title&quot;: &quot;Weight Decay&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;use_amp&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Use torch mixed precision when compute_dtype is 16-bit.&quot;, &quot;title&quot;: &quot;Use Amp&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;learning_rate&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Starting learning rate. Recommended settings: sgd=1, adagrad=0.1, adadelta=1, adam=0.001.&quot;, &quot;title&quot;: &quot;Learning Rate&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;learning_rate_decay&quot;: { &quot;default&quot;: 0.5, &quot;description&quot;: &quot;Decay learning rate by this much if steps have gone past start_decay_steps.&quot;, &quot;title&quot;: &quot;Learning Rate Decay&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;start_decay_steps&quot;: { &quot;default&quot;: 50000, &quot;description&quot;: &quot;Start decaying every decay_steps after this many steps.&quot;, &quot;title&quot;: &quot;Start Decay Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;decay_steps&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Frequency for learning rate decay, in steps.&quot;, &quot;title&quot;: &quot;Decay Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;decay_method&quot;: { &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Custom decay method to use.&quot;, &quot;enum&quot;: [ &quot;noam&quot;, &quot;noamwd&quot;, &quot;cosine&quot;, &quot;rsqrt&quot;, &quot;none&quot; ], &quot;title&quot;: &quot;Decay Method&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;warmup_steps&quot;: { &quot;default&quot;: 4000, &quot;description&quot;: &quot;Number of warmup steps for custom decay.&quot;, &quot;title&quot;: &quot;Warmup Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;reset_optim&quot;: { &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Optimization resetter when using train_from.&quot;, &quot;enum&quot;: [ &quot;none&quot;, &quot;all&quot;, &quot;states&quot;, &quot;keep_states&quot; ], &quot;title&quot;: &quot;Reset Optim&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: adagrad_accumulator_init (float)adam_beta1 (float)adam_beta2 (float)adam_eps (float)decay_method (Literal['noam', 'noamwd', 'cosine', 'rsqrt', 'none'])decay_steps (int)learning_rate (float)learning_rate_decay (float)optim (Literal['sgd', 'adagrad', 'adadelta', 'adam', 'adamw', 'sparseadam', 'adafactor', 'adamw8bit', 'pagedadamw8bit', 'pagedadamw32bit'])reset_optim (Literal['none', 'all', 'states', 'keep_states'])start_decay_steps (int)use_amp (bool)warmup_steps (int)weight_decay (float)  field adagrad_accumulator_init : float = 0​  Initialize the accumulator values in adagrad. Mirrors initial_accumulator_value flag from tensorflow adagrad implementation (default 0.1 there).  field adam_beta1 : float = 0.9​  Beta1 parameter used by Adam. Almost without exception a value of 0.9 is used in the literature, seemingly giving good results, so we would discourage changing this value from the default without due consideration.  field adam_beta2 : float = 0.999​  Beta2 parameter used by Adam. Typically a value of 0.999 is recommended, as this is the value suggested by the original paper describing Adam, and is also the value adopted in other frameworks such as Tensorflow (https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and Keras (https://keras.io/optimizers/). Whereas recently the paper Attention is All You Need suggested a value of 0.98 for beta2, this parameter may not work well for normal models / default baselines.  field adam_eps : float = 1e-08​  Adam epsilon to forward to torch Optimizer.  field decay_method : Literal['noam', 'noamwd', 'cosine', 'rsqrt', 'none'] = 'none'​  Custom decay method to use.  field decay_steps : int = 10000​  Frequency for learning rate decay, in steps.  field learning_rate : float = 1.0​  Starting learning rate. Recommended settings: sgd=1, adagrad=0.1, adadelta=1, adam=0.001.  field learning_rate_decay : float = 0.5​  Decay learning rate by this much if steps have gone past start_decay_steps.  field optim : Literal['sgd', 'adagrad', 'adadelta', 'adam', 'adamw', 'sparseadam', 'adafactor', 'adamw8bit', 'pagedadamw8bit', 'pagedadamw32bit'] = 'sgd'​  Optimization method.  field reset_optim : Literal['none', 'all', 'states', 'keep_states'] = 'none'​  Optimization resetter when using train_from.  field start_decay_steps : int = 50000​  Start decaying every decay_steps after this many steps.  field use_amp : bool = True​  Use torch mixed precision when compute_dtype is 16-bit.  field warmup_steps : int = 4000​  Number of warmup steps for custom decay.  field weight_decay : float = 0.0​  Weight decay to forward to torch Optimizer.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.training.TrainingConfig​","type":1,"pageTitle":"Training","url":"/eole/docs/reference/Config/training#pydantic-model-eoleconfigtrainingtrainingconfig","content":" Bases: RunningConfig, OptimizerConfig, LoRaConfig, QuantizeConfig   Show JSON schema { &quot;title&quot;: &quot;TrainingConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;quant_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be compressed in 4/8bit.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Quant Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;quant_type&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Type of compression.&quot;, &quot;enum&quot;: [ &quot;&quot;, &quot;bnb_8bit&quot;, &quot;bnb_FP4&quot;, &quot;bnb_NF4&quot;, &quot;awq_gemm&quot;, &quot;awq_gemv&quot; ], &quot;title&quot;: &quot;Quant Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;w_bit&quot;: { &quot;default&quot;: 4, &quot;description&quot;: &quot;W_bit quantization&quot;, &quot;title&quot;: &quot;W Bit&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;group_size&quot;: { &quot;default&quot;: 128, &quot;description&quot;: &quot;Group size quantization.&quot;, &quot;title&quot;: &quot;Group Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be replaced by LoRa layers. E.g. ['linear_values', 'linear_query'] (\\u00a74.2 in https://arxiv.org/abs/2106.09685)&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Lora Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;lora_embedding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Replace embeddings with LoRa Embeddings (\\u00a75.1)&quot;, &quot;title&quot;: &quot;Lora Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lora_rank&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;r=2 successfully tested with NLLB-200 3.3B&quot;, &quot;title&quot;: &quot;Lora Rank&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_alpha&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;\\u00a74.1 https://arxiv.org/abs/2106.09685&quot;, &quot;title&quot;: &quot;Lora Alpha&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_dropout&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rule of thumb: same value as in main model.&quot;, &quot;title&quot;: &quot;Lora Dropout&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;optim&quot;: { &quot;default&quot;: &quot;sgd&quot;, &quot;description&quot;: &quot;Optimization method.&quot;, &quot;enum&quot;: [ &quot;sgd&quot;, &quot;adagrad&quot;, &quot;adadelta&quot;, &quot;adam&quot;, &quot;adamw&quot;, &quot;sparseadam&quot;, &quot;adafactor&quot;, &quot;adamw8bit&quot;, &quot;pagedadamw8bit&quot;, &quot;pagedadamw32bit&quot; ], &quot;title&quot;: &quot;Optim&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;adagrad_accumulator_init&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Initialize the accumulator values in adagrad. Mirrors initial_accumulator_value flag from tensorflow adagrad implementation (default 0.1 there).&quot;, &quot;title&quot;: &quot;Adagrad Accumulator Init&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;adam_beta1&quot;: { &quot;default&quot;: 0.9, &quot;description&quot;: &quot;Beta1 parameter used by Adam. Almost without exception a value of 0.9 is used in the literature, seemingly giving good results, so we would discourage changing this value from the default without due consideration.&quot;, &quot;title&quot;: &quot;Adam Beta1&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;adam_beta2&quot;: { &quot;default&quot;: 0.999, &quot;description&quot;: &quot;Beta2 parameter used by Adam. Typically a value of 0.999 is recommended, as this is the value suggested by the original paper describing Adam, and is also the value adopted in other frameworks such as Tensorflow (https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and Keras (https://keras.io/optimizers/). Whereas recently the paper Attention is All You Need suggested a value of 0.98 for beta2, this parameter may not work well for normal models / default baselines.&quot;, &quot;title&quot;: &quot;Adam Beta2&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;adam_eps&quot;: { &quot;default&quot;: 1e-08, &quot;description&quot;: &quot;Adam epsilon to forward to torch Optimizer.&quot;, &quot;title&quot;: &quot;Adam Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;weight_decay&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Weight decay to forward to torch Optimizer.&quot;, &quot;title&quot;: &quot;Weight Decay&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;use_amp&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Use torch mixed precision when compute_dtype is 16-bit.&quot;, &quot;title&quot;: &quot;Use Amp&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;learning_rate&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Starting learning rate. Recommended settings: sgd=1, adagrad=0.1, adadelta=1, adam=0.001.&quot;, &quot;title&quot;: &quot;Learning Rate&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;learning_rate_decay&quot;: { &quot;default&quot;: 0.5, &quot;description&quot;: &quot;Decay learning rate by this much if steps have gone past start_decay_steps.&quot;, &quot;title&quot;: &quot;Learning Rate Decay&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;start_decay_steps&quot;: { &quot;default&quot;: 50000, &quot;description&quot;: &quot;Start decaying every decay_steps after this many steps.&quot;, &quot;title&quot;: &quot;Start Decay Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;decay_steps&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Frequency for learning rate decay, in steps.&quot;, &quot;title&quot;: &quot;Decay Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;decay_method&quot;: { &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Custom decay method to use.&quot;, &quot;enum&quot;: [ &quot;noam&quot;, &quot;noamwd&quot;, &quot;cosine&quot;, &quot;rsqrt&quot;, &quot;none&quot; ], &quot;title&quot;: &quot;Decay Method&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;warmup_steps&quot;: { &quot;default&quot;: 4000, &quot;description&quot;: &quot;Number of warmup steps for custom decay.&quot;, &quot;title&quot;: &quot;Warmup Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;reset_optim&quot;: { &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Optimization resetter when using train_from.&quot;, &quot;enum&quot;: [ &quot;none&quot;, &quot;all&quot;, &quot;states&quot;, &quot;keep_states&quot; ], &quot;title&quot;: &quot;Reset Optim&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_ranks&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of ranks for each process.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Gpu Ranks&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;world_size&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Total number of distributed processes.&quot;, &quot;title&quot;: &quot;World Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;parallel_mode&quot;: { &quot;default&quot;: &quot;data_parallel&quot;, &quot;description&quot;: &quot;Distributed mode.&quot;, &quot;enum&quot;: [ &quot;data_parallel&quot;, &quot;tensor_parallel&quot; ], &quot;title&quot;: &quot;Parallel Mode&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_backend&quot;: { &quot;default&quot;: &quot;nccl&quot;, &quot;description&quot;: &quot;Type of torch distributed backend.&quot;, &quot;title&quot;: &quot;Gpu Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_verbose_level&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Gives more info on each process per GPU.&quot;, &quot;title&quot;: &quot;Gpu Verbose Level&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;master_ip&quot;: { &quot;default&quot;: &quot;localhost&quot;, &quot;description&quot;: &quot;IP of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Ip&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;master_port&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Port of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Port&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;timeout&quot;: { &quot;default&quot;: 60, &quot;description&quot;: &quot;Timeout for one GPU to wait for the others.&quot;, &quot;title&quot;: &quot;Timeout&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;model_path&quot;: { &quot;default&quot;: &quot;model&quot;, &quot;description&quot;: &quot;Path to directory containing all model components.&quot;, &quot;title&quot;: &quot;Model Path&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;self_attn_backend&quot;: { &quot;default&quot;: &quot;flash&quot;, &quot;description&quot;: &quot;Self-attention backend.&quot;, &quot;enum&quot;: [ &quot;flash&quot;, &quot;pytorch&quot; ], &quot;title&quot;: &quot;Self Attn Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;compute_dtype&quot;: { &quot;description&quot;: &quot;Compute dtype (precision) to use for main compute. Some parameters might have other dtypes for specific cases (e.g. torch.amp -- See eole.config.training.TrainingConfig.storage_dtype) fp32 to force slow fp16 model on gtx1080, int8 to enable pytorch native 8-bit quantization (cpu only).&quot;, &quot;enum&quot;: [ &quot;fp32&quot;, &quot;fp16&quot;, &quot;int8&quot;, &quot;bf16&quot; ], &quot;title&quot;: &quot;Compute Dtype&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;torch_compile&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use torch.compile with dynamic=True.&quot;, &quot;title&quot;: &quot;Torch Compile&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;param_init&quot;: { &quot;default&quot;: 0.1, &quot;description&quot;: &quot;Support value for uniform distribution parameters initialization. Set to 0 not to use initialization.&quot;, &quot;title&quot;: &quot;Param Init&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;param_init_method&quot;: { &quot;default&quot;: &quot;uniform&quot;, &quot;description&quot;: &quot;Parameter initialization method.&quot;, &quot;enum&quot;: [ &quot;xavier_uniform&quot;, &quot;uniform&quot;, &quot;normal&quot; ], &quot;title&quot;: &quot;Param Init Method&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;freeze_encoder&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze parameters in encoder.&quot;, &quot;title&quot;: &quot;Freeze Encoder&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_decoder&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze parameters in decoder.&quot;, &quot;title&quot;: &quot;Freeze Decoder&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;pre_word_vecs_enc&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;If a valid path is specified, will load pretrained word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Pre Word Vecs Enc&quot; }, &quot;pre_word_vecs_dec&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;If a valid path is specified, will load pretrained word embeddings on the decoder side.&quot;, &quot;title&quot;: &quot;Pre Word Vecs Dec&quot; }, &quot;data_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;text&quot;, &quot;title&quot;: &quot;Data Type&quot; }, &quot;bucket_size&quot;: { &quot;default&quot;: 262144, &quot;description&quot;: &quot;A bucket is a buffer of bucket_size examples to pick from the various corpora. The dynamic iterator batches batch_size items from the bucket and shuffle them.&quot;, &quot;title&quot;: &quot;Bucket Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bucket_size_init&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Bucket size is initialized with this amount of examples (see bucket_size_increment).&quot;, &quot;title&quot;: &quot;Bucket Size Init&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bucket_size_increment&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Bucket size incremented with this amount of examples at each new bucket (up to bucket_size).&quot;, &quot;title&quot;: &quot;Bucket Size Increment&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;prefetch_factor&quot;: { &quot;default&quot;: 200, &quot;description&quot;: &quot;Number of mini-batches loaded in advance to avoid the GPU waiting during processing of next bucket.&quot;, &quot;title&quot;: &quot;Prefetch Factor&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;save_format&quot;: { &quot;default&quot;: &quot;pytorch&quot;, &quot;description&quot;: &quot;Format to save the model weights.&quot;, &quot;enum&quot;: [ &quot;pytorch&quot;, &quot;safetensors&quot; ], &quot;title&quot;: &quot;Save Format&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;save_checkpoint_steps&quot;: { &quot;default&quot;: 5000, &quot;description&quot;: &quot;Frequency of checkpoint saving (in steps).&quot;, &quot;title&quot;: &quot;Save Checkpoint Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;keep_checkpoint&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of checkpoints to retain. (-1 retains all)&quot;, &quot;title&quot;: &quot;Keep Checkpoint&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;train_from&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Pretrained model/checkpoint weights to continue training from.&quot;, &quot;title&quot;: &quot;Train From&quot; }, &quot;num_workers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of workers for pytorch.DataLoader objects.&quot;, &quot;title&quot;: &quot;Num Workers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;batch_size&quot;: { &quot;default&quot;: 64, &quot;description&quot;: &quot;Maximum batch size for training.&quot;, &quot;title&quot;: &quot;Batch Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;batch_size_multiple&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Batch size multiple for token batches.&quot;, &quot;title&quot;: &quot;Batch Size Multiple&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;batch_type&quot;: { &quot;default&quot;: &quot;sents&quot;, &quot;description&quot;: &quot;Batch grouping for batch_size.&quot;, &quot;enum&quot;: [ &quot;sents&quot;, &quot;tokens&quot; ], &quot;title&quot;: &quot;Batch Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;normalization&quot;: { &quot;default&quot;: &quot;sents&quot;, &quot;description&quot;: &quot;Normalization method of the gradient.&quot;, &quot;enum&quot;: [ &quot;sents&quot;, &quot;tokens&quot; ], &quot;title&quot;: &quot;Normalization&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;accum_count&quot;: { &quot;default&quot;: [ 1 ], &quot;description&quot;: &quot;Accumulate gradient this many times. Approximately equivalent to updating batch_size * accum_count batches at once. Recommended for transformer.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Accum Count&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;accum_steps&quot;: { &quot;default&quot;: [ 0 ], &quot;description&quot;: &quot;Steps at which accum_count values change.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Accum Steps&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;valid_steps&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Frequency of validation, in steps.&quot;, &quot;title&quot;: &quot;Valid Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;valid_batch_size&quot;: { &quot;default&quot;: 32, &quot;description&quot;: &quot;Maximum batch size for validation.&quot;, &quot;title&quot;: &quot;Valid Batch Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;train_steps&quot;: { &quot;default&quot;: 100000, &quot;description&quot;: &quot;Number of training steps.&quot;, &quot;title&quot;: &quot;Train Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;single_pass&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Make a single pass over the training dataset.&quot;, &quot;title&quot;: &quot;Single Pass&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;early_stopping&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of validation steps without improving that will trigger early stop of training.&quot;, &quot;title&quot;: &quot;Early Stopping&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;early_stopping_criteria&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Criteria to use for early stopping.&quot;, &quot;title&quot;: &quot;Early Stopping Criteria&quot; }, &quot;max_grad_norm&quot;: { &quot;default&quot;: 5, &quot;description&quot;: &quot;If the norm of the gradient vector exceeds this value, renormalize it to have the norm equal to max_grad_norm.&quot;, &quot;title&quot;: &quot;Max Grad Norm&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;dropout&quot;: { &quot;default&quot;: [ 0.3 ], &quot;description&quot;: &quot;Dropout probability.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;title&quot;: &quot;Dropout&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;attention_dropout&quot;: { &quot;default&quot;: [ 0.1 ], &quot;description&quot;: &quot;Attention dropout probability.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;title&quot;: &quot;Attention Dropout&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;dropout_steps&quot;: { &quot;default&quot;: [ 0 ], &quot;description&quot;: &quot;Steps at which dropout changes.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Dropout Steps&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;label_smoothing&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Label smoothing value epsilon. Probability of all non-true labels will be smoothed by epsilon/(vocab_size-1). Set to 0 to turn off label smoothing. (https://arxiv.org/abs/1512.00567)&quot;, &quot;title&quot;: &quot;Label Smoothing&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;average_decay&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Exponential moving average decay (https://en.wikipedia.org/wiki/Moving_average). Set to other than 0 (e.g. 1e-4) to activate. Similar to Marian NMT implementation (http://www.aclweb.org/anthology/P18-4020).&quot;, &quot;title&quot;: &quot;Average Decay&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;average_every&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Step for moving average. Default is every update if average_decay is set.&quot;, &quot;title&quot;: &quot;Average Every&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;zero_out_prompt_loss&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Set the prompt loss to zero. Mostly for LLM finetuning. Will be enabled only if the `insert_mask_before_placeholder` transform is applied.&quot;, &quot;title&quot;: &quot;Zero Out Prompt Loss&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;use_ckpting&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;Use gradient checkpointing for those modules.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Use Ckpting&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;update_vocab&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Update source and target existing vocabularies.&quot;, &quot;title&quot;: &quot;Update Vocab&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lm_prior_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;LM model to use to train the TM.&quot;, &quot;title&quot;: &quot;Lm Prior Model&quot; }, &quot;lm_prior_lambda&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;LM Prior Lambda&quot;, &quot;title&quot;: &quot;Lm Prior Lambda&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;lm_prior_tau&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;LM Prior Tau&quot;, &quot;title&quot;: &quot;Lm Prior Tau&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;estim_loss_lambda&quot;: { &quot;default&quot;: [ 1.0 ], &quot;description&quot;: &quot;Weight applied to estimator loss&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;title&quot;: &quot;Estim Loss Lambda&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;estim_loss_lambda_steps&quot;: { &quot;default&quot;: [ 0 ], &quot;description&quot;: &quot;Steps at which estimator loss lambda changes&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Estim Loss Lambda Steps&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;score_threshold&quot;: { &quot;default&quot;: 0.68, &quot;description&quot;: &quot;Threshold to filterout data&quot;, &quot;title&quot;: &quot;Score Threshold&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;log_attention_entropy&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Whether to compute and log attention entropy during training.&quot;, &quot;title&quot;: &quot;Log Attention Entropy&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;attention_entropy_types&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Which attention types to compute entropy for. If None, computes for all available types (e.g., ['std', 'self', 'context']).&quot;, &quot;title&quot;: &quot;Attention Entropy Types&quot; }, &quot;attention_entropy_layers&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Which attention layer indices to include in entropy computation. If None, includes all layers.&quot;, &quot;title&quot;: &quot;Attention Entropy Layers&quot; }, &quot;attention_entropy_aggregation&quot;: { &quot;default&quot;: &quot;mean&quot;, &quot;description&quot;: &quot;How to aggregate attention entropy across different attention types/layers.&quot;, &quot;enum&quot;: [ &quot;mean&quot;, &quot;max&quot;, &quot;min&quot; ], &quot;title&quot;: &quot;Attention Entropy Aggregation&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = ()arbitrary_types_allowed: bool = True Fields: accum_count (List[int])accum_steps (List[int])attention_dropout (List[float])attention_entropy_aggregation (Literal['mean', 'max', 'min'])attention_entropy_layers (List[int] | None)attention_entropy_types (List[str] | None)average_decay (float)average_every (int)batch_size (int)batch_size_multiple (int)batch_type (Literal['sents', 'tokens'])bucket_size (int)bucket_size_increment (int)bucket_size_init (int)data_type (str | None)dropout (List[float])dropout_steps (List[int])early_stopping (int)early_stopping_criteria (str | None)estim_loss_lambda (List[float])estim_loss_lambda_steps (List[int])freeze_decoder (bool)freeze_encoder (bool)keep_checkpoint (int)label_smoothing (float)lm_prior_lambda (float)lm_prior_model (str | None)lm_prior_tau (float)log_attention_entropy (bool)max_grad_norm (float)normalization (Literal['sents', 'tokens'])num_workers (int)param_init (float)param_init_method (Literal['xavier_uniform', 'uniform', 'normal'])pre_word_vecs_dec (str | None)pre_word_vecs_enc (str | None)prefetch_factor (int)save_checkpoint_steps (int)save_format (Literal['pytorch', 'safetensors'])score_threshold (float)single_pass (bool)train_from (str | None)train_steps (int)update_vocab (bool)use_ckpting (List[str])valid_batch_size (int)valid_steps (int)zero_out_prompt_loss (bool) Validators: _validate_running_config » all fieldscheckpointing_layers » use_ckpting  field accum_count : List[int] = [1]​  Accumulate gradient this many times. Approximately equivalent to updating batch_size * accum_count batches at once. Recommended for transformer.  Validated by: _validate_running_config  field accum_steps : List[int] = [0]​  Steps at which accum_count values change.  Validated by: _validate_running_config  field attention_dropout : List[float] = [0.1]​  Attention dropout probability.  Validated by: _validate_running_config  field attention_entropy_aggregation : Literal['mean', 'max', 'min'] = 'mean'​  How to aggregate attention entropy across different attention types/layers.  Validated by: _validate_running_config  field attention_entropy_layers : List[int] | None = None​  Which attention layer indices to include in entropy computation. If None, includes all layers.  Validated by: _validate_running_config  field attention_entropy_types : List[str] | None = None​  Which attention types to compute entropy for. If None, computes for all available types (e.g., [‘std’, ‘self’, ‘context’]).  Validated by: _validate_running_config  field average_decay : float = 0.0​  Exponential moving average decay (https://en.wikipedia.org/wiki/Moving_average). Set to other than 0 (e.g. 1e-4) to activate. Similar to Marian NMT implementation (http://www.aclweb.org/anthology/P18-4020).  Validated by: _validate_running_config  field average_every : int = 1​  Step for moving average. Default is every update if average_decay is set.  Validated by: _validate_running_config  field batch_size : int = 64​  Maximum batch size for training.  Validated by: _validate_running_config  field batch_size_multiple : int = 1​  Batch size multiple for token batches.  Validated by: _validate_running_config  field batch_type : Literal['sents', 'tokens'] = 'sents'​  Batch grouping for batch_size.  Validated by: _validate_running_config  field bucket_size : int = 262144​  A bucket is a buffer of bucket_size examples to pick from the various corpora. The dynamic iterator batches batch_size items from the bucket and shuffle them.  Validated by: _validate_running_config  field bucket_size_increment : int = 0​  Bucket size incremented with this amount of examples at each new bucket (up to bucket_size).  Validated by: _validate_running_config  field bucket_size_init : int = -1​  Bucket size is initialized with this amount of examples (see bucket_size_increment).  Validated by: _validate_running_config  field data_type : str | None = 'text'​  Validated by: _validate_running_config  field dropout : List[float] = [0.3]​  Dropout probability.  Validated by: _validate_running_config  field dropout_steps : List[int] = [0]​  Steps at which dropout changes.  Validated by: _validate_running_config  field early_stopping : int = 0​  Number of validation steps without improving that will trigger early stop of training.  Validated by: _validate_running_config  field early_stopping_criteria : str | None = None​  Criteria to use for early stopping.  Validated by: _validate_running_config  field estim_loss_lambda : List[float] = [1.0]​  Weight applied to estimator loss  Validated by: _validate_running_config  field estim_loss_lambda_steps : List[int] = [0]​  Steps at which estimator loss lambda changes  Validated by: _validate_running_config  field freeze_decoder : bool = False​  Freeze parameters in decoder.  Validated by: _validate_running_config  field freeze_encoder : bool = False​  Freeze parameters in encoder.  Validated by: _validate_running_config  field keep_checkpoint : int = -1​  Number of checkpoints to retain. (-1 retains all)  Validated by: _validate_running_config  field label_smoothing : float = 0.0​  Label smoothing value epsilon. Probability of all non-true labels will be smoothed by epsilon/(vocab_size-1). Set to 0 to turn off label smoothing. (https://arxiv.org/abs/1512.00567)  Validated by: _validate_running_config  field lm_prior_lambda : float = 0.0​  LM Prior Lambda  Validated by: _validate_running_config  field lm_prior_model : str | None = None​  LM model to use to train the TM.  Validated by: _validate_running_config  field lm_prior_tau : float = 1.0​  LM Prior Tau  Validated by: _validate_running_config  field log_attention_entropy : bool = True​  Whether to compute and log attention entropy during training.  Validated by: _validate_running_config  field max_grad_norm : float = 5​  If the norm of the gradient vector exceeds this value, renormalize it to have the norm equal to max_grad_norm.  Validated by: _validate_running_config  field normalization : Literal['sents', 'tokens'] = 'sents'​  Normalization method of the gradient.  Validated by: _validate_running_config  field num_workers : int = 2​  Number of workers for pytorch.DataLoader objects.  Validated by: _validate_running_config  field param_init : float = 0.1​  Support value for uniform distribution parameters initialization. Set to 0 not to use initialization.  Validated by: _validate_running_config  field param_init_method : Literal['xavier_uniform', 'uniform', 'normal'] = 'uniform'​  Parameter initialization method.  Validated by: _validate_running_config  field pre_word_vecs_dec : str | None = None​  If a valid path is specified, will load pretrained word embeddings on the decoder side.  Validated by: _validate_running_config  field pre_word_vecs_enc : str | None = None​  If a valid path is specified, will load pretrained word embeddings on the encoder side.  Validated by: _validate_running_config  field prefetch_factor : int = 200​  Number of mini-batches loaded in advance to avoid the GPU waiting during processing of next bucket.  Validated by: _validate_running_config  field save_checkpoint_steps : int = 5000​  Frequency of checkpoint saving (in steps).  Validated by: _validate_running_config  field save_format : Literal['pytorch', 'safetensors'] = 'pytorch'​  Format to save the model weights.  Validated by: _validate_running_config  field score_threshold : float = 0.68​  Threshold to filterout data  Validated by: _validate_running_config  field single_pass : bool = False​  Make a single pass over the training dataset.  Validated by: _validate_running_config  field train_from : str | None = None​  Pretrained model/checkpoint weights to continue training from.  Validated by: _validate_running_config  field train_steps : int = 100000​  Number of training steps.  Validated by: _validate_running_config  field update_vocab : bool = False​  Update source and target existing vocabularies.  Validated by: _validate_running_config  field use_ckpting : List[str] = []​  Use gradient checkpointing for those modules.  Validated by: _validate_running_configcheckpointing_layers  field valid_batch_size : int = 32​  Maximum batch size for validation.  Validated by: _validate_running_config  field valid_steps : int = 10000​  Frequency of validation, in steps.  Validated by: _validate_running_config  field zero_out_prompt_loss : bool = False​  Set the prompt loss to zero. Mostly for LLM finetuning. Will be enabled only if the insert_mask_before_placeholder transform is applied.  Validated by: _validate_running_config  validator checkpointing_layers » use_ckpting​  get_model_path()​  property storage_dtype : dtype​  Deduce which dtype to use for main model parameters. E.g. with mixed precision a copy is kept in float32. ","version":"Next","tagName":"h3"},{"title":"Transforms","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/transforms","content":"","keywords":"","version":"Next"},{"title":"pydantic model eole.transforms.tokenize.ONMTTokenizerConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformstokenizeonmttokenizerconfig","content":" Bases: BaseTokenizerConfig   Show JSON schema { &quot;title&quot;: &quot;ONMTTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; }, &quot;src_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for src (or shared) in pyonmttok.&quot;, &quot;title&quot;: &quot;Src Subword Type&quot; }, &quot;tgt_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for tgt in pyonmttok.&quot;, &quot;title&quot;: &quot;Tgt Subword Type&quot; }, &quot;src_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for src in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Src Onmttok Kwargs&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for tgt in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Tgt Onmttok Kwargs&quot; }, &quot;gpt2_pretok&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Preprocess sentence with byte-level mapping.&quot;, &quot;title&quot;: &quot;Gpt2 Pretok&quot; }, &quot;mapped_tokens&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;maxItems&quot;: 2, &quot;minItems&quot;: 2, &quot;prefixItems&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;string&quot; } ], &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Mapped tokens for placeholders preservation&quot;, &quot;title&quot;: &quot;Mapped Tokens&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: gpt2_pretok (bool | None)mapped_tokens (List[Tuple[str, str]] | None)src_onmttok_kwargs (dict | None)src_subword_type (Literal['none', 'sentencepiece', 'bpe'] | None)tgt_onmttok_kwargs (dict | None)tgt_subword_type (Literal['none', 'sentencepiece', 'bpe'] | None) Validators: check_values » all fields  field gpt2_pretok : bool | None = False​  Preprocess sentence with byte-level mapping.  Validated by: check_values  field mapped_tokens : List[Tuple[str, str]] | None = None​  Mapped tokens for placeholders preservation  Validated by: check_values  field src_onmttok_kwargs : dict | None = {'mode': 'none'}​  Other pyonmttok options for src in dict string, except subword related options listed earlier.  Validated by: check_values  field src_subword_type : Literal['none', 'sentencepiece', 'bpe'] | None = 'none'​  Type of subword model for src (or shared) in pyonmttok.  Validated by: check_values  field tgt_onmttok_kwargs : dict | None = {'mode': 'none'}​  Other pyonmttok options for tgt in dict string, except subword related options listed earlier.  Validated by: check_values  field tgt_subword_type : Literal['none', 'sentencepiece', 'bpe'] | None = 'none'​  Type of subword model for tgt in pyonmttok.  Validated by: check_values  validator check_values » all fields​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.tokenize.BaseTokenizerConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformstokenizebasetokenizerconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;BaseTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: src_subword_alpha (float | None)src_subword_model (str | None)src_subword_nbest (int | None)src_subword_vocab (str | None)src_vocab_threshold (int | None)tgt_subword_alpha (float | None)tgt_subword_model (str | None)tgt_subword_nbest (int | None)tgt_subword_vocab (str | None)tgt_vocab_threshold (int | None) Validators: check_values » all fields  field src_subword_alpha : float | None = 0​  Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)  Validated by: check_values  field src_subword_model : str | None = None​  Path of subword model for src (or shared).  Validated by: check_values  field src_subword_nbest : int | None = 1​  Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)  Validated by: check_values  field src_subword_vocab : str | None = ''​  Path to the vocabulary file for src subword. Format: t per line.  Validated by: check_values  field src_vocab_threshold : int | None = 0​  Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.  Validated by: check_values  field tgt_subword_alpha : float | None = 0​  Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)  Validated by: check_values  field tgt_subword_model : str | None = None​  Path of subword model for tgt.  Validated by: check_values  field tgt_subword_nbest : int | None = 1​  Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)  Validated by: check_values  field tgt_subword_vocab : str | None = ''​  Path to the vocabulary file for tgt subword. Format: t per line.  Validated by: check_values  field tgt_vocab_threshold : int | None = 0​  Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.  Validated by: check_values  validator check_values » all fields​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.docify.DocifyConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsdocifydocifyconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;DocifyConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;doc_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 200, &quot;description&quot;: &quot;Number of tokens per doc.&quot;, &quot;title&quot;: &quot;Doc Length&quot; }, &quot;max_context&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Max context segments.&quot;, &quot;title&quot;: &quot;Max Context&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: doc_length (int | None)max_context (int | None)  field doc_length : int | None = 200​  Number of tokens per doc.  field max_context : int | None = 1​  Max context segments.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.clean.CleanConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformscleancleanconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;CleanConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_eq_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex src==tgt&quot;, &quot;title&quot;: &quot;Src Eq Tgt&quot; }, &quot;same_char&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same char more than 4 times&quot;, &quot;title&quot;: &quot;Same Char&quot; }, &quot;same_word&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same word more than 3 times&quot;, &quot;title&quot;: &quot;Same Word&quot; }, &quot;scripts_ok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;description&quot;: &quot;list of unicodata scripts accepted&quot;, &quot;title&quot;: &quot;Scripts Ok&quot; }, &quot;scripts_nok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of unicodata scripts not accepted&quot;, &quot;title&quot;: &quot;Scripts Nok&quot; }, &quot;src_tgt_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 2.0, &quot;description&quot;: &quot;ratio between src and tgt&quot;, &quot;title&quot;: &quot;Src Tgt Ratio&quot; }, &quot;avg_tok_min&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;average length of tokens min&quot;, &quot;title&quot;: &quot;Avg Tok Min&quot; }, &quot;avg_tok_max&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 20.0, &quot;description&quot;: &quot;average length of tokens max&quot;, &quot;title&quot;: &quot;Avg Tok Max&quot; }, &quot;langid&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of languages accepted&quot;, &quot;title&quot;: &quot;Langid&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: avg_tok_max (float | None)avg_tok_min (float | None)langid (List[str] | None)same_char (bool | None)same_word (bool | None)scripts_nok (List[str] | None)scripts_ok (List[str] | None)src_eq_tgt (bool | None)src_tgt_ratio (float | None)  field avg_tok_max : float | None = 20.0​  average length of tokens max  field avg_tok_min : float | None = 3.0​  average length of tokens min  field langid : List[str] | None = []​  list of languages accepted  field same_char : bool | None = False​  Remove ex with same char more than 4 times  field same_word : bool | None = False​  Remove ex with same word more than 3 times  field scripts_nok : List[str] | None = []​  list of unicodata scripts not accepted  field scripts_ok : List[str] | None = ['Latin', 'Common']​  list of unicodata scripts accepted  field src_eq_tgt : bool | None = False​  Remove ex src==tgt  field src_tgt_ratio : float | None = 2.0​  ratio between src and tgt  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.bart.BARTNoiseConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsbartbartnoiseconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;BARTNoiseConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;permute_sent_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Permute this proportion of sentences (boundaries defined by ['.', '?', '!']) in all inputs.&quot;, &quot;title&quot;: &quot;Permute Sent Ratio&quot; }, &quot;rotate_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rotate this proportion of inputs.&quot;, &quot;title&quot;: &quot;Rotate Ratio&quot; }, &quot;insert_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Insert this percentage of additional random tokens.&quot;, &quot;title&quot;: &quot;Insert Ratio&quot; }, &quot;random_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Instead of using &lt;mask&gt;, use random token this often.&quot;, &quot;title&quot;: &quot;Random Ratio&quot; }, &quot;mask_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Fraction of words/subwords that will be masked.&quot;, &quot;title&quot;: &quot;Mask Ratio&quot; }, &quot;mask_length&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;subword&quot;, &quot;word&quot;, &quot;span-poisson&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;subword&quot;, &quot;description&quot;: &quot;Length of masking window to apply.&quot;, &quot;title&quot;: &quot;Mask Length&quot; }, &quot;poisson_lambda&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;Lambda for Poisson distribution to sample span length if `-mask_length` set to span-poisson.&quot;, &quot;title&quot;: &quot;Poisson Lambda&quot; }, &quot;replace_length&quot;: { &quot;anyOf&quot;: [ { &quot;maximum&quot;: 1, &quot;minimum&quot;: -1, &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: -1, &quot;description&quot;: &quot;When masking N tokens, replace with 0, 1, or N tokens. (use -1 for N)&quot;, &quot;title&quot;: &quot;Replace Length&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: insert_ratio (float | None)mask_length (Literal['subword', 'word', 'span-poisson'] | None)mask_ratio (float | None)permute_sent_ratio (float | None)poisson_lambda (float | None)random_ratio (float | None)replace_length (int | None)rotate_ratio (float | None)  field insert_ratio : float | None = 0.0​  Insert this percentage of additional random tokens.  field mask_length : Literal['subword', 'word', 'span-poisson'] | None = 'subword'​  Length of masking window to apply.  field mask_ratio : float | None = 0.0​  Fraction of words/subwords that will be masked.  field permute_sent_ratio : float | None = 0.0​  Permute this proportion of sentences (boundaries defined by [‘.’, ‘?’, ‘!’]) in all inputs.  field poisson_lambda : float | None = 3.0​  Lambda for Poisson distribution to sample span length if -mask_length set to span-poisson.  field random_ratio : float | None = 0.0​  Instead of using , use random token this often.  field replace_length : int | None = -1​  When masking N tokens, replace with 0, 1, or N tokens. (use -1 for N)  Constraints: ge = -1le = 1  field rotate_ratio : float | None = 0.0​  Rotate this proportion of inputs.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.fuzzymatch.FuzzyMatchConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsfuzzymatchfuzzymatchconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;FuzzyMatchConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;tm_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a flat text TM.&quot;, &quot;title&quot;: &quot;Tm Path&quot; }, &quot;fuzzy_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.1, &quot;description&quot;: &quot;Ratio of corpus to augment with fuzzy matches.&quot;, &quot;title&quot;: &quot;Fuzzy Corpus Ratio&quot; }, &quot;fuzzy_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 70, &quot;description&quot;: &quot;The fuzzy matching threshold.&quot;, &quot;title&quot;: &quot;Fuzzy Threshold&quot; }, &quot;tm_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\t&quot;, &quot;description&quot;: &quot;The delimiter used in the flat text TM.&quot;, &quot;title&quot;: &quot;Tm Delimiter&quot; }, &quot;fuzzy_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;The fuzzy token to be added with the matches.&quot;, &quot;title&quot;: &quot;Fuzzy Token&quot; }, &quot;fuzzymatch_min_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 70, &quot;description&quot;: &quot;Max length for TM entries and examples to match.&quot;, &quot;title&quot;: &quot;Fuzzymatch Min Length&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: fuzzy_corpus_ratio (float | None)fuzzy_threshold (float | None)fuzzy_token (str | None)fuzzymatch_min_length (int | None)tm_delimiter (str | None)tm_path (str | None)  field fuzzy_corpus_ratio : float | None = 0.1​  Ratio of corpus to augment with fuzzy matches.  field fuzzy_threshold : float | None = 70​  The fuzzy matching threshold.  field fuzzy_token : str | None = '｟fuzzy｠'​  The fuzzy token to be added with the matches.  field fuzzymatch_min_length : int | None = 70​  Max length for TM entries and examples to match.  field tm_delimiter : str | None = '\\t'​  The delimiter used in the flat text TM.  field tm_path : str | None = None​  Path to a flat text TM.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.inlinetags.InlineTagsConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsinlinetagsinlinetagsconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;InlineTagsConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;tags_dictionary_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a flat term dictionary.&quot;, &quot;title&quot;: &quot;Tags Dictionary Path&quot; }, &quot;tags_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.1, &quot;description&quot;: &quot;Ratio of corpus to augment with tags.&quot;, &quot;title&quot;: &quot;Tags Corpus Ratio&quot; }, &quot;max_tags&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 12, &quot;description&quot;: &quot;Maximum number of tags that can be added to a single sentence.&quot;, &quot;title&quot;: &quot;Max Tags&quot; }, &quot;paired_stag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;description&quot;: &quot;The format of an opening paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Stag&quot; }, &quot;paired_etag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;description&quot;: &quot;The format of a closing paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Etag&quot; }, &quot;isolated_tag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;description&quot;: &quot;The format of an isolated inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Isolated Tag&quot; }, &quot;src_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented src sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Src Delimiter&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: isolated_tag (str | None)max_tags (int | None)paired_etag (str | None)paired_stag (str | None)src_delimiter (str | None)tags_corpus_ratio (float | None)tags_dictionary_path (str | None)  field isolated_tag : str | None = '｟ph_#_std｠'​  The format of an isolated inline tag. Must include the character #.  field max_tags : int | None = 12​  Maximum number of tags that can be added to a single sentence.  field paired_etag : str | None = '｟ph_#_end｠'​  The format of a closing paired inline tag. Must include the character #.  field paired_stag : str | None = '｟ph_#_beg｠'​  The format of an opening paired inline tag. Must include the character #.  field src_delimiter : str | None = '｟fuzzy｠'​  Any special token used for augmented src sentences. The default is the fuzzy token used in the FuzzyMatch transform.  field tags_corpus_ratio : float | None = 0.1​  Ratio of corpus to augment with tags.  field tags_dictionary_path : str | None = None​  Path to a flat term dictionary.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.uppercase.UpperCaseConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsuppercaseuppercaseconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;UpperCaseConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;upper_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.01, &quot;description&quot;: &quot;Corpus ratio to apply uppercasing.&quot;, &quot;title&quot;: &quot;Upper Corpus Ratio&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: upper_corpus_ratio (float | None)  field upper_corpus_ratio : float | None = 0.01​  Corpus ratio to apply uppercasing.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.sampling.TokenDropConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformssamplingtokendropconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;TokenDropConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;tokendrop_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token deletion.&quot;, &quot;title&quot;: &quot;Tokendrop Temperature&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: tokendrop_temperature (float | None)  field tokendrop_temperature : float | None = 1.0​  Sampling temperature for token deletion.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.sampling.TokenMaskConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformssamplingtokenmaskconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;TokenMaskConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;tokenmask_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token masking.&quot;, &quot;title&quot;: &quot;Tokenmask Temperature&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: tokenmask_temperature (float | None)  field tokenmask_temperature : float | None = 1.0​  Sampling temperature for token masking.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.sampling.SwitchOutConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformssamplingswitchoutconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;SwitchOutConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;switchout_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for SwitchOut. :math:`\\\\tau^{-1}` in :cite:`DBLP:journals/corr/abs-1808-07512`. Smaller value makes data more diverse.&quot;, &quot;title&quot;: &quot;Switchout Temperature&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: switchout_temperature (float | None)  field switchout_temperature : float | None = 1.0​  Sampling temperature for SwitchOut. $\\tau^{-1}$ in []. Smaller value makes data more diverse.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.terminology.TerminologyConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsterminologyterminologyconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;TerminologyConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;termbase_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a dictionary file with terms.&quot;, &quot;title&quot;: &quot;Termbase Path&quot; }, &quot;src_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the source corpus.&quot;, &quot;title&quot;: &quot;Src Spacy Language Model&quot; }, &quot;tgt_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the target corpus.&quot;, &quot;title&quot;: &quot;Tgt Spacy Language Model&quot; }, &quot;term_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.3, &quot;description&quot;: &quot;Ratio of corpus to augment with terms.&quot;, &quot;title&quot;: &quot;Term Corpus Ratio&quot; }, &quot;term_example_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.2, &quot;description&quot;: &quot;Maximum terms allowed in an example.&quot;, &quot;title&quot;: &quot;Term Example Ratio&quot; }, &quot;src_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;description&quot;: &quot;The source term start token.&quot;, &quot;title&quot;: &quot;Src Term Stoken&quot; }, &quot;tgt_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;description&quot;: &quot;The target term start token.&quot;, &quot;title&quot;: &quot;Tgt Term Stoken&quot; }, &quot;tgt_term_etoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;description&quot;: &quot;The target term end token.&quot;, &quot;title&quot;: &quot;Tgt Term Etoken&quot; }, &quot;term_source_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented source sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Term Source Delimiter&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: src_spacy_language_model (str | None)src_term_stoken (str | None)term_corpus_ratio (float | None)term_example_ratio (float | None)term_source_delimiter (str | None)termbase_path (str | None)tgt_spacy_language_model (str | None)tgt_term_etoken (str | None)tgt_term_stoken (str | None)  field src_spacy_language_model : str | None = None​  Name of the spaCy language model for the source corpus.  field src_term_stoken : str | None = '｟src_term_start｠'​  The source term start token.  field term_corpus_ratio : float | None = 0.3​  Ratio of corpus to augment with terms.  field term_example_ratio : float | None = 0.2​  Maximum terms allowed in an example.  field term_source_delimiter : str | None = '｟fuzzy｠'​  Any special token used for augmented source sentences. The default is the fuzzy token used in the FuzzyMatch transform.  field termbase_path : str | None = None​  Path to a dictionary file with terms.  field tgt_spacy_language_model : str | None = None​  Name of the spaCy language model for the target corpus.  field tgt_term_etoken : str | None = '｟tgt_term_end｠'​  The target term end token.  field tgt_term_stoken : str | None = '｟tgt_term_start｠'​  The target term start token.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.misc.FilterTooLongConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsmiscfiltertoolongconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;FilterTooLongConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: src_seq_length (int | None)tgt_seq_length (int | None)  field src_seq_length : int | None = 192​  Maximum source sequence length.  field tgt_seq_length : int | None = 192​  Maximum target sequence length.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.misc.PrefixConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsmiscprefixconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;PrefixConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all source examples.&quot;, &quot;title&quot;: &quot;Src Prefix&quot; }, &quot;tgt_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Prefix&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: src_prefix (str | None)tgt_prefix (str | None)  field src_prefix : str | None = ''​  String to prepend to all source examples.  field tgt_prefix : str | None = ''​  String to prepend to all target examples.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.misc.SuffixConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsmiscsuffixconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;SuffixConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all source examples.&quot;, &quot;title&quot;: &quot;Src Suffix&quot; }, &quot;tgt_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Suffix&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: src_suffix (str | None)tgt_suffix (str | None)  field src_suffix : str | None = ''​  String to append to all source examples.  field tgt_suffix : str | None = ''​  String to append to all target examples.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.normalize.NormalizeConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsnormalizenormalizeconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;NormalizeConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Source language code&quot;, &quot;title&quot;: &quot;Src Lang&quot; }, &quot;tgt_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Target language code&quot;, &quot;title&quot;: &quot;Tgt Lang&quot; }, &quot;penn&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Penn substitution&quot;, &quot;title&quot;: &quot;Penn&quot; }, &quot;norm_quote_commas&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize quotations and commas&quot;, &quot;title&quot;: &quot;Norm Quote Commas&quot; }, &quot;norm_numbers&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize numbers&quot;, &quot;title&quot;: &quot;Norm Numbers&quot; }, &quot;pre_replace_unicode_punct&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Replace unicode punct&quot;, &quot;title&quot;: &quot;Pre Replace Unicode Punct&quot; }, &quot;post_remove_control_chars&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove control chars&quot;, &quot;title&quot;: &quot;Post Remove Control Chars&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: norm_numbers (bool | None)norm_quote_commas (bool | None)penn (bool | None)post_remove_control_chars (bool | None)pre_replace_unicode_punct (bool | None)src_lang (str | None)tgt_lang (str | None)  field norm_numbers : bool | None = True​  Normalize numbers  field norm_quote_commas : bool | None = True​  Normalize quotations and commas  field penn : bool | None = True​  Penn substitution  field post_remove_control_chars : bool | None = False​  Remove control chars  field pre_replace_unicode_punct : bool | None = False​  Replace unicode punct  field src_lang : str | None = ''​  Source language code  field tgt_lang : str | None = ''​  Target language code  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.transforms.insert_mask_before_placeholder.InsertMaskBeforePlaceholderConfig​","type":1,"pageTitle":"Transforms","url":"/eole/docs/reference/Config/transforms#pydantic-model-eoletransformsinsert_mask_before_placeholderinsertmaskbeforeplaceholderconfig","content":" Bases: TransformConfig   Show JSON schema { &quot;title&quot;: &quot;InsertMaskBeforePlaceholderConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;response_patterns&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ], &quot;description&quot;: &quot;Response pattern to locate the end of the prompt.&quot;, &quot;title&quot;: &quot;Response Patterns&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: response_patterns (List[str] | None)  field response_patterns : List[str] | None = ['Response : ｟newline｠']​  Response pattern to locate the end of the prompt. ","version":"Next","tagName":"h3"},{"title":"Framework","type":0,"sectionRef":"#","url":"/eole/docs/reference/Core API/core","content":"","keywords":"","version":"Next"},{"title":"Model​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#model","content":" ","version":"Next","tagName":"h2"},{"title":"Trainer​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#trainer","content":" ","version":"Next","tagName":"h2"},{"title":"class eole.utils.Statistics(loss=0, auxloss=0, n_batchs=0, n_sents=0, n_tokens=0, n_correct=0, computed_metrics=None, data_stats=None, attention_entropy=0, n_attention_samples=0)​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#class-eoleutilsstatisticsloss0-auxloss0-n_batchs0-n_sents0-n_tokens0-n_correct0-computed_metricsnone-data_statsnone-attention_entropy0-n_attention_samples0","content":" Bases: object  Accumulator for loss statistics. Currently calculates:  accuracyperplexityelapsed time  accuracy()​  compute accuracy  static all_gather_stats(stat, max_size=4096)​  Gather a Statistics object accross multiple process/nodes  Parameters: stat**(** – obj:Statistics): the statistics object to gather accross all processes/nodesmax_size (int) – max buffer size to use Returns:Statistics, the update stats object  static all_gather_stats_list(stat_list, max_size=4096)​  Gather a Statistics list accross all processes/nodes  Parameters: stat_list (list([Statistics])) – list of statistics objects to gather accross all processes/nodesmax_size (int) – max buffer size to use Returns:list of updated statsReturn type:our_stats(list([Statistics]))  avg_attention_entropy()​  compute average attention entropy  computed_metric(metric)​  check if metric(TER/BLEU) is computed and return it  elapsed_time()​  compute elapsed time  log_tensorboard(prefix, writer, learning_rate, patience, step)​  display statistics to tensorboard  output(step, num_steps, learning_rate, start)​  Write out statistics to stdout.  Parameters: step (int) – current stepn_batch (int) – total batchesstart (int) – start time of step.  ppl()​  compute perplexity  update(stat, update_n_src_tokens=False)​  Update statistics by suming values with another Statistics object  Parameters: stat – another statistic objectupdate_n_src_tokens (bool) – whether to update (sum) n_src_tokens or not  xent()​  compute cross entropy  ","version":"Next","tagName":"h3"},{"title":"Loss​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#loss","content":" ","version":"Next","tagName":"h2"},{"title":"Optimizer​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#optimizer","content":" ","version":"Next","tagName":"h2"},{"title":"class eole.utils.Optimizer(optimizer, learning_rate, learning_rate_decay_fn=None, max_grad_norm=None, use_amp=True)​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#class-eoleutilsoptimizeroptimizer-learning_rate-learning_rate_decay_fnnone-max_grad_normnone-use_amptrue","content":" Bases: object  Controller class for optimization. Mostly a thin wrapper for optim, but also useful for implementing rate scheduling beyond what is currently available. Also implements necessary methods for training RNNs such as grad manipulations.  Parameters: optimizer – A torch.optim.Optimizer instance.learning_rate – The initial learning rate.learning_rate_decay_fn – An optional callable taking the current step as argument and return a learning rate scaling factor.max_grad_norm – Clip gradients to this global norm.  property amp​  True if use torch amp mix precision training.  backward(loss)​  Wrapper for backward pass. Some optimizer requires ownership of the backward pass.  classmethod from_config(model, config, checkpoint=None)​  Builds the optimizer from options.  Parameters: cls – The Optimizer class to instantiate.model – The model to optimize.opt – The dict of user options.checkpoint – An optional checkpoint to load states from. Returns:An Optimizer instance.  learning_rate(step=None)​  Returns the current learning rate.  step()​  Update the model parameters based on current gradients.  Optionally, will employ gradient modification or update learning rate.  property training_step​  The current training step.  zero_grad(set_to_none=True)​  Zero the gradients of optimized parameters.  ","version":"Next","tagName":"h3"},{"title":"class eole.utils.AdaFactor(params, lr=None, beta1=0.9, beta2=0.999, eps1=1e-30, eps2=0.001, cliping_threshold=1, non_constant_decay=True, enable_factorization=True, ams_grad=True, weight_decay=0)​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#class-eoleutilsadafactorparams-lrnone-beta109-beta20999-eps11e-30-eps20001-cliping_threshold1-non_constant_decaytrue-enable_factorizationtrue-ams_gradtrue-weight_decay0","content":" Bases: Optimizer  step(closure=None)​  Perform a single optimization step to update parameter.  Parameters:closure (Callable) – A closure that reevaluates the model and returns the loss. Optional for most optimizers. ","version":"Next","tagName":"h3"},{"title":"Data Loaders","type":0,"sectionRef":"#","url":"/eole/docs/reference/Core API/dataloaders","content":"","keywords":"","version":"Next"},{"title":"Data Iterator​","type":1,"pageTitle":"Data Loaders","url":"/eole/docs/reference/Core API/dataloaders#data-iterator","content":" ","version":"Next","tagName":"h2"},{"title":"Dataset​","type":1,"pageTitle":"Data Loaders","url":"/eole/docs/reference/Core API/dataloaders#dataset","content":"","version":"Next","tagName":"h2"},{"title":"Data","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/data","content":"","keywords":"","version":"Next"},{"title":"pydantic model eole.config.data.BaseVocabConfig​","type":1,"pageTitle":"Data","url":"/eole/docs/reference/Config/data#pydantic-model-eoleconfigdatabasevocabconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;BaseVocabConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;description&quot;: &quot;Path to src (or shared) vocabulary file. Format: one &lt;word&gt; or &lt;word&gt;\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Vocab&quot; }, &quot;tgt_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to tgt vocabulary file. Format: one &lt;word&gt; or &lt;word&gt;\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Vocab&quot; }, &quot;share_vocab&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share source and target vocabulary.&quot;, &quot;title&quot;: &quot;Share Vocab&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;decoder_start_token&quot;: { &quot;default&quot;: &quot;&amp;lt;s&amp;gt;&quot;, &quot;description&quot;: &quot;Default decoder start token. For most models it is &amp;lt;s&amp;gt; = BOS. Some fairseq models require &amp;lt;/s&amp;gt;.&quot;, &quot;title&quot;: &quot;Decoder Start Token&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bos_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&amp;lt;s&amp;gt;&quot;, &quot;title&quot;: &quot;Bos Token&quot; }, &quot;eos_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&amp;lt;/s&amp;gt;&quot;, &quot;title&quot;: &quot;Eos Token&quot; }, &quot;unk_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&lt;unk&gt;&quot;, &quot;title&quot;: &quot;Unk Token&quot; }, &quot;pad_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&lt;blank&gt;&quot;, &quot;title&quot;: &quot;Pad Token&quot; }, &quot;both_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for both source and target tokens.&quot;, &quot;title&quot;: &quot;Both Embeddings&quot; }, &quot;src_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for source tokens.&quot;, &quot;title&quot;: &quot;Src Embeddings&quot; }, &quot;tgt_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for target tokens.&quot;, &quot;title&quot;: &quot;Tgt Embeddings&quot; }, &quot;embeddings_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;GloVe&quot;, &quot;word2vec&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Type of embeddings file.&quot;, &quot;title&quot;: &quot;Embeddings Type&quot; } }, &quot;additionalProperties&quot;: false, &quot;required&quot;: [ &quot;src_vocab&quot; ] }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: bos_token (str | None)both_embeddings (str | None)decoder_start_token (str)embeddings_type (Literal['GloVe', 'word2vec'] | None)eos_token (str | None)pad_token (str | None)share_vocab (bool)src_embeddings (str | None)src_vocab (str | None)tgt_embeddings (str | None)tgt_vocab (str | None)unk_token (str | None)  field bos_token : str | None = '&lt;s&gt;'​  field both_embeddings : str | None = None​  Path to the embeddings file to use for both source and target tokens.  field decoder_start_token : str = '&lt;s&gt;'​  Default decoder start token. For most models it is &lt;s&gt; = BOS. Some fairseq models require &lt;/s&gt;.  field embeddings_type : Literal['GloVe', 'word2vec'] | None = None​  Type of embeddings file.  field eos_token : str | None = '&lt;/s&gt;'​  field pad_token : str | None = ''​  field share_vocab : bool = False​  Share source and target vocabulary.  field src_embeddings : str | None = None​  Path to the embeddings file to use for source tokens.  field src_vocab : str | None [Required]​  Path to src (or shared) vocabulary file. Format: one or per line.  field tgt_embeddings : str | None = None​  Path to the embeddings file to use for target tokens.  field tgt_vocab : str | None = None​  Path to tgt vocabulary file. Format: one or per line.  field unk_token : str | None = ''​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.data.VocabConfig​","type":1,"pageTitle":"Data","url":"/eole/docs/reference/Config/data#pydantic-model-eoleconfigdatavocabconfig","content":" Bases: BaseVocabConfig   Show JSON schema { &quot;title&quot;: &quot;VocabConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;description&quot;: &quot;Path to src (or shared) vocabulary file. Format: one &lt;word&gt; or &lt;word&gt;\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Vocab&quot; }, &quot;tgt_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to tgt vocabulary file. Format: one &lt;word&gt; or &lt;word&gt;\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Vocab&quot; }, &quot;share_vocab&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share source and target vocabulary.&quot;, &quot;title&quot;: &quot;Share Vocab&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;decoder_start_token&quot;: { &quot;default&quot;: &quot;&amp;lt;s&amp;gt;&quot;, &quot;description&quot;: &quot;Default decoder start token. For most models it is &amp;lt;s&amp;gt; = BOS. Some fairseq models require &amp;lt;/s&amp;gt;.&quot;, &quot;title&quot;: &quot;Decoder Start Token&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bos_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&amp;lt;s&amp;gt;&quot;, &quot;title&quot;: &quot;Bos Token&quot; }, &quot;eos_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&amp;lt;/s&amp;gt;&quot;, &quot;title&quot;: &quot;Eos Token&quot; }, &quot;unk_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&lt;unk&gt;&quot;, &quot;title&quot;: &quot;Unk Token&quot; }, &quot;pad_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&lt;blank&gt;&quot;, &quot;title&quot;: &quot;Pad Token&quot; }, &quot;both_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for both source and target tokens.&quot;, &quot;title&quot;: &quot;Both Embeddings&quot; }, &quot;src_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for source tokens.&quot;, &quot;title&quot;: &quot;Src Embeddings&quot; }, &quot;tgt_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for target tokens.&quot;, &quot;title&quot;: &quot;Tgt Embeddings&quot; }, &quot;embeddings_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;GloVe&quot;, &quot;word2vec&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Type of embeddings file.&quot;, &quot;title&quot;: &quot;Embeddings Type&quot; }, &quot;src_vocab_size&quot;: { &quot;default&quot;: 32758, &quot;description&quot;: &quot;Maximum size of the source vocabulary.&quot;, &quot;title&quot;: &quot;Src Vocab Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_vocab_size&quot;: { &quot;default&quot;: 32768, &quot;description&quot;: &quot;Maximum size of the target vocabulary.&quot;, &quot;title&quot;: &quot;Tgt Vocab Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;vocab_size_multiple&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Make the vocabulary size a multiple of this value. (Adds dummy tokens if needed.)&quot;, &quot;title&quot;: &quot;Vocab Size Multiple&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_words_min_frequency&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Discard source words with lower frequency.&quot;, &quot;title&quot;: &quot;Src Words Min Frequency&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_words_min_frequency&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Discard target words with lower frequency.&quot;, &quot;title&quot;: &quot;Tgt Words Min Frequency&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;additionalProperties&quot;: false, &quot;required&quot;: [ &quot;src_vocab&quot; ] }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: src_vocab_size (int)src_words_min_frequency (int)tgt_vocab_size (int)tgt_words_min_frequency (int)vocab_size_multiple (int)  field src_vocab_size : int = 32758​  Maximum size of the source vocabulary.  field src_words_min_frequency : int = 0​  Discard source words with lower frequency.  field tgt_vocab_size : int = 32768​  Maximum size of the target vocabulary.  field tgt_words_min_frequency : int = 0​  Discard target words with lower frequency.  field vocab_size_multiple : int = 8​  Make the vocabulary size a multiple of this value. (Adds dummy tokens if needed.)  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.data.Dataset​","type":1,"pageTitle":"Data","url":"/eole/docs/reference/Config/data#pydantic-model-eoleconfigdatadataset","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;Dataset&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;name&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Name&quot; }, &quot;weight&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;title&quot;: &quot;Weight&quot; }, &quot;transforms&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Transforms&quot; }, &quot;path_src&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Src&quot; }, &quot;path_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Tgt&quot; }, &quot;path_sco&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Sco&quot; }, &quot;path_txt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Txt&quot; }, &quot;path_align&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Align&quot; }, &quot;src_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Prefix&quot; }, &quot;tgt_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Prefix&quot; }, &quot;src_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Suffix&quot; }, &quot;tgt_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Suffix&quot; }, &quot;src_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Lang&quot; }, &quot;tgt_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Lang&quot; }, &quot;penn&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Penn&quot; }, &quot;norm_quote_commas&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Norm Quote Commas&quot; }, &quot;norm_numbers&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Norm Numbers&quot; }, &quot;pre_replace_unicode_punct&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Pre Replace Unicode Punct&quot; }, &quot;post_remove_control_chars&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Post Remove Control Chars&quot; }, &quot;src_eq_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Src Eq Tgt&quot; }, &quot;same_char&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Same Char&quot; }, &quot;same_word&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Same Word&quot; }, &quot;scripts_ok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;title&quot;: &quot;Scripts Ok&quot; }, &quot;scripts_nok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;title&quot;: &quot;Scripts Nok&quot; }, &quot;src_tgt_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 2, &quot;title&quot;: &quot;Src Tgt Ratio&quot; }, &quot;avg_tok_min&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Avg Tok Min&quot; }, &quot;avg_tok_max&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 20, &quot;title&quot;: &quot;Avg Tok Max&quot; }, &quot;lang_id&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;en&quot;, &quot;fr&quot; ], &quot;title&quot;: &quot;Lang Id&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: avg_tok_max (float | None)avg_tok_min (float | None)lang_id (List[str] | None)name (str | None)norm_numbers (bool | None)norm_quote_commas (bool | None)path_align (str | None)path_sco (str | None)path_src (str | None)path_tgt (str | None)path_txt (str | None)penn (bool | None)post_remove_control_chars (bool | None)pre_replace_unicode_punct (bool | None)same_char (bool | None)same_word (bool | None)scripts_nok (List[str] | None)scripts_ok (List[str] | None)src_eq_tgt (bool | None)src_lang (str | None)src_prefix (str | None)src_suffix (str | None)src_tgt_ratio (float | None)tgt_lang (str | None)tgt_prefix (str | None)tgt_suffix (str | None)transforms (List[str] | None)weight (int | None)  field avg_tok_max : float | None = 20​  field avg_tok_min : float | None = 3​  field lang_id : List[str] | None = ['en', 'fr']​  field name : str | None = None​  field norm_numbers : bool | None = True​  field norm_quote_commas : bool | None = True​  field path_align : str | None = None​  field path_sco : str | None = None​  field path_src : str | None = None​  field path_tgt : str | None = None​  field path_txt : str | None = None​  field penn : bool | None = True​  field post_remove_control_chars : bool | None = False​  field pre_replace_unicode_punct : bool | None = False​  field same_char : bool | None = True​  field same_word : bool | None = True​  field scripts_nok : List[str] | None = []​  field scripts_ok : List[str] | None = ['Latin', 'Common']​  field src_eq_tgt : bool | None = True​  field src_lang : str | None = None​  field src_prefix : str | None = None​  field src_suffix : str | None = None​  field src_tgt_ratio : float | None = 2​  field tgt_lang : str | None = None​  field tgt_prefix : str | None = None​  field tgt_suffix : str | None = None​  field transforms : List[str] | None = None​  field weight : int | None = 1​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.data.DataConfig​","type":1,"pageTitle":"Data","url":"/eole/docs/reference/Config/data#pydantic-model-eoleconfigdatadataconfig","content":" Bases: VocabConfig   Show JSON schema { &quot;title&quot;: &quot;DataConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;description&quot;: &quot;Path to src (or shared) vocabulary file. Format: one &lt;word&gt; or &lt;word&gt;\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Vocab&quot; }, &quot;tgt_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to tgt vocabulary file. Format: one &lt;word&gt; or &lt;word&gt;\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Vocab&quot; }, &quot;share_vocab&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share source and target vocabulary.&quot;, &quot;title&quot;: &quot;Share Vocab&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;decoder_start_token&quot;: { &quot;default&quot;: &quot;&amp;lt;s&amp;gt;&quot;, &quot;description&quot;: &quot;Default decoder start token. For most models it is &amp;lt;s&amp;gt; = BOS. Some fairseq models require &amp;lt;/s&amp;gt;.&quot;, &quot;title&quot;: &quot;Decoder Start Token&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bos_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&amp;lt;s&amp;gt;&quot;, &quot;title&quot;: &quot;Bos Token&quot; }, &quot;eos_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&amp;lt;/s&amp;gt;&quot;, &quot;title&quot;: &quot;Eos Token&quot; }, &quot;unk_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&lt;unk&gt;&quot;, &quot;title&quot;: &quot;Unk Token&quot; }, &quot;pad_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&lt;blank&gt;&quot;, &quot;title&quot;: &quot;Pad Token&quot; }, &quot;both_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for both source and target tokens.&quot;, &quot;title&quot;: &quot;Both Embeddings&quot; }, &quot;src_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for source tokens.&quot;, &quot;title&quot;: &quot;Src Embeddings&quot; }, &quot;tgt_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for target tokens.&quot;, &quot;title&quot;: &quot;Tgt Embeddings&quot; }, &quot;embeddings_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;GloVe&quot;, &quot;word2vec&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Type of embeddings file.&quot;, &quot;title&quot;: &quot;Embeddings Type&quot; }, &quot;src_vocab_size&quot;: { &quot;default&quot;: 32758, &quot;description&quot;: &quot;Maximum size of the source vocabulary.&quot;, &quot;title&quot;: &quot;Src Vocab Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_vocab_size&quot;: { &quot;default&quot;: 32768, &quot;description&quot;: &quot;Maximum size of the target vocabulary.&quot;, &quot;title&quot;: &quot;Tgt Vocab Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;vocab_size_multiple&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Make the vocabulary size a multiple of this value. (Adds dummy tokens if needed.)&quot;, &quot;title&quot;: &quot;Vocab Size Multiple&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_words_min_frequency&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Discard source words with lower frequency.&quot;, &quot;title&quot;: &quot;Src Words Min Frequency&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_words_min_frequency&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Discard target words with lower frequency.&quot;, &quot;title&quot;: &quot;Tgt Words Min Frequency&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;data&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: { &quot;$ref&quot;: &quot;#/$defs/Dataset&quot; }, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;description&quot;: &quot;All datasets and their specifications. See examples/*.yaml for further details.&quot;, &quot;title&quot;: &quot;Data&quot; }, &quot;transforms&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;Default transform pipeline to apply to data. Can be specified in each corpus of data to override.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Transforms&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;transforms_configs&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/NestedAllTransformsConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ] }, &quot;skip_empty_level&quot;: { &quot;default&quot;: &quot;warning&quot;, &quot;description&quot;: &quot;Logging level when encoutering empty examples. (silent: silently ignore/skip empty examples, warning: warn when ignoring/skipping empty examples, error: raise an error and stop execution when any empty example)&quot;, &quot;enum&quot;: [ &quot;silent&quot;, &quot;warning&quot;, &quot;error&quot; ], &quot;title&quot;: &quot;Skip Empty Level&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;n_sample&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;title&quot;: &quot;N Sample&quot; }, &quot;save_data&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Output base path for objects that will be saved (vocab, transforms, embeddings, ...)&quot;, &quot;title&quot;: &quot;Save Data&quot; }, &quot;overwrite&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Overwrite existing objects if any.&quot;, &quot;title&quot;: &quot;Overwrite&quot;, &quot;type&quot;: &quot;boolean&quot; } }, &quot;$defs&quot;: { &quot;BARTNoiseConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;permute_sent_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Permute this proportion of sentences (boundaries defined by ['.', '?', '!']) in all inputs.&quot;, &quot;title&quot;: &quot;Permute Sent Ratio&quot; }, &quot;rotate_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rotate this proportion of inputs.&quot;, &quot;title&quot;: &quot;Rotate Ratio&quot; }, &quot;insert_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Insert this percentage of additional random tokens.&quot;, &quot;title&quot;: &quot;Insert Ratio&quot; }, &quot;random_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Instead of using &lt;mask&gt;, use random token this often.&quot;, &quot;title&quot;: &quot;Random Ratio&quot; }, &quot;mask_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Fraction of words/subwords that will be masked.&quot;, &quot;title&quot;: &quot;Mask Ratio&quot; }, &quot;mask_length&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;subword&quot;, &quot;word&quot;, &quot;span-poisson&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;subword&quot;, &quot;description&quot;: &quot;Length of masking window to apply.&quot;, &quot;title&quot;: &quot;Mask Length&quot; }, &quot;poisson_lambda&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;Lambda for Poisson distribution to sample span length if `-mask_length` set to span-poisson.&quot;, &quot;title&quot;: &quot;Poisson Lambda&quot; }, &quot;replace_length&quot;: { &quot;anyOf&quot;: [ { &quot;maximum&quot;: 1, &quot;minimum&quot;: -1, &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: -1, &quot;description&quot;: &quot;When masking N tokens, replace with 0, 1, or N tokens. (use -1 for N)&quot;, &quot;title&quot;: &quot;Replace Length&quot; } }, &quot;title&quot;: &quot;BARTNoiseConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;BaseTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; } }, &quot;title&quot;: &quot;BaseTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CleanConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_eq_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex src==tgt&quot;, &quot;title&quot;: &quot;Src Eq Tgt&quot; }, &quot;same_char&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same char more than 4 times&quot;, &quot;title&quot;: &quot;Same Char&quot; }, &quot;same_word&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same word more than 3 times&quot;, &quot;title&quot;: &quot;Same Word&quot; }, &quot;scripts_ok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;description&quot;: &quot;list of unicodata scripts accepted&quot;, &quot;title&quot;: &quot;Scripts Ok&quot; }, &quot;scripts_nok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of unicodata scripts not accepted&quot;, &quot;title&quot;: &quot;Scripts Nok&quot; }, &quot;src_tgt_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 2.0, &quot;description&quot;: &quot;ratio between src and tgt&quot;, &quot;title&quot;: &quot;Src Tgt Ratio&quot; }, &quot;avg_tok_min&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;average length of tokens min&quot;, &quot;title&quot;: &quot;Avg Tok Min&quot; }, &quot;avg_tok_max&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 20.0, &quot;description&quot;: &quot;average length of tokens max&quot;, &quot;title&quot;: &quot;Avg Tok Max&quot; }, &quot;langid&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of languages accepted&quot;, &quot;title&quot;: &quot;Langid&quot; } }, &quot;title&quot;: &quot;CleanConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;Dataset&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;name&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Name&quot; }, &quot;weight&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;title&quot;: &quot;Weight&quot; }, &quot;transforms&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Transforms&quot; }, &quot;path_src&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Src&quot; }, &quot;path_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Tgt&quot; }, &quot;path_sco&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Sco&quot; }, &quot;path_txt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Txt&quot; }, &quot;path_align&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Align&quot; }, &quot;src_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Prefix&quot; }, &quot;tgt_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Prefix&quot; }, &quot;src_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Suffix&quot; }, &quot;tgt_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Suffix&quot; }, &quot;src_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Lang&quot; }, &quot;tgt_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Lang&quot; }, &quot;penn&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Penn&quot; }, &quot;norm_quote_commas&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Norm Quote Commas&quot; }, &quot;norm_numbers&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Norm Numbers&quot; }, &quot;pre_replace_unicode_punct&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Pre Replace Unicode Punct&quot; }, &quot;post_remove_control_chars&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Post Remove Control Chars&quot; }, &quot;src_eq_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Src Eq Tgt&quot; }, &quot;same_char&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Same Char&quot; }, &quot;same_word&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Same Word&quot; }, &quot;scripts_ok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;title&quot;: &quot;Scripts Ok&quot; }, &quot;scripts_nok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;title&quot;: &quot;Scripts Nok&quot; }, &quot;src_tgt_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 2, &quot;title&quot;: &quot;Src Tgt Ratio&quot; }, &quot;avg_tok_min&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Avg Tok Min&quot; }, &quot;avg_tok_max&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 20, &quot;title&quot;: &quot;Avg Tok Max&quot; }, &quot;lang_id&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;en&quot;, &quot;fr&quot; ], &quot;title&quot;: &quot;Lang Id&quot; } }, &quot;title&quot;: &quot;Dataset&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;DocifyConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;doc_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 200, &quot;description&quot;: &quot;Number of tokens per doc.&quot;, &quot;title&quot;: &quot;Doc Length&quot; }, &quot;max_context&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Max context segments.&quot;, &quot;title&quot;: &quot;Max Context&quot; } }, &quot;title&quot;: &quot;DocifyConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;FilterTooLongConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;title&quot;: &quot;FilterTooLongConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;FilterTooShortConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 48, &quot;description&quot;: &quot;Minimum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 48, &quot;description&quot;: &quot;Minimum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;title&quot;: &quot;FilterTooShortConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;HuggingfaceTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;max_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Max Length&quot; } }, &quot;title&quot;: &quot;HuggingfaceTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InlineTagsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tags_dictionary_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a flat term dictionary.&quot;, &quot;title&quot;: &quot;Tags Dictionary Path&quot; }, &quot;tags_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.1, &quot;description&quot;: &quot;Ratio of corpus to augment with tags.&quot;, &quot;title&quot;: &quot;Tags Corpus Ratio&quot; }, &quot;max_tags&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 12, &quot;description&quot;: &quot;Maximum number of tags that can be added to a single sentence.&quot;, &quot;title&quot;: &quot;Max Tags&quot; }, &quot;paired_stag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;description&quot;: &quot;The format of an opening paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Stag&quot; }, &quot;paired_etag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;description&quot;: &quot;The format of a closing paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Etag&quot; }, &quot;isolated_tag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;description&quot;: &quot;The format of an isolated inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Isolated Tag&quot; }, &quot;src_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented src sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Src Delimiter&quot; } }, &quot;title&quot;: &quot;InlineTagsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InsertMaskBeforePlaceholderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;response_patterns&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ], &quot;description&quot;: &quot;Response pattern to locate the end of the prompt.&quot;, &quot;title&quot;: &quot;Response Patterns&quot; } }, &quot;title&quot;: &quot;InsertMaskBeforePlaceholderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;NestedAllTransformsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;huggingface_tokenize&quot;: { &quot;$ref&quot;: &quot;#/$defs/HuggingfaceTokenizerConfig&quot;, &quot;default&quot;: { &quot;path&quot;: null, &quot;huggingface_model&quot;: null, &quot;max_length&quot;: null } }, &quot;uppercase&quot;: { &quot;$ref&quot;: &quot;#/$defs/UpperCaseConfig&quot;, &quot;default&quot;: { &quot;upper_corpus_ratio&quot;: 0.01 } }, &quot;terminology&quot;: { &quot;$ref&quot;: &quot;#/$defs/TerminologyConfig&quot;, &quot;default&quot;: { &quot;termbase_path&quot;: null, &quot;src_spacy_language_model&quot;: null, &quot;tgt_spacy_language_model&quot;: null, &quot;term_corpus_ratio&quot;: 0.3, &quot;term_example_ratio&quot;: 0.2, &quot;src_term_stoken&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;tgt_term_stoken&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;tgt_term_etoken&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;term_source_delimiter&quot;: &quot;\\uff5ffuzzy\\uff60&quot; } }, &quot;sentencepiece&quot;: { &quot;$ref&quot;: &quot;#/$defs/BaseTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0 } }, &quot;bpe&quot;: { &quot;$ref&quot;: &quot;#/$defs/BaseTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0 } }, &quot;onmt_tokenize&quot;: { &quot;$ref&quot;: &quot;#/$defs/ONMTTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0, &quot;src_subword_type&quot;: &quot;none&quot;, &quot;tgt_subword_type&quot;: &quot;none&quot;, &quot;src_onmttok_kwargs&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;gpt2_pretok&quot;: false, &quot;mapped_tokens&quot;: null } }, &quot;bart&quot;: { &quot;$ref&quot;: &quot;#/$defs/BARTNoiseConfig&quot;, &quot;default&quot;: { &quot;permute_sent_ratio&quot;: 0.0, &quot;rotate_ratio&quot;: 0.0, &quot;insert_ratio&quot;: 0.0, &quot;random_ratio&quot;: 0.0, &quot;mask_ratio&quot;: 0.0, &quot;mask_length&quot;: &quot;subword&quot;, &quot;poisson_lambda&quot;: 3.0, &quot;replace_length&quot;: -1 } }, &quot;switchout&quot;: { &quot;$ref&quot;: &quot;#/$defs/SwitchOutConfig&quot;, &quot;default&quot;: { &quot;switchout_temperature&quot;: 1.0 } }, &quot;tokendrop&quot;: { &quot;$ref&quot;: &quot;#/$defs/TokenDropConfig&quot;, &quot;default&quot;: { &quot;tokendrop_temperature&quot;: 1.0 } }, &quot;tokenmask&quot;: { &quot;$ref&quot;: &quot;#/$defs/TokenMaskConfig&quot;, &quot;default&quot;: { &quot;tokenmask_temperature&quot;: 1.0 } }, &quot;normalize&quot;: { &quot;$ref&quot;: &quot;#/$defs/NormalizeConfig&quot;, &quot;default&quot;: { &quot;src_lang&quot;: &quot;&quot;, &quot;tgt_lang&quot;: &quot;&quot;, &quot;penn&quot;: true, &quot;norm_quote_commas&quot;: true, &quot;norm_numbers&quot;: true, &quot;pre_replace_unicode_punct&quot;: false, &quot;post_remove_control_chars&quot;: false } }, &quot;insert_mask_before_placeholder&quot;: { &quot;$ref&quot;: &quot;#/$defs/InsertMaskBeforePlaceholderConfig&quot;, &quot;default&quot;: { &quot;response_patterns&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ] } }, &quot;filtertooshort&quot;: { &quot;$ref&quot;: &quot;#/$defs/FilterTooShortConfig&quot;, &quot;default&quot;: { &quot;src_seq_length&quot;: 48, &quot;tgt_seq_length&quot;: 48 } }, &quot;filtertoolong&quot;: { &quot;$ref&quot;: &quot;#/$defs/FilterTooLongConfig&quot;, &quot;default&quot;: { &quot;src_seq_length&quot;: 192, &quot;tgt_seq_length&quot;: 192 } }, &quot;prefix&quot;: { &quot;$ref&quot;: &quot;#/$defs/PrefixConfig&quot;, &quot;default&quot;: { &quot;src_prefix&quot;: &quot;&quot;, &quot;tgt_prefix&quot;: &quot;&quot; } }, &quot;suffix&quot;: { &quot;$ref&quot;: &quot;#/$defs/SuffixConfig&quot;, &quot;default&quot;: { &quot;src_suffix&quot;: &quot;&quot;, &quot;tgt_suffix&quot;: &quot;&quot; } }, &quot;docify&quot;: { &quot;$ref&quot;: &quot;#/$defs/DocifyConfig&quot;, &quot;default&quot;: { &quot;doc_length&quot;: 200, &quot;max_context&quot;: 1 } }, &quot;inlinetags&quot;: { &quot;$ref&quot;: &quot;#/$defs/InlineTagsConfig&quot;, &quot;default&quot;: { &quot;tags_dictionary_path&quot;: null, &quot;tags_corpus_ratio&quot;: 0.1, &quot;max_tags&quot;: 12, &quot;paired_stag&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;paired_etag&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;isolated_tag&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;src_delimiter&quot;: &quot;\\uff5ffuzzy\\uff60&quot; } }, &quot;clean&quot;: { &quot;$ref&quot;: &quot;#/$defs/CleanConfig&quot;, &quot;default&quot;: { &quot;src_eq_tgt&quot;: false, &quot;same_char&quot;: false, &quot;same_word&quot;: false, &quot;scripts_ok&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;scripts_nok&quot;: [], &quot;src_tgt_ratio&quot;: 2.0, &quot;avg_tok_min&quot;: 3.0, &quot;avg_tok_max&quot;: 20.0, &quot;langid&quot;: [] } } }, &quot;title&quot;: &quot;NestedAllTransformsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;NormalizeConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Source language code&quot;, &quot;title&quot;: &quot;Src Lang&quot; }, &quot;tgt_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Target language code&quot;, &quot;title&quot;: &quot;Tgt Lang&quot; }, &quot;penn&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Penn substitution&quot;, &quot;title&quot;: &quot;Penn&quot; }, &quot;norm_quote_commas&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize quotations and commas&quot;, &quot;title&quot;: &quot;Norm Quote Commas&quot; }, &quot;norm_numbers&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize numbers&quot;, &quot;title&quot;: &quot;Norm Numbers&quot; }, &quot;pre_replace_unicode_punct&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Replace unicode punct&quot;, &quot;title&quot;: &quot;Pre Replace Unicode Punct&quot; }, &quot;post_remove_control_chars&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove control chars&quot;, &quot;title&quot;: &quot;Post Remove Control Chars&quot; } }, &quot;title&quot;: &quot;NormalizeConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;ONMTTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; }, &quot;src_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for src (or shared) in pyonmttok.&quot;, &quot;title&quot;: &quot;Src Subword Type&quot; }, &quot;tgt_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for tgt in pyonmttok.&quot;, &quot;title&quot;: &quot;Tgt Subword Type&quot; }, &quot;src_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for src in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Src Onmttok Kwargs&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for tgt in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Tgt Onmttok Kwargs&quot; }, &quot;gpt2_pretok&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Preprocess sentence with byte-level mapping.&quot;, &quot;title&quot;: &quot;Gpt2 Pretok&quot; }, &quot;mapped_tokens&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;maxItems&quot;: 2, &quot;minItems&quot;: 2, &quot;prefixItems&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;string&quot; } ], &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Mapped tokens for placeholders preservation&quot;, &quot;title&quot;: &quot;Mapped Tokens&quot; } }, &quot;title&quot;: &quot;ONMTTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PrefixConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all source examples.&quot;, &quot;title&quot;: &quot;Src Prefix&quot; }, &quot;tgt_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Prefix&quot; } }, &quot;title&quot;: &quot;PrefixConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;SuffixConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all source examples.&quot;, &quot;title&quot;: &quot;Src Suffix&quot; }, &quot;tgt_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Suffix&quot; } }, &quot;title&quot;: &quot;SuffixConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;SwitchOutConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;switchout_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for SwitchOut. :math:`\\\\tau^{-1}` in :cite:`DBLP:journals/corr/abs-1808-07512`. Smaller value makes data more diverse.&quot;, &quot;title&quot;: &quot;Switchout Temperature&quot; } }, &quot;title&quot;: &quot;SwitchOutConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TerminologyConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;termbase_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a dictionary file with terms.&quot;, &quot;title&quot;: &quot;Termbase Path&quot; }, &quot;src_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the source corpus.&quot;, &quot;title&quot;: &quot;Src Spacy Language Model&quot; }, &quot;tgt_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the target corpus.&quot;, &quot;title&quot;: &quot;Tgt Spacy Language Model&quot; }, &quot;term_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.3, &quot;description&quot;: &quot;Ratio of corpus to augment with terms.&quot;, &quot;title&quot;: &quot;Term Corpus Ratio&quot; }, &quot;term_example_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.2, &quot;description&quot;: &quot;Maximum terms allowed in an example.&quot;, &quot;title&quot;: &quot;Term Example Ratio&quot; }, &quot;src_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;description&quot;: &quot;The source term start token.&quot;, &quot;title&quot;: &quot;Src Term Stoken&quot; }, &quot;tgt_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;description&quot;: &quot;The target term start token.&quot;, &quot;title&quot;: &quot;Tgt Term Stoken&quot; }, &quot;tgt_term_etoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;description&quot;: &quot;The target term end token.&quot;, &quot;title&quot;: &quot;Tgt Term Etoken&quot; }, &quot;term_source_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented source sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Term Source Delimiter&quot; } }, &quot;title&quot;: &quot;TerminologyConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TokenDropConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tokendrop_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token deletion.&quot;, &quot;title&quot;: &quot;Tokendrop Temperature&quot; } }, &quot;title&quot;: &quot;TokenDropConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TokenMaskConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tokenmask_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token masking.&quot;, &quot;title&quot;: &quot;Tokenmask Temperature&quot; } }, &quot;title&quot;: &quot;TokenMaskConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;UpperCaseConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;upper_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.01, &quot;description&quot;: &quot;Corpus ratio to apply uppercasing.&quot;, &quot;title&quot;: &quot;Upper Corpus Ratio&quot; } }, &quot;title&quot;: &quot;UpperCaseConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false, &quot;required&quot;: [ &quot;src_vocab&quot;, &quot;data&quot; ] }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: data (Dict[str, eole.config.data.Dataset] | None)n_sample (int | None)overwrite (bool)save_data (str | None)skip_empty_level (Literal['silent', 'warning', 'error'])transforms (List[str])transforms_configs (eole.config.data.NestedAllTransformsConfig | None) Validators: _maybe_set_huggingface_model » all fields_str_to_dict » transforms_configs  field data : Dict[str, Dataset] | None [Required]​  All datasets and their specifications. See examples/  *   .yaml for further details.  Validated by: _maybe_set_huggingface_model  field n_sample : int | None = 0​  Validated by: _maybe_set_huggingface_model  field overwrite : bool = False​  Overwrite existing objects if any.  Validated by: _maybe_set_huggingface_model  field save_data : str | None = None​  Output base path for objects that will be saved (vocab, transforms, embeddings, …)  Validated by: _maybe_set_huggingface_model  field skip_empty_level : Literal['silent', 'warning', 'error'] = 'warning'​  Logging level when encoutering empty examples. (silent: silently ignore/skip empty examples, warning: warn when ignoring/skipping empty examples, error: raise an error and stop execution when any empty example)  Validated by: _maybe_set_huggingface_model  field transforms : List[str] = []​  Default transform pipeline to apply to data. Can be specified in each corpus of data to override.  Validated by: _maybe_set_huggingface_model  field transforms_configs : NestedAllTransformsConfig | None [Optional]​  Validated by: _maybe_set_huggingface_model_str_to_dict  model_post_init(context: Any, /)​  This function is meant to behave like a BaseModel method to initialise private attributes.  It takes context as an argument since that’s what pydantic-core passes when calling it.  Parameters: self – The BaseModel instance.context – The context.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.data.NestedAllTransformsConfig​","type":1,"pageTitle":"Data","url":"/eole/docs/reference/Config/data#pydantic-model-eoleconfigdatanestedalltransformsconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;NestedAllTransformsConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;huggingface_tokenize&quot;: { &quot;$ref&quot;: &quot;#/$defs/HuggingfaceTokenizerConfig&quot;, &quot;default&quot;: { &quot;path&quot;: null, &quot;huggingface_model&quot;: null, &quot;max_length&quot;: null } }, &quot;uppercase&quot;: { &quot;$ref&quot;: &quot;#/$defs/UpperCaseConfig&quot;, &quot;default&quot;: { &quot;upper_corpus_ratio&quot;: 0.01 } }, &quot;terminology&quot;: { &quot;$ref&quot;: &quot;#/$defs/TerminologyConfig&quot;, &quot;default&quot;: { &quot;termbase_path&quot;: null, &quot;src_spacy_language_model&quot;: null, &quot;tgt_spacy_language_model&quot;: null, &quot;term_corpus_ratio&quot;: 0.3, &quot;term_example_ratio&quot;: 0.2, &quot;src_term_stoken&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;tgt_term_stoken&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;tgt_term_etoken&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;term_source_delimiter&quot;: &quot;\\uff5ffuzzy\\uff60&quot; } }, &quot;sentencepiece&quot;: { &quot;$ref&quot;: &quot;#/$defs/BaseTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0 } }, &quot;bpe&quot;: { &quot;$ref&quot;: &quot;#/$defs/BaseTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0 } }, &quot;onmt_tokenize&quot;: { &quot;$ref&quot;: &quot;#/$defs/ONMTTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0, &quot;src_subword_type&quot;: &quot;none&quot;, &quot;tgt_subword_type&quot;: &quot;none&quot;, &quot;src_onmttok_kwargs&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;gpt2_pretok&quot;: false, &quot;mapped_tokens&quot;: null } }, &quot;bart&quot;: { &quot;$ref&quot;: &quot;#/$defs/BARTNoiseConfig&quot;, &quot;default&quot;: { &quot;permute_sent_ratio&quot;: 0.0, &quot;rotate_ratio&quot;: 0.0, &quot;insert_ratio&quot;: 0.0, &quot;random_ratio&quot;: 0.0, &quot;mask_ratio&quot;: 0.0, &quot;mask_length&quot;: &quot;subword&quot;, &quot;poisson_lambda&quot;: 3.0, &quot;replace_length&quot;: -1 } }, &quot;switchout&quot;: { &quot;$ref&quot;: &quot;#/$defs/SwitchOutConfig&quot;, &quot;default&quot;: { &quot;switchout_temperature&quot;: 1.0 } }, &quot;tokendrop&quot;: { &quot;$ref&quot;: &quot;#/$defs/TokenDropConfig&quot;, &quot;default&quot;: { &quot;tokendrop_temperature&quot;: 1.0 } }, &quot;tokenmask&quot;: { &quot;$ref&quot;: &quot;#/$defs/TokenMaskConfig&quot;, &quot;default&quot;: { &quot;tokenmask_temperature&quot;: 1.0 } }, &quot;normalize&quot;: { &quot;$ref&quot;: &quot;#/$defs/NormalizeConfig&quot;, &quot;default&quot;: { &quot;src_lang&quot;: &quot;&quot;, &quot;tgt_lang&quot;: &quot;&quot;, &quot;penn&quot;: true, &quot;norm_quote_commas&quot;: true, &quot;norm_numbers&quot;: true, &quot;pre_replace_unicode_punct&quot;: false, &quot;post_remove_control_chars&quot;: false } }, &quot;insert_mask_before_placeholder&quot;: { &quot;$ref&quot;: &quot;#/$defs/InsertMaskBeforePlaceholderConfig&quot;, &quot;default&quot;: { &quot;response_patterns&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ] } }, &quot;filtertooshort&quot;: { &quot;$ref&quot;: &quot;#/$defs/FilterTooShortConfig&quot;, &quot;default&quot;: { &quot;src_seq_length&quot;: 48, &quot;tgt_seq_length&quot;: 48 } }, &quot;filtertoolong&quot;: { &quot;$ref&quot;: &quot;#/$defs/FilterTooLongConfig&quot;, &quot;default&quot;: { &quot;src_seq_length&quot;: 192, &quot;tgt_seq_length&quot;: 192 } }, &quot;prefix&quot;: { &quot;$ref&quot;: &quot;#/$defs/PrefixConfig&quot;, &quot;default&quot;: { &quot;src_prefix&quot;: &quot;&quot;, &quot;tgt_prefix&quot;: &quot;&quot; } }, &quot;suffix&quot;: { &quot;$ref&quot;: &quot;#/$defs/SuffixConfig&quot;, &quot;default&quot;: { &quot;src_suffix&quot;: &quot;&quot;, &quot;tgt_suffix&quot;: &quot;&quot; } }, &quot;docify&quot;: { &quot;$ref&quot;: &quot;#/$defs/DocifyConfig&quot;, &quot;default&quot;: { &quot;doc_length&quot;: 200, &quot;max_context&quot;: 1 } }, &quot;inlinetags&quot;: { &quot;$ref&quot;: &quot;#/$defs/InlineTagsConfig&quot;, &quot;default&quot;: { &quot;tags_dictionary_path&quot;: null, &quot;tags_corpus_ratio&quot;: 0.1, &quot;max_tags&quot;: 12, &quot;paired_stag&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;paired_etag&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;isolated_tag&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;src_delimiter&quot;: &quot;\\uff5ffuzzy\\uff60&quot; } }, &quot;clean&quot;: { &quot;$ref&quot;: &quot;#/$defs/CleanConfig&quot;, &quot;default&quot;: { &quot;src_eq_tgt&quot;: false, &quot;same_char&quot;: false, &quot;same_word&quot;: false, &quot;scripts_ok&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;scripts_nok&quot;: [], &quot;src_tgt_ratio&quot;: 2.0, &quot;avg_tok_min&quot;: 3.0, &quot;avg_tok_max&quot;: 20.0, &quot;langid&quot;: [] } } }, &quot;$defs&quot;: { &quot;BARTNoiseConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;permute_sent_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Permute this proportion of sentences (boundaries defined by ['.', '?', '!']) in all inputs.&quot;, &quot;title&quot;: &quot;Permute Sent Ratio&quot; }, &quot;rotate_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rotate this proportion of inputs.&quot;, &quot;title&quot;: &quot;Rotate Ratio&quot; }, &quot;insert_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Insert this percentage of additional random tokens.&quot;, &quot;title&quot;: &quot;Insert Ratio&quot; }, &quot;random_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Instead of using &lt;mask&gt;, use random token this often.&quot;, &quot;title&quot;: &quot;Random Ratio&quot; }, &quot;mask_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Fraction of words/subwords that will be masked.&quot;, &quot;title&quot;: &quot;Mask Ratio&quot; }, &quot;mask_length&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;subword&quot;, &quot;word&quot;, &quot;span-poisson&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;subword&quot;, &quot;description&quot;: &quot;Length of masking window to apply.&quot;, &quot;title&quot;: &quot;Mask Length&quot; }, &quot;poisson_lambda&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;Lambda for Poisson distribution to sample span length if `-mask_length` set to span-poisson.&quot;, &quot;title&quot;: &quot;Poisson Lambda&quot; }, &quot;replace_length&quot;: { &quot;anyOf&quot;: [ { &quot;maximum&quot;: 1, &quot;minimum&quot;: -1, &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: -1, &quot;description&quot;: &quot;When masking N tokens, replace with 0, 1, or N tokens. (use -1 for N)&quot;, &quot;title&quot;: &quot;Replace Length&quot; } }, &quot;title&quot;: &quot;BARTNoiseConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;BaseTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; } }, &quot;title&quot;: &quot;BaseTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CleanConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_eq_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex src==tgt&quot;, &quot;title&quot;: &quot;Src Eq Tgt&quot; }, &quot;same_char&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same char more than 4 times&quot;, &quot;title&quot;: &quot;Same Char&quot; }, &quot;same_word&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same word more than 3 times&quot;, &quot;title&quot;: &quot;Same Word&quot; }, &quot;scripts_ok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;description&quot;: &quot;list of unicodata scripts accepted&quot;, &quot;title&quot;: &quot;Scripts Ok&quot; }, &quot;scripts_nok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of unicodata scripts not accepted&quot;, &quot;title&quot;: &quot;Scripts Nok&quot; }, &quot;src_tgt_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 2.0, &quot;description&quot;: &quot;ratio between src and tgt&quot;, &quot;title&quot;: &quot;Src Tgt Ratio&quot; }, &quot;avg_tok_min&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;average length of tokens min&quot;, &quot;title&quot;: &quot;Avg Tok Min&quot; }, &quot;avg_tok_max&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 20.0, &quot;description&quot;: &quot;average length of tokens max&quot;, &quot;title&quot;: &quot;Avg Tok Max&quot; }, &quot;langid&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of languages accepted&quot;, &quot;title&quot;: &quot;Langid&quot; } }, &quot;title&quot;: &quot;CleanConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;DocifyConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;doc_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 200, &quot;description&quot;: &quot;Number of tokens per doc.&quot;, &quot;title&quot;: &quot;Doc Length&quot; }, &quot;max_context&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Max context segments.&quot;, &quot;title&quot;: &quot;Max Context&quot; } }, &quot;title&quot;: &quot;DocifyConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;FilterTooLongConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;title&quot;: &quot;FilterTooLongConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;FilterTooShortConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 48, &quot;description&quot;: &quot;Minimum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 48, &quot;description&quot;: &quot;Minimum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;title&quot;: &quot;FilterTooShortConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;HuggingfaceTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;max_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Max Length&quot; } }, &quot;title&quot;: &quot;HuggingfaceTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InlineTagsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tags_dictionary_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a flat term dictionary.&quot;, &quot;title&quot;: &quot;Tags Dictionary Path&quot; }, &quot;tags_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.1, &quot;description&quot;: &quot;Ratio of corpus to augment with tags.&quot;, &quot;title&quot;: &quot;Tags Corpus Ratio&quot; }, &quot;max_tags&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 12, &quot;description&quot;: &quot;Maximum number of tags that can be added to a single sentence.&quot;, &quot;title&quot;: &quot;Max Tags&quot; }, &quot;paired_stag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;description&quot;: &quot;The format of an opening paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Stag&quot; }, &quot;paired_etag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;description&quot;: &quot;The format of a closing paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Etag&quot; }, &quot;isolated_tag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;description&quot;: &quot;The format of an isolated inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Isolated Tag&quot; }, &quot;src_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented src sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Src Delimiter&quot; } }, &quot;title&quot;: &quot;InlineTagsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InsertMaskBeforePlaceholderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;response_patterns&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ], &quot;description&quot;: &quot;Response pattern to locate the end of the prompt.&quot;, &quot;title&quot;: &quot;Response Patterns&quot; } }, &quot;title&quot;: &quot;InsertMaskBeforePlaceholderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;NormalizeConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Source language code&quot;, &quot;title&quot;: &quot;Src Lang&quot; }, &quot;tgt_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Target language code&quot;, &quot;title&quot;: &quot;Tgt Lang&quot; }, &quot;penn&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Penn substitution&quot;, &quot;title&quot;: &quot;Penn&quot; }, &quot;norm_quote_commas&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize quotations and commas&quot;, &quot;title&quot;: &quot;Norm Quote Commas&quot; }, &quot;norm_numbers&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize numbers&quot;, &quot;title&quot;: &quot;Norm Numbers&quot; }, &quot;pre_replace_unicode_punct&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Replace unicode punct&quot;, &quot;title&quot;: &quot;Pre Replace Unicode Punct&quot; }, &quot;post_remove_control_chars&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove control chars&quot;, &quot;title&quot;: &quot;Post Remove Control Chars&quot; } }, &quot;title&quot;: &quot;NormalizeConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;ONMTTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; }, &quot;src_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for src (or shared) in pyonmttok.&quot;, &quot;title&quot;: &quot;Src Subword Type&quot; }, &quot;tgt_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for tgt in pyonmttok.&quot;, &quot;title&quot;: &quot;Tgt Subword Type&quot; }, &quot;src_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for src in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Src Onmttok Kwargs&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for tgt in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Tgt Onmttok Kwargs&quot; }, &quot;gpt2_pretok&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Preprocess sentence with byte-level mapping.&quot;, &quot;title&quot;: &quot;Gpt2 Pretok&quot; }, &quot;mapped_tokens&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;maxItems&quot;: 2, &quot;minItems&quot;: 2, &quot;prefixItems&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;string&quot; } ], &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Mapped tokens for placeholders preservation&quot;, &quot;title&quot;: &quot;Mapped Tokens&quot; } }, &quot;title&quot;: &quot;ONMTTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PrefixConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all source examples.&quot;, &quot;title&quot;: &quot;Src Prefix&quot; }, &quot;tgt_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Prefix&quot; } }, &quot;title&quot;: &quot;PrefixConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;SuffixConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all source examples.&quot;, &quot;title&quot;: &quot;Src Suffix&quot; }, &quot;tgt_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Suffix&quot; } }, &quot;title&quot;: &quot;SuffixConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;SwitchOutConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;switchout_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for SwitchOut. :math:`\\\\tau^{-1}` in :cite:`DBLP:journals/corr/abs-1808-07512`. Smaller value makes data more diverse.&quot;, &quot;title&quot;: &quot;Switchout Temperature&quot; } }, &quot;title&quot;: &quot;SwitchOutConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TerminologyConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;termbase_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a dictionary file with terms.&quot;, &quot;title&quot;: &quot;Termbase Path&quot; }, &quot;src_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the source corpus.&quot;, &quot;title&quot;: &quot;Src Spacy Language Model&quot; }, &quot;tgt_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the target corpus.&quot;, &quot;title&quot;: &quot;Tgt Spacy Language Model&quot; }, &quot;term_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.3, &quot;description&quot;: &quot;Ratio of corpus to augment with terms.&quot;, &quot;title&quot;: &quot;Term Corpus Ratio&quot; }, &quot;term_example_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.2, &quot;description&quot;: &quot;Maximum terms allowed in an example.&quot;, &quot;title&quot;: &quot;Term Example Ratio&quot; }, &quot;src_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;description&quot;: &quot;The source term start token.&quot;, &quot;title&quot;: &quot;Src Term Stoken&quot; }, &quot;tgt_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;description&quot;: &quot;The target term start token.&quot;, &quot;title&quot;: &quot;Tgt Term Stoken&quot; }, &quot;tgt_term_etoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;description&quot;: &quot;The target term end token.&quot;, &quot;title&quot;: &quot;Tgt Term Etoken&quot; }, &quot;term_source_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented source sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Term Source Delimiter&quot; } }, &quot;title&quot;: &quot;TerminologyConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TokenDropConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tokendrop_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token deletion.&quot;, &quot;title&quot;: &quot;Tokendrop Temperature&quot; } }, &quot;title&quot;: &quot;TokenDropConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TokenMaskConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tokenmask_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token masking.&quot;, &quot;title&quot;: &quot;Tokenmask Temperature&quot; } }, &quot;title&quot;: &quot;TokenMaskConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;UpperCaseConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;upper_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.01, &quot;description&quot;: &quot;Corpus ratio to apply uppercasing.&quot;, &quot;title&quot;: &quot;Upper Corpus Ratio&quot; } }, &quot;title&quot;: &quot;UpperCaseConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: bart (eole.transforms.bart.BARTNoiseConfig)bpe (eole.transforms.tokenize.BaseTokenizerConfig)clean (eole.transforms.clean.CleanConfig)docify (eole.transforms.docify.DocifyConfig)filtertoolong (eole.transforms.misc.FilterTooLongConfig)filtertooshort (eole.transforms.misc.FilterTooShortConfig)huggingface_tokenize (eole.transforms.tokenize_id.HuggingfaceTokenizerConfig)inlinetags (eole.transforms.inlinetags.InlineTagsConfig)insert_mask_before_placeholder (eole.transforms.insert_mask_before_placeholder.InsertMaskBeforePlaceholderConfig)normalize (eole.transforms.normalize.NormalizeConfig)onmt_tokenize (eole.transforms.tokenize.ONMTTokenizerConfig)prefix (eole.transforms.misc.PrefixConfig)sentencepiece (eole.transforms.tokenize.BaseTokenizerConfig)suffix (eole.transforms.misc.SuffixConfig)switchout (eole.transforms.sampling.SwitchOutConfig)terminology (eole.transforms.terminology.TerminologyConfig)tokendrop (eole.transforms.sampling.TokenDropConfig)tokenmask (eole.transforms.sampling.TokenMaskConfig)uppercase (eole.transforms.uppercase.UpperCaseConfig)  field bart : BARTNoiseConfig = BARTNoiseConfig(permute_sent_ratio=0.0, rotate_ratio=0.0, insert_ratio=0.0, random_ratio=0.0, mask_ratio=0.0, mask_length='subword', poisson_lambda=3.0, replace_length=-1)​  field bpe : BaseTokenizerConfig = BaseTokenizerConfig(src_subword_model=None, tgt_subword_model=None, src_subword_nbest=1, tgt_subword_nbest=1, src_subword_alpha=0.0, tgt_subword_alpha=0.0, src_subword_vocab='', tgt_subword_vocab='', src_vocab_threshold=0, tgt_vocab_threshold=0)​  field clean : CleanConfig = CleanConfig(src_eq_tgt=False, same_char=False, same_word=False, scripts_ok=['Latin', 'Common'], scripts_nok=[], src_tgt_ratio=2.0, avg_tok_min=3.0, avg_tok_max=20.0, langid=[])​  field docify : DocifyConfig = DocifyConfig(doc_length=200, max_context=1)​  field filtertoolong : FilterTooLongConfig = FilterTooLongConfig(src_seq_length=192, tgt_seq_length=192)​  field filtertooshort : FilterTooShortConfig = FilterTooShortConfig(src_seq_length=48, tgt_seq_length=48)​  field huggingface_tokenize : HuggingfaceTokenizerConfig = HuggingfaceTokenizerConfig(path=None, huggingface_model=None, max_length=None)​  field inlinetags : InlineTagsConfig = InlineTagsConfig(tags_dictionary_path=None, tags_corpus_ratio=0.1, max_tags=12, paired_stag='｟ph_#beg｠', paired_etag='｟ph#end｠', isolated_tag='｟ph#_std｠', src_delimiter='｟fuzzy｠')​  field insert_mask_before_placeholder : InsertMaskBeforePlaceholderConfig = InsertMaskBeforePlaceholderConfig(response_patterns=['Response : ｟newline｠'])​  field normalize : NormalizeConfig = NormalizeConfig(src_lang='', tgt_lang='', penn=True, norm_quote_commas=True, norm_numbers=True, pre_replace_unicode_punct=False, post_remove_control_chars=False)​  field onmt_tokenize : ONMTTokenizerConfig = ONMTTokenizerConfig(src_subword_model=None, tgt_subword_model=None, src_subword_nbest=1, tgt_subword_nbest=1, src_subword_alpha=0.0, tgt_subword_alpha=0.0, src_subword_vocab='', tgt_subword_vocab='', src_vocab_threshold=0, tgt_vocab_threshold=0, src_subword_type='none', tgt_subword_type='none', src_onmttok_kwargs={'mode': 'none'}, tgt_onmttok_kwargs={'mode': 'none'}, gpt2_pretok=False, mapped_tokens=None)​  field prefix : PrefixConfig = PrefixConfig(src_prefix='', tgt_prefix='')​  field sentencepiece : BaseTokenizerConfig = BaseTokenizerConfig(src_subword_model=None, tgt_subword_model=None, src_subword_nbest=1, tgt_subword_nbest=1, src_subword_alpha=0.0, tgt_subword_alpha=0.0, src_subword_vocab='', tgt_subword_vocab='', src_vocab_threshold=0, tgt_vocab_threshold=0)​  field suffix : SuffixConfig = SuffixConfig(src_suffix='', tgt_suffix='')​  field switchout : SwitchOutConfig = SwitchOutConfig(switchout_temperature=1.0)​  field terminology : TerminologyConfig = TerminologyConfig(termbase_path=None, src_spacy_language_model=None, tgt_spacy_language_model=None, term_corpus_ratio=0.3, term_example_ratio=0.2, src_term_stoken='｟src_term_start｠', tgt_term_stoken='｟tgt_term_start｠', tgt_term_etoken='｟tgt_term_end｠', term_source_delimiter='｟fuzzy｠')​  field tokendrop : TokenDropConfig = TokenDropConfig(tokendrop_temperature=1.0)​  field tokenmask : TokenMaskConfig = TokenMaskConfig(tokenmask_temperature=1.0)​  field uppercase : UpperCaseConfig = UpperCaseConfig(upper_corpus_ratio=0.01)​ ","version":"Next","tagName":"h3"},{"title":"Prediction","type":0,"sectionRef":"#","url":"/eole/docs/reference/Core API/inference","content":"","keywords":"","version":"Next"},{"title":"Predictions​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#predictions","content":" ","version":"Next","tagName":"h2"},{"title":"class eole.predict.prediction.PredictionBuilder(vocabs, n_best=1, replace_unk=False, phrase_table='', tgt_eos_idx=None, id_tokenization=False)​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#class-eolepredictpredictionpredictionbuildervocabs-n_best1-replace_unkfalse-phrase_table-tgt_eos_idxnone-id_tokenizationfalse","content":" Bases: object  Build a word-based prediction from the batch output of predictor and the underlying dictionaries.  Replacement based on “Addressing the Rare Word Problem in Neural Machine Translation” []  Parameters: (****) (vocabs)(****)n_best (int) – number of predictions producedreplace_unk (bool) – replace unknown words using attention  ","version":"Next","tagName":"h3"},{"title":"Predictor Classes​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#predictor-classes","content":" ","version":"Next","tagName":"h2"},{"title":"class eole.predict.inference.Inference(model, vocabs, gpu=-1, n_best=1, min_length=0, max_length=100, max_length_ratio=1.5, ratio=0.0, beam_size=30, top_k=0, top_p=0.0, temperature=1.0, stepwise_penalty=None, dump_beam=False, block_ngram_repeat=0, ignore_when_blocking=frozenset({}), replace_unk=False, ban_unk_token=False, tgt_file_prefix=False, phrase_table='', data_type='text', verbose=False, report_time=False, global_scorer=None, report_align=False, gold_align=False, report_score=True, logger=None, seed=-1, with_score=False, estim_only=False, return_gold_log_probs=False, add_estimator=False, estimator_type='average', optional_eos=[], id_tokenization=False, image_token_id=10, fuse_kvq=False, fuse_gate=False)​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#class-eolepredictinferenceinferencemodel-vocabs-gpu-1-n_best1-min_length0-max_length100-max_length_ratio15-ratio00-beam_size30-top_k0-top_p00-temperature10-stepwise_penaltynone-dump_beamfalse-block_ngram_repeat0-ignore_when_blockingfrozenset-replace_unkfalse-ban_unk_tokenfalse-tgt_file_prefixfalse-phrase_table-data_typetext-verbosefalse-report_timefalse-global_scorernone-report_alignfalse-gold_alignfalse-report_scoretrue-loggernone-seed-1-with_scorefalse-estim_onlyfalse-return_gold_log_probsfalse-add_estimatorfalse-estimator_typeaverage-optional_eos-id_tokenizationfalse-image_token_id10-fuse_kvqfalse-fuse_gatefalse","content":" Bases: object  Predict a batch of sentences with a saved model.  Parameters: model (eole.modules.BaseModel) – Model to use for predictionvocabs (dict *[*str , Vocab ]) – A dict mapping each side’s Vocab.gpu (int) – GPU device. Set to negative for no GPU.n_best (int) – How many beams to wait for.min_length (int) – Seeeole.predict.decode_strategy.DecodeStrategy.max_length (int) – Seeeole.predict.decode_strategy.DecodeStrategy.beam_size (int) – Number of beams.top_p (float) – Seeeole.predict.greedy_search.GreedySearch.top_k (int) – Seeeole.predict.greedy_search.GreedySearch.temperature (float) – Seeeole.predict.greedy_search.GreedySearch.stepwise_penalty (bool) – Whether coverage penalty is applied every step or not.dump_beam (bool) – Debugging option.block_ngram_repeat (int) – Seeeole.predict.decode_strategy.DecodeStrategy.ignore_when_blocking (set or frozenset) – Seeeole.predict.decode_strategy.DecodeStrategy.replace_unk (bool) – Replace unknown token.tgt_file_prefix (bool) – Force the predictions begin with provided -tgt.data_type (str) – Source data type.verbose (bool) – Print/log every prediction.report_time (bool) – Print/log total time/frequency.global_scorer (eole.predict.GNMTGlobalScorer) – Prediction scoring/reranking object.report_score (bool) – Whether to report scoreslogger (logging.Logger or NoneType) – Logger.  classmethod from_config(model, vocabs, config, model_config, device_id=0, global_scorer=None, report_align=False, report_score=True, logger=None)​  Alternate constructor.  Parameters: model (eole.modules.BaseModel) – See __init__().vocabs (dict *[*str , Vocab ]) – See__init__().opt (argparse.Namespace) – Command line optionsmodel_opt (argparse.Namespace) – Command line options saved with the model checkpoint.global_scorer (eole.predict.GNMTGlobalScorer) – See__init__()..report_align (bool) – See __init__().report_score (bool) – See __init__().logger (logging.Logger or NoneType) – See __init__().  predict_batch(batch, attn_debug)​  Predict a batch of sentences.  ","version":"Next","tagName":"h3"},{"title":"Decoding Strategies​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#decoding-strategies","content":" ","version":"Next","tagName":"h2"},{"title":"eole.predict.greedy_search.sample_with_temperature(logits, temperature, top_k, top_p)​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#eolepredictgreedy_searchsample_with_temperaturelogits-temperature-top_k-top_p","content":" Select next tokens randomly from the top k possible next tokens.  Samples from a categorical distribution over the top_k words using the category probabilities logits / temperature.  Parameters: logits (FloatTensor) – Shaped (batch_size, vocab_size). These can be logits ((-inf, inf)) or log-probs ((-inf, 0]). (The distribution actually uses the log-probabilitieslogits - logits.logsumexp(-1), which equals the logits if they are log-probabilities summing to 1.)temperature (float) – Used to scale down logits. The higher the value, the more likely it is that a non-max word will be sampled.top_k (int) – This many words could potentially be chosen. The other logits are set to have probability 0.top_p (float) – Keep most likely words until the cumulated probability is greater than p. If used with top_k: both conditions will be applied Returns: topk_ids: Shaped (batch_size, 1). These are the sampled word indices in the output vocab.topk_scores: Shaped (batch_size, 1). These are essentially (logits / temperature)[topk_ids]. Return type:(LongTensor, FloatTensor)  ","version":"Next","tagName":"h3"},{"title":"Scoring​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#scoring","content":" ","version":"Next","tagName":"h2"},{"title":"class eole.predict.penalties.PenaltyBuilder(cov_pen, length_pen)​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#class-eolepredictpenaltiespenaltybuildercov_pen-length_pen","content":" Bases: object  Returns the Length and Coverage Penalty function for Beam Search.  Parameters: length_pen (str) – option name of length pencov_pen (str) – option name of cov pen Variables: has_cov_pen (bool) – Whether coverage penalty is None (applying it is a no-op). Note that the converse isn’t true. Setting beta to 0 should force coverage length to be a no-op.has_len_pen (bool) – Whether length penalty is None (applying it is a no-op). Note that the converse isn’t true. Setting alpha to 1 should force length penalty to be a no-op.coverage_penalty (callable [ *[*FloatTensor , float ] , FloatTensor ]) – Calculates the coverage penalty.length_penalty (callable [ *[*int , float ] , float ]) – Calculates the length penalty.  coverage_none(cov, beta=0.0)​  Returns zero as penalty  coverage_summary(cov, beta=0.0)​  Our summary penalty.  coverage_wu(cov, beta=0.0)​  GNMT coverage re-ranking score.  See “Google’s Neural Machine Translation System” [].cov is expected to be sized (*, seq_len), where * is probably batch_size x beam_size but could be several dimensions like (batch_size, beam_size). If cov is attention, then the seq_len axis probably sums to (almost) 1.  length_average(cur_len, alpha=1.0)​  Returns the current sequence length.  length_none(cur_len, alpha=0.0)​  Returns unmodified scores.  length_wu(cur_len, alpha=0.0)​  GNMT length re-ranking score.  See “Google’s Neural Machine Translation System” []. ","version":"Next","tagName":"h3"},{"title":"Eole Core API","type":0,"sectionRef":"#","url":"/eole/docs/reference/index","content":"Eole Core API Note : These sections are built via sphinx and converted into Markdown. Some layout or links might be shaky.","keywords":"","version":"Next"},{"title":"Modules","type":0,"sectionRef":"#","url":"/eole/docs/reference/Core API/modules","content":"","keywords":"","version":"Next"},{"title":"Embeddings​","type":1,"pageTitle":"Modules","url":"/eole/docs/reference/Core API/modules#embeddings","content":" ","version":"Next","tagName":"h2"},{"title":"Encoders​","type":1,"pageTitle":"Modules","url":"/eole/docs/reference/Core API/modules#encoders","content":" ","version":"Next","tagName":"h2"},{"title":"Decoders​","type":1,"pageTitle":"Modules","url":"/eole/docs/reference/Core API/modules#decoders","content":" ","version":"Next","tagName":"h2"},{"title":"Attention​","type":1,"pageTitle":"Modules","url":"/eole/docs/reference/Core API/modules#attention","content":"","version":"Next","tagName":"h2"},{"title":"Models","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/models","content":"","keywords":"","version":"Next"},{"title":"Base Configs​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#base-configs","content":" ","version":"Next","tagName":"h2"},{"title":"pydantic model eole.config.models.BaseModelConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelsbasemodelconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;BaseModelConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Architecture&quot; } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;CnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;EmbeddingsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;freeze_word_vecs_enc&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Enc&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_word_vecs_dec&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Dec&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.&quot;, &quot;title&quot;: &quot;Position Encoding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;position_shift&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl&quot;, &quot;title&quot;: &quot;Position Shift&quot; }, &quot;normalize&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909&quot;, &quot;title&quot;: &quot;Normalize&quot; } }, &quot;title&quot;: &quot;EmbeddingsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;MeanEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;mean&quot;, &quot;default&quot;: &quot;mean&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;MeanEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;RnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;context_gate&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Type of context gate to use.&quot;, &quot;enum&quot;: [ &quot;source&quot;, &quot;target&quot;, &quot;both&quot;, null ], &quot;title&quot;: &quot;Context Gate&quot; }, &quot;bidirectional_encoder&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Bidirectional Encoder&quot; } }, &quot;title&quot;: &quot;RnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;default&quot;: &quot;rnn&quot;, &quot;enum&quot;: [ &quot;rnn&quot;, &quot;brnn&quot; ], &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;RnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;aan_useffn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Turn on the FFN layer in the AAN decoder.&quot;, &quot;title&quot;: &quot;Aan Useffn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;alignment_layer&quot;: { &quot;default&quot;: -2, &quot;description&quot;: &quot;Layer number which has to be supervised.&quot;, &quot;title&quot;: &quot;Alignment Layer&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;alignment_heads&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of cross attention heads per layer to supervise with.&quot;, &quot;title&quot;: &quot;Alignment Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;full_context_alignment&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Whether alignment is conditioned on full target context.&quot;, &quot;title&quot;: &quot;Full Context Alignment&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_align&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)&quot;, &quot;title&quot;: &quot;Lambda Align&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;LM_type&quot;: { &quot;default&quot;: &quot;causal&quot;, &quot;description&quot;: &quot;TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)&quot;, &quot;enum&quot;: [ &quot;causal&quot;, &quot;prefix&quot; ], &quot;title&quot;: &quot;Lm Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;TransformerDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;VisionEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Based on mistral-community/pixtral-12b, might evolve later.&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;vision&quot;, &quot;default&quot;: &quot;vision&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;num_channels&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Num Channels&quot; }, &quot;image_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1024, &quot;title&quot;: &quot;Image Size&quot; }, &quot;patch_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 16, &quot;title&quot;: &quot;Patch Size&quot; }, &quot;image_token_id&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 10, &quot;title&quot;: &quot;Image Token Id&quot; }, &quot;mm_tokens_per_image&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 256, &quot;title&quot;: &quot;Mm Tokens Per Image&quot; }, &quot;layernorm_pre&quot;: { &quot;default&quot;: true, &quot;title&quot;: &quot;Layernorm Pre&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;patch_conv_bias&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Patch Conv Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;encoder_sam&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Encoder Sam&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;use_class_embedding&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Use Class Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; } }, &quot;title&quot;: &quot;VisionEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: adapter_bias (bool)add_estimator (bool)architecture (str | None)decoder (eole.config.models.TransformerDecoderConfig | eole.config.models.RnnDecoderConfig | eole.config.models.CnnDecoderConfig | None)embeddings (eole.config.models.EmbeddingsConfig)encoder (eole.config.models.TransformerEncoderConfig | eole.config.models.RnnEncoderConfig | eole.config.models.CnnEncoderConfig | eole.config.models.MeanEncoderConfig | eole.config.models.VisionEncoderConfig | None)eole_version (str | None)estimator_type (Literal['average', 'last_token', 'first_token'])generator_bias (bool)generator_function (Literal['softmax', 'sparsemax'])hidden_size (int)huggingface_model (str | None)input_feed (int)layers (int)left_pad (bool)moe_transformer_ff (int | None)projector_activation_fn (eole.constants.ActivationFunction | None)share_decoder_embeddings (bool)share_embeddings (bool)spatial_merge_size (int | None)transformer_ff (int)word_vec_size (int) Validators: _override_values » all fields_validate_model_config » all fieldsdefault_architecture » all fieldsstr_to_dict » decoderstr_to_dict » embeddingsstr_to_dict » encodervalidate_merge_size » spatial_merge_size  field adapter_bias : bool = False​  Control whether or not the adapter module has bias weights.  Validated by: _override_values_validate_model_configdefault_architecture  field add_estimator : bool = False​  Add estimator layer  Validated by: _override_values_validate_model_configdefault_architecture  field architecture : str | None = None​  Validated by: _override_values_validate_model_configdefault_architecture  field decoder : TransformerDecoderConfig | RnnDecoderConfig | CnnDecoderConfig | None = None​  Major parameters of a decoder.  Validated by: _override_values_validate_model_configdefault_architecturestr_to_dict  field embeddings : EmbeddingsConfig [Optional]​  Contains most of the args useful to build the Embeddings module.  Validated by: _override_values_validate_model_configdefault_architecturestr_to_dict  field encoder : TransformerEncoderConfig | RnnEncoderConfig | CnnEncoderConfig | MeanEncoderConfig | VisionEncoderConfig | None = None​  Major parameters of an encoder.  Validated by: _override_values_validate_model_configdefault_architecturestr_to_dict  field eole_version : str | None = '0.4.0'​  Eole version used to convert/train/save the model.  Validated by: _override_values_validate_model_configdefault_architecture  field estimator_type : Literal['average', 'last_token', 'first_token'] = 'average'​  Which hidden_states to use to feed the estimator  Validated by: _override_values_validate_model_configdefault_architecture  field generator_bias : bool = True​  Control whether or not the generator Linear module has bias weights.  Validated by: _override_values_validate_model_configdefault_architecture  field generator_function : Literal['softmax', 'sparsemax'] = 'softmax'​  Which function to use for generating probabilities over the target vocabulary.  Validated by: _override_values_validate_model_configdefault_architecture  field hidden_size : int = -1​  Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.  Validated by: _override_values_validate_model_configdefault_architecture  field huggingface_model : str | None = None​  Original huggingface model.  Validated by: _override_values_validate_model_configdefault_architecture  field input_feed : int = 1​  Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.  Validated by: _override_values_validate_model_configdefault_architecture  field layers : int = -1​  Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).  Validated by: _override_values_validate_model_configdefault_architecture  field left_pad : bool = False​  Enable left-padding, useful for some LLMs.  Validated by: _override_values_validate_model_configdefault_architecture  field moe_transformer_ff : int | None = None​  Size of hidden moe transformer feed-forward.  Validated by: _override_values_validate_model_configdefault_architecture  field projector_activation_fn : ActivationFunction | None = ActivationFunction.relu​  The activation function to use in adapter projector layer.  Validated by: _override_values_validate_model_configdefault_architecture  field share_decoder_embeddings : bool = False​  Use a share weight matrix for the input and output word embeddings in the decoder.  Validated by: _override_values_validate_model_configdefault_architecture  field share_embeddings : bool = False​  Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.  Validated by: _override_values_validate_model_configdefault_architecture  field spatial_merge_size : int | None = 1​  Control the presence and size of patch merger (Mistral3)  Validated by: _override_values_validate_model_configdefault_architecturevalidate_merge_size  field transformer_ff : int = -1​  Size of hidden transformer feed-forward.  Validated by: _override_values_validate_model_configdefault_architecture  field word_vec_size : int = -1​  Word embedding size for src and tgt.  Validated by: _override_values_validate_model_configdefault_architecture  validator default_architecture » all fields​  validator str_to_dict » encoder , decoder , embeddings​  update_model_opts()​  validator validate_merge_size » spatial_merge_size​  property model_type : ModelType​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.EmbeddingsConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelsembeddingsconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;EmbeddingsConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;freeze_word_vecs_enc&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Enc&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_word_vecs_dec&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Dec&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.&quot;, &quot;title&quot;: &quot;Position Encoding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;position_shift&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl&quot;, &quot;title&quot;: &quot;Position Shift&quot; }, &quot;normalize&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909&quot;, &quot;title&quot;: &quot;Normalize&quot; } }, &quot;$defs&quot;: { &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: freeze_word_vecs_dec (bool)freeze_word_vecs_enc (bool)n_positions (int | None)normalize (bool | None)position_encoding (bool)position_encoding_type (eole.constants.PositionEncodingType | None)position_shift (int | None)src_word_vec_size (int)tgt_word_vec_size (int)word_vec_size (int) Validators: validate_embeddings » all fields  field freeze_word_vecs_dec : bool = False​  Freeze word embeddings on the encoder side.  Validated by: validate_embeddings  field freeze_word_vecs_enc : bool = False​  Freeze word embeddings on the encoder side.  Validated by: validate_embeddings  field n_positions : int | None = None​  Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative  Validated by: validate_embeddings  field normalize : bool | None = False​  Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909  Validated by: validate_embeddings  field position_encoding : bool = False​  Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.  Validated by: validate_embeddings  field position_encoding_type : PositionEncodingType | None = PositionEncodingType.SinusoidalInterleaved​  Type of positional encoding.  Validated by: validate_embeddings  field position_shift : int | None = 0​  Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl  Validated by: validate_embeddings  field src_word_vec_size : int = 512​  Word embedding size for src.  Validated by: validate_embeddings  field tgt_word_vec_size : int = 512​  Word embedding size for tgt.  Validated by: validate_embeddings  field word_vec_size : int = -1​  Word embedding size for src and tgt.  Validated by: validate_embeddings  validator validate_embeddings » all fields​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.EncoderConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelsencoderconfig","content":" Bases: Config  Abstract class for all encoders   Show JSON schema { &quot;title&quot;: &quot;EncoderConfig&quot;, &quot;description&quot;: &quot;Abstract class for all encoders&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;rnn&quot;, &quot;description&quot;: &quot;Type of encoder layer(s) to use.&quot;, &quot;title&quot;: &quot;Encoder Type&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: encoder_type (str | None)hidden_size (int)layers (int)src_word_vec_size (int)  field encoder_type : str | None = 'rnn'​  Type of encoder layer(s) to use.  field hidden_size : int = 512​  Size of encoder hidden states.  field layers : int = 2​  Number of layers in the encoder.  field src_word_vec_size : int = 512​  Word embedding size for src.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.DecoderConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelsdecoderconfig","content":" Bases: Config  Abstract class for all decoders   Show JSON schema { &quot;title&quot;: &quot;DecoderConfig&quot;, &quot;description&quot;: &quot;Abstract class for all decoders&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;rnn&quot;, &quot;description&quot;: &quot;Type of decoder layer(s) to use.&quot;, &quot;title&quot;: &quot;Decoder Type&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: coverage_attn (bool)decoder_type (str | None)global_attention (Literal['dot', 'general', 'mlp', None])global_attention_function (Literal['softmax', 'sparsemax'])hidden_size (int)lambda_coverage (float)layers (int)tgt_word_vec_size (int) Validators: _validate_decoder_config » all fields  field coverage_attn : bool = False​  Train a coverage attention layer.  Validated by: _validate_decoder_config  field decoder_type : str | None = 'rnn'​  Type of decoder layer(s) to use.  Validated by: _validate_decoder_config  field global_attention : Literal['dot', 'general', 'mlp', None] = 'general'​  The attention type to use. (Luong=general, Bahdanau=MLP)  Validated by: _validate_decoder_config  field global_attention_function : Literal['softmax', 'sparsemax'] = 'softmax'​  Global attention function to use.  Validated by: _validate_decoder_config  field hidden_size : int = 512​  Size of decoder hidden states.  Validated by: _validate_decoder_config  field lambda_coverage : float = 0.0​  Lambda value for coverage loss of See et al (2017)  Validated by: _validate_decoder_config  field layers : int = 2​  Number of layers in the decoder.  Validated by: _validate_decoder_config  field tgt_word_vec_size : int = 512​  Word embedding size for tgt.  Validated by: _validate_decoder_config  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.CustomModelConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelscustommodelconfig","content":" Bases: TransformerConfig, BaseModelConfig  Wrap anything that does not fit a set common architecture.   Show JSON schema { &quot;title&quot;: &quot;CustomModelConfig&quot;, &quot;description&quot;: &quot;Wrap anything that does not fit a set common architecture.&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;custom&quot;, &quot;default&quot;: &quot;custom&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;CnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;EmbeddingsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;freeze_word_vecs_enc&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Enc&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_word_vecs_dec&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Dec&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.&quot;, &quot;title&quot;: &quot;Position Encoding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;position_shift&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl&quot;, &quot;title&quot;: &quot;Position Shift&quot; }, &quot;normalize&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909&quot;, &quot;title&quot;: &quot;Normalize&quot; } }, &quot;title&quot;: &quot;EmbeddingsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;MeanEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;mean&quot;, &quot;default&quot;: &quot;mean&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;MeanEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;RnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;context_gate&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Type of context gate to use.&quot;, &quot;enum&quot;: [ &quot;source&quot;, &quot;target&quot;, &quot;both&quot;, null ], &quot;title&quot;: &quot;Context Gate&quot; }, &quot;bidirectional_encoder&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Bidirectional Encoder&quot; } }, &quot;title&quot;: &quot;RnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;default&quot;: &quot;rnn&quot;, &quot;enum&quot;: [ &quot;rnn&quot;, &quot;brnn&quot; ], &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;RnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;aan_useffn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Turn on the FFN layer in the AAN decoder.&quot;, &quot;title&quot;: &quot;Aan Useffn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;alignment_layer&quot;: { &quot;default&quot;: -2, &quot;description&quot;: &quot;Layer number which has to be supervised.&quot;, &quot;title&quot;: &quot;Alignment Layer&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;alignment_heads&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of cross attention heads per layer to supervise with.&quot;, &quot;title&quot;: &quot;Alignment Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;full_context_alignment&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Whether alignment is conditioned on full target context.&quot;, &quot;title&quot;: &quot;Full Context Alignment&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_align&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)&quot;, &quot;title&quot;: &quot;Lambda Align&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;LM_type&quot;: { &quot;default&quot;: &quot;causal&quot;, &quot;description&quot;: &quot;TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)&quot;, &quot;enum&quot;: [ &quot;causal&quot;, &quot;prefix&quot; ], &quot;title&quot;: &quot;Lm Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;TransformerDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;VisionEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Based on mistral-community/pixtral-12b, might evolve later.&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;vision&quot;, &quot;default&quot;: &quot;vision&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;num_channels&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Num Channels&quot; }, &quot;image_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1024, &quot;title&quot;: &quot;Image Size&quot; }, &quot;patch_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 16, &quot;title&quot;: &quot;Patch Size&quot; }, &quot;image_token_id&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 10, &quot;title&quot;: &quot;Image Token Id&quot; }, &quot;mm_tokens_per_image&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 256, &quot;title&quot;: &quot;Mm Tokens Per Image&quot; }, &quot;layernorm_pre&quot;: { &quot;default&quot;: true, &quot;title&quot;: &quot;Layernorm Pre&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;patch_conv_bias&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Patch Conv Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;encoder_sam&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Encoder Sam&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;use_class_embedding&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Use Class Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; } }, &quot;title&quot;: &quot;VisionEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: architecture (Literal['custom']) Validators:  field architecture : Literal['custom'] = 'custom'​  Validated by: _override_values_validate_model_config_validate_transformer_configdefault_architecture  ","version":"Next","tagName":"h3"},{"title":"Transformer​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#transformer","content":" ","version":"Next","tagName":"h2"},{"title":"pydantic model eole.config.models.TransformerConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelstransformerconfig","content":" Bases: Config  This base TransformerConfig class regroups parameters than can both be set at model level or either encoder/decoder level. BaseModelConfig._override_values validator overrides encoder/decoder values with model values if relevant.   Show JSON schema { &quot;title&quot;: &quot;TransformerConfig&quot;, &quot;description&quot;: &quot;This base TransformerConfig class regroups parameters than can\\nboth be set at model level or either encoder/decoder level.\\nBaseModelConfig._override_values validator overrides\\nencoder/decoder values with model values if relevant.&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: add_ffnbias (bool)add_final_linear_bias (bool)add_qkvbias (bool)attn_scaling (float | None)ffn_layernorm (bool)first_k_dense_replace (int)head_dim (int | None)heads (int)heads_kv (int | None)key_norm (bool)layer_norm (Literal['standard', 'standardFP32', 'rms', 'gemma-rms'])mlp_activation_fn (eole.constants.ActivationFunction)moe_softmax_after (bool)moe_transformer_ff (int | None)n_positions (int | None)norm_eps (float)num_experts (int)num_experts_per_tok (int)num_shared_experts (int)parallel_residual (bool)position_encoding_type (eole.constants.PositionEncodingType | None)qk_norm_post_rope (bool)query_norm (bool)relative_positions_buckets (int)rope_config (eole.config.models.RotaryPositionConfig | None)shared_layer_norm (bool)sliding_window (int)transformer_ff (int) Validators: _validate_transformer_config » all fields  field add_ffnbias : bool = False​  Add bias to nn.Linear of MLP FFN.  Validated by: _validate_transformer_config  field add_final_linear_bias : bool = False​  Add bias to nn.Linear of final_linear in MHA.  Validated by: _validate_transformer_config  field add_qkvbias : bool = False​  Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with add_final_linear_bias.  Validated by: _validate_transformer_config  field attn_scaling : float | None = None​  Attention scaling factor, when None uses 1/sqrt(head_dim) by default  Validated by: _validate_transformer_config  field ffn_layernorm : bool = False​  Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.  Validated by: _validate_transformer_config  field first_k_dense_replace : int = 0​  Number of layers using Dense instead of MoE  Validated by: _validate_transformer_config  field head_dim : int | None = None​  Head dimension when this needs to be different vs hidden_size // heads  Validated by: _validate_transformer_config  field heads : int = 8​  Number of heads for transformer self-attention.  Validated by: _validate_transformer_config  field heads_kv : int | None = None​  Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)  Validated by: _validate_transformer_config  field key_norm : bool = False​  Validated by: _validate_transformer_config  field layer_norm : Literal['standard', 'standardFP32', 'rms', 'gemma-rms'] = 'standard'​  Type of layer normalization in transformer architecture.  Validated by: _validate_transformer_config  field mlp_activation_fn : ActivationFunction = ActivationFunction.relu​  The activation function to use in MLP layer.  Validated by: _validate_transformer_config  field moe_softmax_after : bool = False​  Usually softmax is before topk, Mixtral does it after.  Validated by: _validate_transformer_config  field moe_transformer_ff : int | None = None​  Size of hidden moe transformer feed-forward.  Validated by: _validate_transformer_config  field n_positions : int | None = None​  Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative  Validated by: _validate_transformer_config  field norm_eps : float = 1e-05​  Layer norm epsilon.  Validated by: _validate_transformer_config  field num_experts : int = 0​  Number of experts for MoE models.  Validated by: _validate_transformer_config  field num_experts_per_tok : int = 2​  Number of experts per token.  Validated by: _validate_transformer_config  field num_shared_experts : int = 0​  Number of shared experts for MoE models (DeepSeekv2).  Validated by: _validate_transformer_config  field parallel_residual : bool = False​  Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.  Validated by: _validate_transformer_config  field position_encoding_type : PositionEncodingType | None = PositionEncodingType.SinusoidalInterleaved​  Type of positional encoding.  Validated by: _validate_transformer_config  field qk_norm_post_rope : bool = False​  Validated by: _validate_transformer_config  field query_norm : bool = False​  Validated by: _validate_transformer_config  field relative_positions_buckets : int = 0​  Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).  Validated by: _validate_transformer_config  field rope_config : RotaryPositionConfig | None = None​  Rotary position config, if relevant.  Validated by: _validate_transformer_config  field shared_layer_norm : bool = False​  Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.  Validated by: _validate_transformer_config  field sliding_window : int = 0​  Sliding window for transformer self-attention.  Validated by: _validate_transformer_config  field transformer_ff : int = 2048​  Size of hidden transformer feed-forward.  Validated by: _validate_transformer_config  property dim_per_head : int​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.TransformerEncoderConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelstransformerencoderconfig","content":" Bases: TransformerConfig, EncoderConfig   Show JSON schema { &quot;title&quot;: &quot;TransformerEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: encoder_type (Literal['transformer']) Validators:  field encoder_type : Literal['transformer'] = 'transformer'​  Validated by: _validate_transformer_config  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.TransformerDecoderConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelstransformerdecoderconfig","content":" Bases: TransformerConfig, DecoderConfig   Show JSON schema { &quot;title&quot;: &quot;TransformerDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;aan_useffn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Turn on the FFN layer in the AAN decoder.&quot;, &quot;title&quot;: &quot;Aan Useffn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;alignment_layer&quot;: { &quot;default&quot;: -2, &quot;description&quot;: &quot;Layer number which has to be supervised.&quot;, &quot;title&quot;: &quot;Alignment Layer&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;alignment_heads&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of cross attention heads per layer to supervise with.&quot;, &quot;title&quot;: &quot;Alignment Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;full_context_alignment&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Whether alignment is conditioned on full target context.&quot;, &quot;title&quot;: &quot;Full Context Alignment&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_align&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)&quot;, &quot;title&quot;: &quot;Lambda Align&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;LM_type&quot;: { &quot;default&quot;: &quot;causal&quot;, &quot;description&quot;: &quot;TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)&quot;, &quot;enum&quot;: [ &quot;causal&quot;, &quot;prefix&quot; ], &quot;title&quot;: &quot;Lm Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: LM_type (Literal['causal', 'prefix'])aan_useffn (bool)alignment_heads (int)alignment_layer (int)decoder_type (Literal['transformer'])full_context_alignment (bool)lambda_align (float) Validators: _validate_transformer_decoder_config » all fields  field LM_type : Literal['causal', 'prefix'] = 'causal'​  TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)  Validated by: _validate_decoder_config_validate_transformer_config_validate_transformer_decoder_config  field aan_useffn : bool = False​  Turn on the FFN layer in the AAN decoder.  Validated by: _validate_decoder_config_validate_transformer_config_validate_transformer_decoder_config  field alignment_heads : int = 0​  Number of cross attention heads per layer to supervise with.  Validated by: _validate_decoder_config_validate_transformer_config_validate_transformer_decoder_config  field alignment_layer : int = -2​  Layer number which has to be supervised.  Validated by: _validate_decoder_config_validate_transformer_config_validate_transformer_decoder_config  field decoder_type : Literal['transformer'] = 'transformer'​  Validated by: _validate_decoder_config_validate_transformer_config_validate_transformer_decoder_config  field full_context_alignment : bool = False​  Whether alignment is conditioned on full target context.  Validated by: _validate_decoder_config_validate_transformer_config_validate_transformer_decoder_config  field lambda_align : float = 0.0​  Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)  Validated by: _validate_decoder_config_validate_transformer_config_validate_transformer_decoder_config  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.TransformerModelConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelstransformermodelconfig","content":" Bases: TransformerConfig, BaseModelConfig  Facilitate setting some transformer specific params at model level.   Show JSON schema { &quot;title&quot;: &quot;TransformerModelConfig&quot;, &quot;description&quot;: &quot;Facilitate setting some transformer specific params at model level.&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;CnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;EmbeddingsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;freeze_word_vecs_enc&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Enc&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_word_vecs_dec&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Dec&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.&quot;, &quot;title&quot;: &quot;Position Encoding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;position_shift&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl&quot;, &quot;title&quot;: &quot;Position Shift&quot; }, &quot;normalize&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909&quot;, &quot;title&quot;: &quot;Normalize&quot; } }, &quot;title&quot;: &quot;EmbeddingsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;MeanEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;mean&quot;, &quot;default&quot;: &quot;mean&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;MeanEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;RnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;context_gate&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Type of context gate to use.&quot;, &quot;enum&quot;: [ &quot;source&quot;, &quot;target&quot;, &quot;both&quot;, null ], &quot;title&quot;: &quot;Context Gate&quot; }, &quot;bidirectional_encoder&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Bidirectional Encoder&quot; } }, &quot;title&quot;: &quot;RnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;default&quot;: &quot;rnn&quot;, &quot;enum&quot;: [ &quot;rnn&quot;, &quot;brnn&quot; ], &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;RnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;aan_useffn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Turn on the FFN layer in the AAN decoder.&quot;, &quot;title&quot;: &quot;Aan Useffn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;alignment_layer&quot;: { &quot;default&quot;: -2, &quot;description&quot;: &quot;Layer number which has to be supervised.&quot;, &quot;title&quot;: &quot;Alignment Layer&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;alignment_heads&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of cross attention heads per layer to supervise with.&quot;, &quot;title&quot;: &quot;Alignment Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;full_context_alignment&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Whether alignment is conditioned on full target context.&quot;, &quot;title&quot;: &quot;Full Context Alignment&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_align&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)&quot;, &quot;title&quot;: &quot;Lambda Align&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;LM_type&quot;: { &quot;default&quot;: &quot;causal&quot;, &quot;description&quot;: &quot;TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)&quot;, &quot;enum&quot;: [ &quot;causal&quot;, &quot;prefix&quot; ], &quot;title&quot;: &quot;Lm Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;TransformerDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;VisionEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Based on mistral-community/pixtral-12b, might evolve later.&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;vision&quot;, &quot;default&quot;: &quot;vision&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;num_channels&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Num Channels&quot; }, &quot;image_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1024, &quot;title&quot;: &quot;Image Size&quot; }, &quot;patch_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 16, &quot;title&quot;: &quot;Patch Size&quot; }, &quot;image_token_id&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 10, &quot;title&quot;: &quot;Image Token Id&quot; }, &quot;mm_tokens_per_image&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 256, &quot;title&quot;: &quot;Mm Tokens Per Image&quot; }, &quot;layernorm_pre&quot;: { &quot;default&quot;: true, &quot;title&quot;: &quot;Layernorm Pre&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;patch_conv_bias&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Patch Conv Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;encoder_sam&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Encoder Sam&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;use_class_embedding&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Use Class Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; } }, &quot;title&quot;: &quot;VisionEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: architecture (Literal['transformer']) Validators: _validate_transformer » all fieldsdefault_architecture » all fieldsencoder_decoder_type » all fields  field architecture : Literal['transformer'] = 'transformer'​  Validated by: _override_values_validate_model_config_validate_transformer_validate_transformer_configdefault_architectureencoder_decoder_type  validator default_architecture » all fields​  validator encoder_decoder_type » all fields​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.TransformerLMModelConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelstransformerlmmodelconfig","content":" Bases: TransformerConfig, BaseModelConfig  Facilitate setting some transformer specific params at model level.   Show JSON schema { &quot;title&quot;: &quot;TransformerLMModelConfig&quot;, &quot;description&quot;: &quot;Facilitate setting some transformer specific params at model level.&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot;, &quot;type&quot;: &quot;null&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;transformer_lm&quot;, &quot;default&quot;: &quot;transformer_lm&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;CnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;EmbeddingsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;freeze_word_vecs_enc&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Enc&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_word_vecs_dec&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Dec&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.&quot;, &quot;title&quot;: &quot;Position Encoding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;position_shift&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl&quot;, &quot;title&quot;: &quot;Position Shift&quot; }, &quot;normalize&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909&quot;, &quot;title&quot;: &quot;Normalize&quot; } }, &quot;title&quot;: &quot;EmbeddingsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;RnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;context_gate&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Type of context gate to use.&quot;, &quot;enum&quot;: [ &quot;source&quot;, &quot;target&quot;, &quot;both&quot;, null ], &quot;title&quot;: &quot;Context Gate&quot; }, &quot;bidirectional_encoder&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Bidirectional Encoder&quot; } }, &quot;title&quot;: &quot;RnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;aan_useffn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Turn on the FFN layer in the AAN decoder.&quot;, &quot;title&quot;: &quot;Aan Useffn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;alignment_layer&quot;: { &quot;default&quot;: -2, &quot;description&quot;: &quot;Layer number which has to be supervised.&quot;, &quot;title&quot;: &quot;Alignment Layer&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;alignment_heads&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of cross attention heads per layer to supervise with.&quot;, &quot;title&quot;: &quot;Alignment Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;full_context_alignment&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Whether alignment is conditioned on full target context.&quot;, &quot;title&quot;: &quot;Full Context Alignment&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_align&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)&quot;, &quot;title&quot;: &quot;Lambda Align&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;LM_type&quot;: { &quot;default&quot;: &quot;causal&quot;, &quot;description&quot;: &quot;TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)&quot;, &quot;enum&quot;: [ &quot;causal&quot;, &quot;prefix&quot; ], &quot;title&quot;: &quot;Lm Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;TransformerDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: architecture (Literal['transformer_lm'])encoder (None) Validators: _validate_transformer » all fieldsdefault_architecture » all fieldsencoder_decoder_type » all fieldsstr_to_dict » encoder  field architecture : Literal['transformer_lm'] = 'transformer_lm'​  Validated by: _override_values_validate_model_config_validate_transformer_validate_transformer_configdefault_architectureencoder_decoder_type  field encoder : None = None​  Major parameters of an encoder.  Validated by: _override_values_validate_model_config_validate_transformer_validate_transformer_configdefault_architectureencoder_decoder_typestr_to_dict  validator default_architecture » all fields​  validator encoder_decoder_type » all fields​  ","version":"Next","tagName":"h3"},{"title":"RNN​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#rnn","content":" ","version":"Next","tagName":"h2"},{"title":"pydantic model eole.config.models.RnnConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelsrnnconfig","content":" Bases: Config  Just to facilitate testing discriminator stuff.   Show JSON schema { &quot;title&quot;: &quot;RnnConfig&quot;, &quot;description&quot;: &quot;Just to facilitate testing discriminator stuff.&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: bridge (bool)rnn_type (Literal['LSTM', 'GRU'])  field bridge : bool = False​  Have an additional layer between the last encoder state and the first decoder state (RNN specific).  field rnn_type : Literal['LSTM', 'GRU'] = 'LSTM'​  The gate type to use in the RNNs.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.RnnEncoderConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelsrnnencoderconfig","content":" Bases: RnnConfig, EncoderConfig   Show JSON schema { &quot;title&quot;: &quot;RnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;default&quot;: &quot;rnn&quot;, &quot;enum&quot;: [ &quot;rnn&quot;, &quot;brnn&quot; ], &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: encoder_type (Literal['rnn', 'brnn'])  field encoder_type : Literal['rnn', 'brnn'] = 'rnn'​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.RnnDecoderConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelsrnndecoderconfig","content":" Bases: RnnConfig, DecoderConfig   Show JSON schema { &quot;title&quot;: &quot;RnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;context_gate&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Type of context gate to use.&quot;, &quot;enum&quot;: [ &quot;source&quot;, &quot;target&quot;, &quot;both&quot;, null ], &quot;title&quot;: &quot;Context Gate&quot; }, &quot;bidirectional_encoder&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Bidirectional Encoder&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: bidirectional_encoder (bool | None)context_gate (Literal['source', 'target', 'both', None])decoder_type (Literal['rnn']) Validators:  field bidirectional_encoder : bool | None = False​  Validated by: _validate_decoder_config  field context_gate : Literal['source', 'target', 'both', None] = None​  Type of context gate to use.  Validated by: _validate_decoder_config  field decoder_type : Literal['rnn'] = 'rnn'​  Validated by: _validate_decoder_config  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.RnnModelConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelsrnnmodelconfig","content":" Bases: RnnConfig, BaseModelConfig   Show JSON schema { &quot;title&quot;: &quot;RnnModelConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;CnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;EmbeddingsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;freeze_word_vecs_enc&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Enc&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_word_vecs_dec&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Dec&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.&quot;, &quot;title&quot;: &quot;Position Encoding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;position_shift&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl&quot;, &quot;title&quot;: &quot;Position Shift&quot; }, &quot;normalize&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909&quot;, &quot;title&quot;: &quot;Normalize&quot; } }, &quot;title&quot;: &quot;EmbeddingsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;MeanEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;mean&quot;, &quot;default&quot;: &quot;mean&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;MeanEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;RnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;context_gate&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Type of context gate to use.&quot;, &quot;enum&quot;: [ &quot;source&quot;, &quot;target&quot;, &quot;both&quot;, null ], &quot;title&quot;: &quot;Context Gate&quot; }, &quot;bidirectional_encoder&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Bidirectional Encoder&quot; } }, &quot;title&quot;: &quot;RnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;default&quot;: &quot;rnn&quot;, &quot;enum&quot;: [ &quot;rnn&quot;, &quot;brnn&quot; ], &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;RnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;aan_useffn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Turn on the FFN layer in the AAN decoder.&quot;, &quot;title&quot;: &quot;Aan Useffn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;alignment_layer&quot;: { &quot;default&quot;: -2, &quot;description&quot;: &quot;Layer number which has to be supervised.&quot;, &quot;title&quot;: &quot;Alignment Layer&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;alignment_heads&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of cross attention heads per layer to supervise with.&quot;, &quot;title&quot;: &quot;Alignment Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;full_context_alignment&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Whether alignment is conditioned on full target context.&quot;, &quot;title&quot;: &quot;Full Context Alignment&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_align&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)&quot;, &quot;title&quot;: &quot;Lambda Align&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;LM_type&quot;: { &quot;default&quot;: &quot;causal&quot;, &quot;description&quot;: &quot;TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)&quot;, &quot;enum&quot;: [ &quot;causal&quot;, &quot;prefix&quot; ], &quot;title&quot;: &quot;Lm Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;TransformerDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;VisionEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Based on mistral-community/pixtral-12b, might evolve later.&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;vision&quot;, &quot;default&quot;: &quot;vision&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;num_channels&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Num Channels&quot; }, &quot;image_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1024, &quot;title&quot;: &quot;Image Size&quot; }, &quot;patch_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 16, &quot;title&quot;: &quot;Patch Size&quot; }, &quot;image_token_id&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 10, &quot;title&quot;: &quot;Image Token Id&quot; }, &quot;mm_tokens_per_image&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 256, &quot;title&quot;: &quot;Mm Tokens Per Image&quot; }, &quot;layernorm_pre&quot;: { &quot;default&quot;: true, &quot;title&quot;: &quot;Layernorm Pre&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;patch_conv_bias&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Patch Conv Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;encoder_sam&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Encoder Sam&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;use_class_embedding&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Use Class Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; } }, &quot;title&quot;: &quot;VisionEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: architecture (Literal['rnn']) Validators: default_architecture » all fieldsencoder_decoder_type » all fields  field architecture : Literal['rnn'] = 'rnn'​  Validated by: _override_values_validate_model_configdefault_architectureencoder_decoder_type  validator default_architecture » all fields​  validator encoder_decoder_type » all fields​  ","version":"Next","tagName":"h3"},{"title":"CNN​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#cnn","content":" ","version":"Next","tagName":"h2"},{"title":"pydantic model eole.config.models.CnnConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelscnnconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;CnnConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: cnn_kernel_width (int)  field cnn_kernel_width : int = 3​  Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.CnnEncoderConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelscnnencoderconfig","content":" Bases: CnnConfig, EncoderConfig   Show JSON schema { &quot;title&quot;: &quot;CnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: encoder_type (Literal['cnn'])  field encoder_type : Literal['cnn'] = 'cnn'​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.CnnDecoderConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelscnndecoderconfig","content":" Bases: CnnConfig, DecoderConfig   Show JSON schema { &quot;title&quot;: &quot;CnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: decoder_type (Literal['cnn']) Validators:  field decoder_type : Literal['cnn'] = 'cnn'​  Validated by: _validate_decoder_config  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.models.CnnModelConfig​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#pydantic-model-eoleconfigmodelscnnmodelconfig","content":" Bases: CnnConfig, BaseModelConfig   Show JSON schema { &quot;title&quot;: &quot;CnnModelConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;CnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;EmbeddingsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;freeze_word_vecs_enc&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Enc&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_word_vecs_dec&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Dec&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.&quot;, &quot;title&quot;: &quot;Position Encoding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;position_shift&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl&quot;, &quot;title&quot;: &quot;Position Shift&quot; }, &quot;normalize&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909&quot;, &quot;title&quot;: &quot;Normalize&quot; } }, &quot;title&quot;: &quot;EmbeddingsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;MeanEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;mean&quot;, &quot;default&quot;: &quot;mean&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;MeanEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;RnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;context_gate&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Type of context gate to use.&quot;, &quot;enum&quot;: [ &quot;source&quot;, &quot;target&quot;, &quot;both&quot;, null ], &quot;title&quot;: &quot;Context Gate&quot; }, &quot;bidirectional_encoder&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Bidirectional Encoder&quot; } }, &quot;title&quot;: &quot;RnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;default&quot;: &quot;rnn&quot;, &quot;enum&quot;: [ &quot;rnn&quot;, &quot;brnn&quot; ], &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;RnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;aan_useffn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Turn on the FFN layer in the AAN decoder.&quot;, &quot;title&quot;: &quot;Aan Useffn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;alignment_layer&quot;: { &quot;default&quot;: -2, &quot;description&quot;: &quot;Layer number which has to be supervised.&quot;, &quot;title&quot;: &quot;Alignment Layer&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;alignment_heads&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of cross attention heads per layer to supervise with.&quot;, &quot;title&quot;: &quot;Alignment Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;full_context_alignment&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Whether alignment is conditioned on full target context.&quot;, &quot;title&quot;: &quot;Full Context Alignment&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_align&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)&quot;, &quot;title&quot;: &quot;Lambda Align&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;LM_type&quot;: { &quot;default&quot;: &quot;causal&quot;, &quot;description&quot;: &quot;TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)&quot;, &quot;enum&quot;: [ &quot;causal&quot;, &quot;prefix&quot; ], &quot;title&quot;: &quot;Lm Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;TransformerDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;VisionEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Based on mistral-community/pixtral-12b, might evolve later.&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;vision&quot;, &quot;default&quot;: &quot;vision&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;num_channels&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Num Channels&quot; }, &quot;image_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1024, &quot;title&quot;: &quot;Image Size&quot; }, &quot;patch_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 16, &quot;title&quot;: &quot;Patch Size&quot; }, &quot;image_token_id&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 10, &quot;title&quot;: &quot;Image Token Id&quot; }, &quot;mm_tokens_per_image&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 256, &quot;title&quot;: &quot;Mm Tokens Per Image&quot; }, &quot;layernorm_pre&quot;: { &quot;default&quot;: true, &quot;title&quot;: &quot;Layernorm Pre&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;patch_conv_bias&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Patch Conv Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;encoder_sam&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Encoder Sam&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;use_class_embedding&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Use Class Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; } }, &quot;title&quot;: &quot;VisionEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: architecture (Literal['cnn']) Validators: default_architecture » all fieldsencoder_decoder_type » all fields  field architecture : Literal['cnn'] = 'cnn'​  Validated by: _override_values_validate_model_configdefault_architectureencoder_decoder_type  validator default_architecture » all fields​  validator encoder_decoder_type » all fields​ ","version":"Next","tagName":"h3"},{"title":"Main Entrypoints","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/run","content":"","keywords":"","version":"Next"},{"title":"Common Base Config​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#common-base-config","content":" ","version":"Next","tagName":"h2"},{"title":"pydantic model eole.config.common.DistributedConfig​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#pydantic-model-eoleconfigcommondistributedconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;DistributedConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;gpu_ranks&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of ranks for each process.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Gpu Ranks&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;world_size&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Total number of distributed processes.&quot;, &quot;title&quot;: &quot;World Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;parallel_mode&quot;: { &quot;default&quot;: &quot;data_parallel&quot;, &quot;description&quot;: &quot;Distributed mode.&quot;, &quot;enum&quot;: [ &quot;data_parallel&quot;, &quot;tensor_parallel&quot; ], &quot;title&quot;: &quot;Parallel Mode&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_backend&quot;: { &quot;default&quot;: &quot;nccl&quot;, &quot;description&quot;: &quot;Type of torch distributed backend.&quot;, &quot;title&quot;: &quot;Gpu Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_verbose_level&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Gives more info on each process per GPU.&quot;, &quot;title&quot;: &quot;Gpu Verbose Level&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;master_ip&quot;: { &quot;default&quot;: &quot;localhost&quot;, &quot;description&quot;: &quot;IP of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Ip&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;master_port&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Port of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Port&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;timeout&quot;: { &quot;default&quot;: 60, &quot;description&quot;: &quot;Timeout for one GPU to wait for the others.&quot;, &quot;title&quot;: &quot;Timeout&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: gpu_backend (str)gpu_ranks (List[int])gpu_verbose_level (int)master_ip (str)master_port (int)parallel_mode (Literal['data_parallel', 'tensor_parallel'])timeout (int)world_size (int)  field gpu_backend : str = 'nccl'​  Type of torch distributed backend.  field gpu_ranks : List[int] = []​  List of ranks for each process.  field gpu_verbose_level : int = 0​  Gives more info on each process per GPU.  field master_ip : str = 'localhost'​  IP of master for torch.distributed training.  field master_port : int = 10000​  Port of master for torch.distributed training.  field parallel_mode : Literal['data_parallel', 'tensor_parallel'] = 'data_parallel'​  Distributed mode.  field timeout : int = 60​  Timeout for one GPU to wait for the others.  field world_size : int = 1​  Total number of distributed processes.  property parallel_gpu : int​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.common.LoggingConfig​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#pydantic-model-eoleconfigcommonloggingconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;LoggingConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;log_file&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Output logs to a file under this path.&quot;, &quot;title&quot;: &quot;Log File&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;report_every&quot;: { &quot;default&quot;: 50, &quot;description&quot;: &quot;Print stats at this interval (in steps).&quot;, &quot;title&quot;: &quot;Report Every&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;valid_metrics&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of names of additional validation metrics.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Valid Metrics&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;scoring_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Dump src/ref/pred of the current batch.&quot;, &quot;title&quot;: &quot;Scoring Debug&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;dump_preds&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Folder to dump predictions to.&quot;, &quot;title&quot;: &quot;Dump Preds&quot; }, &quot;tensorboard&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use tensorboard for visualization during training.&quot;, &quot;title&quot;: &quot;Tensorboard&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;tensorboard_log_dir&quot;: { &quot;default&quot;: &quot;runs/eole&quot;, &quot;description&quot;: &quot;Log directory for tensorboard (also the name of the run).&quot;, &quot;title&quot;: &quot;Tensorboard Log Dir&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;tensorboard_log_dir_dated&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tensorboard Log Dir Dated&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: dump_preds (str | None)log_file (str)report_every (int)scoring_debug (bool)tensorboard (bool)tensorboard_log_dir (str)tensorboard_log_dir_dated (str | None)valid_metrics (List[str])  field dump_preds : str | None = None​  Folder to dump predictions to.  field log_file : str = ''​  Output logs to a file under this path.  field report_every : int = 50​  Print stats at this interval (in steps).  field scoring_debug : bool = False​  Dump src/ref/pred of the current batch.  field tensorboard : bool = False​  Use tensorboard for visualization during training.  field tensorboard_log_dir : str = 'runs/eole'​  Log directory for tensorboard (also the name of the run).  field tensorboard_log_dir_dated : str | None = None​  field valid_metrics : List[str] = []​  List of names of additional validation metrics.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.common.LoRaConfig​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#pydantic-model-eoleconfigcommonloraconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;LoRaConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;lora_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be replaced by LoRa layers. E.g. ['linear_values', 'linear_query'] (\\u00a74.2 in https://arxiv.org/abs/2106.09685)&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Lora Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;lora_embedding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Replace embeddings with LoRa Embeddings (\\u00a75.1)&quot;, &quot;title&quot;: &quot;Lora Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lora_rank&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;r=2 successfully tested with NLLB-200 3.3B&quot;, &quot;title&quot;: &quot;Lora Rank&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_alpha&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;\\u00a74.1 https://arxiv.org/abs/2106.09685&quot;, &quot;title&quot;: &quot;Lora Alpha&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_dropout&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rule of thumb: same value as in main model.&quot;, &quot;title&quot;: &quot;Lora Dropout&quot;, &quot;type&quot;: &quot;number&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: lora_alpha (int)lora_dropout (float)lora_embedding (bool)lora_layers (List[str])lora_rank (int)  field lora_alpha : int = 1​  §4.1 https://arxiv.org/abs/2106.09685  field lora_dropout : float = 0.0​  Rule of thumb: same value as in main model.  field lora_embedding : bool = False​  Replace embeddings with LoRa Embeddings (§5.1)  field lora_layers : List[str] = []​  List of layers to be replaced by LoRa layers. E.g. [‘linear_values’, ‘linear_query’] (§4.2 in https://arxiv.org/abs/2106.09685)  field lora_rank : int = 2​  r=2 successfully tested with NLLB-200 3.3B  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.common.QuantizeConfig​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#pydantic-model-eoleconfigcommonquantizeconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;QuantizeConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;quant_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be compressed in 4/8bit.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Quant Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;quant_type&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Type of compression.&quot;, &quot;enum&quot;: [ &quot;&quot;, &quot;bnb_8bit&quot;, &quot;bnb_FP4&quot;, &quot;bnb_NF4&quot;, &quot;awq_gemm&quot;, &quot;awq_gemv&quot; ], &quot;title&quot;: &quot;Quant Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;w_bit&quot;: { &quot;default&quot;: 4, &quot;description&quot;: &quot;W_bit quantization&quot;, &quot;title&quot;: &quot;W Bit&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;group_size&quot;: { &quot;default&quot;: 128, &quot;description&quot;: &quot;Group size quantization.&quot;, &quot;title&quot;: &quot;Group Size&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: group_size (int)quant_layers (List[str])quant_type (Literal['', 'bnb_8bit', 'bnb_FP4', 'bnb_NF4', 'awq_gemm', 'awq_gemv'])w_bit (int)  field group_size : int = 128​  Group size quantization.  field quant_layers : List[str] = []​  List of layers to be compressed in 4/8bit.  field quant_type : Literal['', 'bnb_8bit', 'bnb_FP4', 'bnb_NF4', 'awq_gemm', 'awq_gemv'] = ''​  Type of compression.  field w_bit : int = 4​  W_bit quantization  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.common.MiscConfig​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#pydantic-model-eoleconfigcommonmiscconfig","content":" Bases: Config   Show JSON schema { &quot;title&quot;: &quot;MiscConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;seed&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Set random seed used for better reproducibility between experiments.&quot;, &quot;title&quot;: &quot;Seed&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;additionalProperties&quot;: false }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: seed (int)  field seed : int = -1​  Set random seed used for better reproducibility between experiments.  ","version":"Next","tagName":"h3"},{"title":"Run Config​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#run-config","content":" ","version":"Next","tagName":"h2"},{"title":"pydantic model eole.config.run.TrainConfig​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#pydantic-model-eoleconfigruntrainconfig","content":" Bases: LoggingConfig, MiscConfig, DataConfig, VocabConfig   Show JSON schema { &quot;title&quot;: &quot;TrainConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;description&quot;: &quot;Path to src (or shared) vocabulary file. Format: one &lt;word&gt; or &lt;word&gt;\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Vocab&quot; }, &quot;tgt_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to tgt vocabulary file. Format: one &lt;word&gt; or &lt;word&gt;\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Vocab&quot; }, &quot;share_vocab&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share source and target vocabulary.&quot;, &quot;title&quot;: &quot;Share Vocab&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;decoder_start_token&quot;: { &quot;default&quot;: &quot;&amp;lt;s&amp;gt;&quot;, &quot;description&quot;: &quot;Default decoder start token. For most models it is &amp;lt;s&amp;gt; = BOS. Some fairseq models require &amp;lt;/s&amp;gt;.&quot;, &quot;title&quot;: &quot;Decoder Start Token&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bos_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&amp;lt;s&amp;gt;&quot;, &quot;title&quot;: &quot;Bos Token&quot; }, &quot;eos_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&amp;lt;/s&amp;gt;&quot;, &quot;title&quot;: &quot;Eos Token&quot; }, &quot;unk_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&lt;unk&gt;&quot;, &quot;title&quot;: &quot;Unk Token&quot; }, &quot;pad_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&lt;blank&gt;&quot;, &quot;title&quot;: &quot;Pad Token&quot; }, &quot;both_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for both source and target tokens.&quot;, &quot;title&quot;: &quot;Both Embeddings&quot; }, &quot;src_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for source tokens.&quot;, &quot;title&quot;: &quot;Src Embeddings&quot; }, &quot;tgt_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for target tokens.&quot;, &quot;title&quot;: &quot;Tgt Embeddings&quot; }, &quot;embeddings_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;GloVe&quot;, &quot;word2vec&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Type of embeddings file.&quot;, &quot;title&quot;: &quot;Embeddings Type&quot; }, &quot;src_vocab_size&quot;: { &quot;default&quot;: 32758, &quot;description&quot;: &quot;Maximum size of the source vocabulary.&quot;, &quot;title&quot;: &quot;Src Vocab Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_vocab_size&quot;: { &quot;default&quot;: 32768, &quot;description&quot;: &quot;Maximum size of the target vocabulary.&quot;, &quot;title&quot;: &quot;Tgt Vocab Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;vocab_size_multiple&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Make the vocabulary size a multiple of this value. (Adds dummy tokens if needed.)&quot;, &quot;title&quot;: &quot;Vocab Size Multiple&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_words_min_frequency&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Discard source words with lower frequency.&quot;, &quot;title&quot;: &quot;Src Words Min Frequency&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_words_min_frequency&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Discard target words with lower frequency.&quot;, &quot;title&quot;: &quot;Tgt Words Min Frequency&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;data&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: { &quot;$ref&quot;: &quot;#/$defs/Dataset&quot; }, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;description&quot;: &quot;All datasets and their specifications. See examples/*.yaml for further details.&quot;, &quot;title&quot;: &quot;Data&quot; }, &quot;transforms&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;Default transform pipeline to apply to data. Can be specified in each corpus of data to override.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Transforms&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;transforms_configs&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/NestedAllTransformsConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ] }, &quot;skip_empty_level&quot;: { &quot;default&quot;: &quot;warning&quot;, &quot;description&quot;: &quot;Logging level when encoutering empty examples. (silent: silently ignore/skip empty examples, warning: warn when ignoring/skipping empty examples, error: raise an error and stop execution when any empty example)&quot;, &quot;enum&quot;: [ &quot;silent&quot;, &quot;warning&quot;, &quot;error&quot; ], &quot;title&quot;: &quot;Skip Empty Level&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;n_sample&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of transformed samples per corpus to use to build the vocabulary. Set to -1 to use the full corpora.&quot;, &quot;title&quot;: &quot;N Sample&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;save_data&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Output base path for objects that will be saved (vocab, transforms, embeddings, ...)&quot;, &quot;title&quot;: &quot;Save Data&quot; }, &quot;overwrite&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Overwrite existing objects if any.&quot;, &quot;title&quot;: &quot;Overwrite&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;seed&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Set random seed used for better reproducibility between experiments.&quot;, &quot;title&quot;: &quot;Seed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;log_file&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Output logs to a file under this path.&quot;, &quot;title&quot;: &quot;Log File&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;report_every&quot;: { &quot;default&quot;: 50, &quot;description&quot;: &quot;Print stats at this interval (in steps).&quot;, &quot;title&quot;: &quot;Report Every&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;valid_metrics&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of names of additional validation metrics.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Valid Metrics&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;scoring_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Dump src/ref/pred of the current batch.&quot;, &quot;title&quot;: &quot;Scoring Debug&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;dump_preds&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Folder to dump predictions to.&quot;, &quot;title&quot;: &quot;Dump Preds&quot; }, &quot;tensorboard&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use tensorboard for visualization during training.&quot;, &quot;title&quot;: &quot;Tensorboard&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;tensorboard_log_dir&quot;: { &quot;default&quot;: &quot;runs/eole&quot;, &quot;description&quot;: &quot;Log directory for tensorboard (also the name of the run).&quot;, &quot;title&quot;: &quot;Tensorboard Log Dir&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;tensorboard_log_dir_dated&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tensorboard Log Dir Dated&quot; }, &quot;verbose&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print data loading and statistics for all process (default only logs the first process shard).&quot;, &quot;title&quot;: &quot;Verbose&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;model&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnModelConfig&quot;, &quot;custom&quot;: &quot;#/$defs/CustomModelConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnModelConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerModelConfig&quot;, &quot;transformer_encoder&quot;: &quot;#/$defs/TransformerEncoderModelConfig&quot;, &quot;transformer_lm&quot;: &quot;#/$defs/TransformerLMModelConfig&quot;, &quot;vision_transformer_lm&quot;: &quot;#/$defs/VisionTransformerLMModelConfig&quot; }, &quot;propertyName&quot;: &quot;architecture&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/TransformerLMModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionTransformerLMModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CustomModelConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Model&quot; }, &quot;training&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TrainingConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ] }, &quot;inference&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/InferenceConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;BARTNoiseConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;permute_sent_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Permute this proportion of sentences (boundaries defined by ['.', '?', '!']) in all inputs.&quot;, &quot;title&quot;: &quot;Permute Sent Ratio&quot; }, &quot;rotate_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rotate this proportion of inputs.&quot;, &quot;title&quot;: &quot;Rotate Ratio&quot; }, &quot;insert_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Insert this percentage of additional random tokens.&quot;, &quot;title&quot;: &quot;Insert Ratio&quot; }, &quot;random_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Instead of using &lt;mask&gt;, use random token this often.&quot;, &quot;title&quot;: &quot;Random Ratio&quot; }, &quot;mask_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Fraction of words/subwords that will be masked.&quot;, &quot;title&quot;: &quot;Mask Ratio&quot; }, &quot;mask_length&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;subword&quot;, &quot;word&quot;, &quot;span-poisson&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;subword&quot;, &quot;description&quot;: &quot;Length of masking window to apply.&quot;, &quot;title&quot;: &quot;Mask Length&quot; }, &quot;poisson_lambda&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;Lambda for Poisson distribution to sample span length if `-mask_length` set to span-poisson.&quot;, &quot;title&quot;: &quot;Poisson Lambda&quot; }, &quot;replace_length&quot;: { &quot;anyOf&quot;: [ { &quot;maximum&quot;: 1, &quot;minimum&quot;: -1, &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: -1, &quot;description&quot;: &quot;When masking N tokens, replace with 0, 1, or N tokens. (use -1 for N)&quot;, &quot;title&quot;: &quot;Replace Length&quot; } }, &quot;title&quot;: &quot;BARTNoiseConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;BaseTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; } }, &quot;title&quot;: &quot;BaseTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CleanConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_eq_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex src==tgt&quot;, &quot;title&quot;: &quot;Src Eq Tgt&quot; }, &quot;same_char&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same char more than 4 times&quot;, &quot;title&quot;: &quot;Same Char&quot; }, &quot;same_word&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same word more than 3 times&quot;, &quot;title&quot;: &quot;Same Word&quot; }, &quot;scripts_ok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;description&quot;: &quot;list of unicodata scripts accepted&quot;, &quot;title&quot;: &quot;Scripts Ok&quot; }, &quot;scripts_nok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of unicodata scripts not accepted&quot;, &quot;title&quot;: &quot;Scripts Nok&quot; }, &quot;src_tgt_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 2.0, &quot;description&quot;: &quot;ratio between src and tgt&quot;, &quot;title&quot;: &quot;Src Tgt Ratio&quot; }, &quot;avg_tok_min&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;average length of tokens min&quot;, &quot;title&quot;: &quot;Avg Tok Min&quot; }, &quot;avg_tok_max&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 20.0, &quot;description&quot;: &quot;average length of tokens max&quot;, &quot;title&quot;: &quot;Avg Tok Max&quot; }, &quot;langid&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of languages accepted&quot;, &quot;title&quot;: &quot;Langid&quot; } }, &quot;title&quot;: &quot;CleanConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CustomModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Wrap anything that does not fit a set common architecture.&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;custom&quot;, &quot;default&quot;: &quot;custom&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;CustomModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;Dataset&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;name&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Name&quot; }, &quot;weight&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;title&quot;: &quot;Weight&quot; }, &quot;transforms&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Transforms&quot; }, &quot;path_src&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Src&quot; }, &quot;path_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Tgt&quot; }, &quot;path_sco&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Sco&quot; }, &quot;path_txt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Txt&quot; }, &quot;path_align&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Align&quot; }, &quot;src_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Prefix&quot; }, &quot;tgt_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Prefix&quot; }, &quot;src_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Suffix&quot; }, &quot;tgt_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Suffix&quot; }, &quot;src_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Lang&quot; }, &quot;tgt_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Lang&quot; }, &quot;penn&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Penn&quot; }, &quot;norm_quote_commas&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Norm Quote Commas&quot; }, &quot;norm_numbers&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Norm Numbers&quot; }, &quot;pre_replace_unicode_punct&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Pre Replace Unicode Punct&quot; }, &quot;post_remove_control_chars&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Post Remove Control Chars&quot; }, &quot;src_eq_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Src Eq Tgt&quot; }, &quot;same_char&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Same Char&quot; }, &quot;same_word&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Same Word&quot; }, &quot;scripts_ok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;title&quot;: &quot;Scripts Ok&quot; }, &quot;scripts_nok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;title&quot;: &quot;Scripts Nok&quot; }, &quot;src_tgt_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 2, &quot;title&quot;: &quot;Src Tgt Ratio&quot; }, &quot;avg_tok_min&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Avg Tok Min&quot; }, &quot;avg_tok_max&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 20, &quot;title&quot;: &quot;Avg Tok Max&quot; }, &quot;lang_id&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;en&quot;, &quot;fr&quot; ], &quot;title&quot;: &quot;Lang Id&quot; } }, &quot;title&quot;: &quot;Dataset&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;DocifyConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;doc_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 200, &quot;description&quot;: &quot;Number of tokens per doc.&quot;, &quot;title&quot;: &quot;Doc Length&quot; }, &quot;max_context&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Max context segments.&quot;, &quot;title&quot;: &quot;Max Context&quot; } }, &quot;title&quot;: &quot;DocifyConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;EmbeddingsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;freeze_word_vecs_enc&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Enc&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_word_vecs_dec&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Dec&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.&quot;, &quot;title&quot;: &quot;Position Encoding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;position_shift&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl&quot;, &quot;title&quot;: &quot;Position Shift&quot; }, &quot;normalize&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909&quot;, &quot;title&quot;: &quot;Normalize&quot; } }, &quot;title&quot;: &quot;EmbeddingsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;FilterTooLongConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;title&quot;: &quot;FilterTooLongConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;FilterTooShortConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 48, &quot;description&quot;: &quot;Minimum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 48, &quot;description&quot;: &quot;Minimum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;title&quot;: &quot;FilterTooShortConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;HuggingfaceTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;max_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Max Length&quot; } }, &quot;title&quot;: &quot;HuggingfaceTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InferenceConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;quant_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be compressed in 4/8bit.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Quant Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;quant_type&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Type of compression.&quot;, &quot;enum&quot;: [ &quot;&quot;, &quot;bnb_8bit&quot;, &quot;bnb_FP4&quot;, &quot;bnb_NF4&quot;, &quot;awq_gemm&quot;, &quot;awq_gemv&quot; ], &quot;title&quot;: &quot;Quant Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;w_bit&quot;: { &quot;default&quot;: 4, &quot;description&quot;: &quot;W_bit quantization&quot;, &quot;title&quot;: &quot;W Bit&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;group_size&quot;: { &quot;default&quot;: 128, &quot;description&quot;: &quot;Group size quantization.&quot;, &quot;title&quot;: &quot;Group Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be replaced by LoRa layers. E.g. ['linear_values', 'linear_query'] (\\u00a74.2 in https://arxiv.org/abs/2106.09685)&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Lora Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;lora_embedding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Replace embeddings with LoRa Embeddings (\\u00a75.1)&quot;, &quot;title&quot;: &quot;Lora Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lora_rank&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;r=2 successfully tested with NLLB-200 3.3B&quot;, &quot;title&quot;: &quot;Lora Rank&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_alpha&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;\\u00a74.1 https://arxiv.org/abs/2106.09685&quot;, &quot;title&quot;: &quot;Lora Alpha&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_dropout&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rule of thumb: same value as in main model.&quot;, &quot;title&quot;: &quot;Lora Dropout&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;beam_size&quot;: { &quot;default&quot;: 5, &quot;description&quot;: &quot;Beam size.&quot;, &quot;title&quot;: &quot;Beam Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;ratio&quot;: { &quot;default&quot;: -0.0, &quot;description&quot;: &quot;Ratio based beam stop condition.&quot;, &quot;title&quot;: &quot;Ratio&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;top_k&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Set this to -1 to do random sampling from full distribution. Set this to value k&gt;1 to do random sampling restricted to the k most likely next tokens. Set this to 1 to use argmax.&quot;, &quot;title&quot;: &quot;Top K&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;top_p&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Probability for top-p/nucleus sampling. Restrict tokens to the most likely until the cumulated probability is over p. In range [0,1]. (https://arxiv.org/abs/1904.09751)&quot;, &quot;lte&quot;: 1.0, &quot;minimum&quot;: 0.0, &quot;title&quot;: &quot;Top P&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;temperature&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;If doing random sampling, divide the logits by this before computing softmax during decoding.&quot;, &quot;title&quot;: &quot;Temperature&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;length_penalty&quot;: { &quot;default&quot;: &quot;avg&quot;, &quot;description&quot;: &quot;Length penalty to use.&quot;, &quot;enum&quot;: [ &quot;avg&quot;, &quot;wu&quot;, &quot;none&quot; ], &quot;title&quot;: &quot;Length Penalty&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;alpha&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Length penalty parameter (higher = longer generation)&quot;, &quot;title&quot;: &quot;Alpha&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;coverage_penalty&quot;: { &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Coverage penalty to use. Only available in beam search.&quot;, &quot;enum&quot;: [ &quot;none&quot;, &quot;wu&quot;, &quot;summary&quot; ], &quot;title&quot;: &quot;Coverage Penalty&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;beta&quot;: { &quot;default&quot;: -0.0, &quot;description&quot;: &quot;Coverage penalty parameter.&quot;, &quot;title&quot;: &quot;Beta&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;stepwise_penalty&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Apply coverage penalty at every decoding step. Helpful for summary penalty.&quot;, &quot;title&quot;: &quot;Stepwise Penalty&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;min_length&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Minimum prediction length.&quot;, &quot;minimum&quot;: 0, &quot;title&quot;: &quot;Min Length&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;max_length&quot;: { &quot;default&quot;: 250, &quot;description&quot;: &quot;Maximum prediction length.&quot;, &quot;title&quot;: &quot;Max Length&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;max_length_ratio&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Maximum prediction length ratio. For European languages, 2 is large enough, for target Asian charageters, need to increase to 2-3, for special languages (Burmese, Amharic) to 10.&quot;, &quot;minimum&quot;: 1, &quot;title&quot;: &quot;Max Length Ratio&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;block_ngram_repeat&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Block repetition of ngrams during decoding.&quot;, &quot;title&quot;: &quot;Block Ngram Repeat&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;ignore_when_blocking&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;Ignore these strings when blocking repeats. You want to block sentence delimiters.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Ignore When Blocking&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;replace_unk&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Replace the generated UNK tokens with the source token that had the highest attention weight. If phrase_table is provided, it will lok up the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table), then it will copy the source token.&quot;, &quot;title&quot;: &quot;Replace Unk&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ban_unk_token&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Prevent unk token generation by setting unk probability to 0.&quot;, &quot;title&quot;: &quot;Ban Unk Token&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;phrase_table&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;If phrase_table is provided (with replace_unk), it will look up the identified source token and give the corresponding target token.&quot;, &quot;title&quot;: &quot;Phrase Table&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;n_best&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Output the n_best decoded sentences.&quot;, &quot;title&quot;: &quot;N Best&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;dump_beam&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;File to dump beam information to.&quot;, &quot;title&quot;: &quot;Dump Beam&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;verbose&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print scores and predictions for each input.&quot;, &quot;title&quot;: &quot;Verbose&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;with_score&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add a tab separated score to each output.&quot;, &quot;title&quot;: &quot;With Score&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estim_only&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Process the input to estimator only (no decoder).&quot;, &quot;title&quot;: &quot;Estim Only&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;attn_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print best attn for each word.&quot;, &quot;title&quot;: &quot;Attn Debug&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;align_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print best align for each word.&quot;, &quot;title&quot;: &quot;Align Debug&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;gpu_ranks&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of ranks for each process.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Gpu Ranks&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;world_size&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Total number of distributed processes.&quot;, &quot;title&quot;: &quot;World Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;parallel_mode&quot;: { &quot;default&quot;: &quot;data_parallel&quot;, &quot;description&quot;: &quot;Distributed mode.&quot;, &quot;enum&quot;: [ &quot;data_parallel&quot;, &quot;tensor_parallel&quot; ], &quot;title&quot;: &quot;Parallel Mode&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_backend&quot;: { &quot;default&quot;: &quot;nccl&quot;, &quot;description&quot;: &quot;Type of torch distributed backend.&quot;, &quot;title&quot;: &quot;Gpu Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_verbose_level&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Gives more info on each process per GPU.&quot;, &quot;title&quot;: &quot;Gpu Verbose Level&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;master_ip&quot;: { &quot;default&quot;: &quot;localhost&quot;, &quot;description&quot;: &quot;IP of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Ip&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;master_port&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Port of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Port&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;timeout&quot;: { &quot;default&quot;: 60, &quot;description&quot;: &quot;Timeout for one GPU to wait for the others.&quot;, &quot;title&quot;: &quot;Timeout&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;model_path&quot;: { &quot;default&quot;: &quot;model&quot;, &quot;description&quot;: &quot;Path to directory containing all model components.&quot;, &quot;title&quot;: &quot;Model Path&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;self_attn_backend&quot;: { &quot;default&quot;: &quot;flash&quot;, &quot;description&quot;: &quot;Self-attention backend.&quot;, &quot;enum&quot;: [ &quot;flash&quot;, &quot;pytorch&quot; ], &quot;title&quot;: &quot;Self Attn Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;compute_dtype&quot;: { &quot;description&quot;: &quot;Compute dtype (precision) to use for main compute. Some parameters might have other dtypes for specific cases (e.g. torch.amp -- See eole.config.training.TrainingConfig.storage_dtype) fp32 to force slow fp16 model on gtx1080, int8 to enable pytorch native 8-bit quantization (cpu only).&quot;, &quot;enum&quot;: [ &quot;fp32&quot;, &quot;fp16&quot;, &quot;int8&quot;, &quot;bf16&quot; ], &quot;title&quot;: &quot;Compute Dtype&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;torch_compile&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use torch.compile with dynamic=True.&quot;, &quot;title&quot;: &quot;Torch Compile&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;report_align&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report alignment for each translation.&quot;, &quot;title&quot;: &quot;Report Align&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;gold_align&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report alignment between source and gold target. Useful to test the performance of learnt alignments.&quot;, &quot;title&quot;: &quot;Gold Align&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;report_time&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report some translation time metrics.&quot;, &quot;title&quot;: &quot;Report Time&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;fuse_kvq&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Fuse K, V, Q Linear layers into a single KVQ in Self Attn.&quot;, &quot;title&quot;: &quot;Fuse Kvq&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;fuse_gate&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Fuse gate_up_proj and up_proj Linear layers into a single Linear.&quot;, &quot;title&quot;: &quot;Fuse Gate&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;profile&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report pytorch profiling stats.&quot;, &quot;title&quot;: &quot;Profile&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;batch_size&quot;: { &quot;default&quot;: 30, &quot;description&quot;: &quot;Batch size.&quot;, &quot;title&quot;: &quot;Batch Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;batch_type&quot;: { &quot;default&quot;: &quot;sents&quot;, &quot;description&quot;: &quot;Batch grouping for batch size.&quot;, &quot;enum&quot;: [ &quot;sents&quot;, &quot;tokens&quot; ], &quot;title&quot;: &quot;Batch Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;avg_raw_probs&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;If set, during ensembling scores from different models will be combined by averaging their raw probabilities and then taking the log. Otherwise, the log probabilities will be averaged directly. Necessary for models whose output layers can assign zero probability.&quot;, &quot;title&quot;: &quot;Avg Raw Probs&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;data_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;text&quot;, &quot;title&quot;: &quot;Data Type&quot; }, &quot;chat_template&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Chat Template&quot; }, &quot;optional_eos&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;Optional EOS tokens that would stop generation, e.g. &lt;|eot_id|&gt; for Llama3&quot;, &quot;title&quot;: &quot;Optional Eos&quot; } }, &quot;title&quot;: &quot;InferenceConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InlineTagsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tags_dictionary_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a flat term dictionary.&quot;, &quot;title&quot;: &quot;Tags Dictionary Path&quot; }, &quot;tags_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.1, &quot;description&quot;: &quot;Ratio of corpus to augment with tags.&quot;, &quot;title&quot;: &quot;Tags Corpus Ratio&quot; }, &quot;max_tags&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 12, &quot;description&quot;: &quot;Maximum number of tags that can be added to a single sentence.&quot;, &quot;title&quot;: &quot;Max Tags&quot; }, &quot;paired_stag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;description&quot;: &quot;The format of an opening paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Stag&quot; }, &quot;paired_etag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;description&quot;: &quot;The format of a closing paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Etag&quot; }, &quot;isolated_tag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;description&quot;: &quot;The format of an isolated inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Isolated Tag&quot; }, &quot;src_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented src sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Src Delimiter&quot; } }, &quot;title&quot;: &quot;InlineTagsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InsertMaskBeforePlaceholderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;response_patterns&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ], &quot;description&quot;: &quot;Response pattern to locate the end of the prompt.&quot;, &quot;title&quot;: &quot;Response Patterns&quot; } }, &quot;title&quot;: &quot;InsertMaskBeforePlaceholderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;MeanEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;mean&quot;, &quot;default&quot;: &quot;mean&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;MeanEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;NestedAllTransformsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;huggingface_tokenize&quot;: { &quot;$ref&quot;: &quot;#/$defs/HuggingfaceTokenizerConfig&quot;, &quot;default&quot;: { &quot;path&quot;: null, &quot;huggingface_model&quot;: null, &quot;max_length&quot;: null } }, &quot;uppercase&quot;: { &quot;$ref&quot;: &quot;#/$defs/UpperCaseConfig&quot;, &quot;default&quot;: { &quot;upper_corpus_ratio&quot;: 0.01 } }, &quot;terminology&quot;: { &quot;$ref&quot;: &quot;#/$defs/TerminologyConfig&quot;, &quot;default&quot;: { &quot;termbase_path&quot;: null, &quot;src_spacy_language_model&quot;: null, &quot;tgt_spacy_language_model&quot;: null, &quot;term_corpus_ratio&quot;: 0.3, &quot;term_example_ratio&quot;: 0.2, &quot;src_term_stoken&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;tgt_term_stoken&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;tgt_term_etoken&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;term_source_delimiter&quot;: &quot;\\uff5ffuzzy\\uff60&quot; } }, &quot;sentencepiece&quot;: { &quot;$ref&quot;: &quot;#/$defs/BaseTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0 } }, &quot;bpe&quot;: { &quot;$ref&quot;: &quot;#/$defs/BaseTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0 } }, &quot;onmt_tokenize&quot;: { &quot;$ref&quot;: &quot;#/$defs/ONMTTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0, &quot;src_subword_type&quot;: &quot;none&quot;, &quot;tgt_subword_type&quot;: &quot;none&quot;, &quot;src_onmttok_kwargs&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;gpt2_pretok&quot;: false, &quot;mapped_tokens&quot;: null } }, &quot;bart&quot;: { &quot;$ref&quot;: &quot;#/$defs/BARTNoiseConfig&quot;, &quot;default&quot;: { &quot;permute_sent_ratio&quot;: 0.0, &quot;rotate_ratio&quot;: 0.0, &quot;insert_ratio&quot;: 0.0, &quot;random_ratio&quot;: 0.0, &quot;mask_ratio&quot;: 0.0, &quot;mask_length&quot;: &quot;subword&quot;, &quot;poisson_lambda&quot;: 3.0, &quot;replace_length&quot;: -1 } }, &quot;switchout&quot;: { &quot;$ref&quot;: &quot;#/$defs/SwitchOutConfig&quot;, &quot;default&quot;: { &quot;switchout_temperature&quot;: 1.0 } }, &quot;tokendrop&quot;: { &quot;$ref&quot;: &quot;#/$defs/TokenDropConfig&quot;, &quot;default&quot;: { &quot;tokendrop_temperature&quot;: 1.0 } }, &quot;tokenmask&quot;: { &quot;$ref&quot;: &quot;#/$defs/TokenMaskConfig&quot;, &quot;default&quot;: { &quot;tokenmask_temperature&quot;: 1.0 } }, &quot;normalize&quot;: { &quot;$ref&quot;: &quot;#/$defs/NormalizeConfig&quot;, &quot;default&quot;: { &quot;src_lang&quot;: &quot;&quot;, &quot;tgt_lang&quot;: &quot;&quot;, &quot;penn&quot;: true, &quot;norm_quote_commas&quot;: true, &quot;norm_numbers&quot;: true, &quot;pre_replace_unicode_punct&quot;: false, &quot;post_remove_control_chars&quot;: false } }, &quot;insert_mask_before_placeholder&quot;: { &quot;$ref&quot;: &quot;#/$defs/InsertMaskBeforePlaceholderConfig&quot;, &quot;default&quot;: { &quot;response_patterns&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ] } }, &quot;filtertooshort&quot;: { &quot;$ref&quot;: &quot;#/$defs/FilterTooShortConfig&quot;, &quot;default&quot;: { &quot;src_seq_length&quot;: 48, &quot;tgt_seq_length&quot;: 48 } }, &quot;filtertoolong&quot;: { &quot;$ref&quot;: &quot;#/$defs/FilterTooLongConfig&quot;, &quot;default&quot;: { &quot;src_seq_length&quot;: 192, &quot;tgt_seq_length&quot;: 192 } }, &quot;prefix&quot;: { &quot;$ref&quot;: &quot;#/$defs/PrefixConfig&quot;, &quot;default&quot;: { &quot;src_prefix&quot;: &quot;&quot;, &quot;tgt_prefix&quot;: &quot;&quot; } }, &quot;suffix&quot;: { &quot;$ref&quot;: &quot;#/$defs/SuffixConfig&quot;, &quot;default&quot;: { &quot;src_suffix&quot;: &quot;&quot;, &quot;tgt_suffix&quot;: &quot;&quot; } }, &quot;docify&quot;: { &quot;$ref&quot;: &quot;#/$defs/DocifyConfig&quot;, &quot;default&quot;: { &quot;doc_length&quot;: 200, &quot;max_context&quot;: 1 } }, &quot;inlinetags&quot;: { &quot;$ref&quot;: &quot;#/$defs/InlineTagsConfig&quot;, &quot;default&quot;: { &quot;tags_dictionary_path&quot;: null, &quot;tags_corpus_ratio&quot;: 0.1, &quot;max_tags&quot;: 12, &quot;paired_stag&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;paired_etag&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;isolated_tag&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;src_delimiter&quot;: &quot;\\uff5ffuzzy\\uff60&quot; } }, &quot;clean&quot;: { &quot;$ref&quot;: &quot;#/$defs/CleanConfig&quot;, &quot;default&quot;: { &quot;src_eq_tgt&quot;: false, &quot;same_char&quot;: false, &quot;same_word&quot;: false, &quot;scripts_ok&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;scripts_nok&quot;: [], &quot;src_tgt_ratio&quot;: 2.0, &quot;avg_tok_min&quot;: 3.0, &quot;avg_tok_max&quot;: 20.0, &quot;langid&quot;: [] } } }, &quot;title&quot;: &quot;NestedAllTransformsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;NormalizeConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Source language code&quot;, &quot;title&quot;: &quot;Src Lang&quot; }, &quot;tgt_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Target language code&quot;, &quot;title&quot;: &quot;Tgt Lang&quot; }, &quot;penn&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Penn substitution&quot;, &quot;title&quot;: &quot;Penn&quot; }, &quot;norm_quote_commas&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize quotations and commas&quot;, &quot;title&quot;: &quot;Norm Quote Commas&quot; }, &quot;norm_numbers&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize numbers&quot;, &quot;title&quot;: &quot;Norm Numbers&quot; }, &quot;pre_replace_unicode_punct&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Replace unicode punct&quot;, &quot;title&quot;: &quot;Pre Replace Unicode Punct&quot; }, &quot;post_remove_control_chars&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove control chars&quot;, &quot;title&quot;: &quot;Post Remove Control Chars&quot; } }, &quot;title&quot;: &quot;NormalizeConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;ONMTTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; }, &quot;src_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for src (or shared) in pyonmttok.&quot;, &quot;title&quot;: &quot;Src Subword Type&quot; }, &quot;tgt_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for tgt in pyonmttok.&quot;, &quot;title&quot;: &quot;Tgt Subword Type&quot; }, &quot;src_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for src in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Src Onmttok Kwargs&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for tgt in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Tgt Onmttok Kwargs&quot; }, &quot;gpt2_pretok&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Preprocess sentence with byte-level mapping.&quot;, &quot;title&quot;: &quot;Gpt2 Pretok&quot; }, &quot;mapped_tokens&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;maxItems&quot;: 2, &quot;minItems&quot;: 2, &quot;prefixItems&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;string&quot; } ], &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Mapped tokens for placeholders preservation&quot;, &quot;title&quot;: &quot;Mapped Tokens&quot; } }, &quot;title&quot;: &quot;ONMTTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;PrefixConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all source examples.&quot;, &quot;title&quot;: &quot;Src Prefix&quot; }, &quot;tgt_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Prefix&quot; } }, &quot;title&quot;: &quot;PrefixConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;context_gate&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Type of context gate to use.&quot;, &quot;enum&quot;: [ &quot;source&quot;, &quot;target&quot;, &quot;both&quot;, null ], &quot;title&quot;: &quot;Context Gate&quot; }, &quot;bidirectional_encoder&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Bidirectional Encoder&quot; } }, &quot;title&quot;: &quot;RnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;default&quot;: &quot;rnn&quot;, &quot;enum&quot;: [ &quot;rnn&quot;, &quot;brnn&quot; ], &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;RnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;RnnModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;SuffixConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all source examples.&quot;, &quot;title&quot;: &quot;Src Suffix&quot; }, &quot;tgt_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Suffix&quot; } }, &quot;title&quot;: &quot;SuffixConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;SwitchOutConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;switchout_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for SwitchOut. :math:`\\\\tau^{-1}` in :cite:`DBLP:journals/corr/abs-1808-07512`. Smaller value makes data more diverse.&quot;, &quot;title&quot;: &quot;Switchout Temperature&quot; } }, &quot;title&quot;: &quot;SwitchOutConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TerminologyConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;termbase_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a dictionary file with terms.&quot;, &quot;title&quot;: &quot;Termbase Path&quot; }, &quot;src_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the source corpus.&quot;, &quot;title&quot;: &quot;Src Spacy Language Model&quot; }, &quot;tgt_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the target corpus.&quot;, &quot;title&quot;: &quot;Tgt Spacy Language Model&quot; }, &quot;term_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.3, &quot;description&quot;: &quot;Ratio of corpus to augment with terms.&quot;, &quot;title&quot;: &quot;Term Corpus Ratio&quot; }, &quot;term_example_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.2, &quot;description&quot;: &quot;Maximum terms allowed in an example.&quot;, &quot;title&quot;: &quot;Term Example Ratio&quot; }, &quot;src_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;description&quot;: &quot;The source term start token.&quot;, &quot;title&quot;: &quot;Src Term Stoken&quot; }, &quot;tgt_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;description&quot;: &quot;The target term start token.&quot;, &quot;title&quot;: &quot;Tgt Term Stoken&quot; }, &quot;tgt_term_etoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;description&quot;: &quot;The target term end token.&quot;, &quot;title&quot;: &quot;Tgt Term Etoken&quot; }, &quot;term_source_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented source sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Term Source Delimiter&quot; } }, &quot;title&quot;: &quot;TerminologyConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TokenDropConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tokendrop_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token deletion.&quot;, &quot;title&quot;: &quot;Tokendrop Temperature&quot; } }, &quot;title&quot;: &quot;TokenDropConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TokenMaskConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tokenmask_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token masking.&quot;, &quot;title&quot;: &quot;Tokenmask Temperature&quot; } }, &quot;title&quot;: &quot;TokenMaskConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TrainingConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;quant_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be compressed in 4/8bit.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Quant Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;quant_type&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Type of compression.&quot;, &quot;enum&quot;: [ &quot;&quot;, &quot;bnb_8bit&quot;, &quot;bnb_FP4&quot;, &quot;bnb_NF4&quot;, &quot;awq_gemm&quot;, &quot;awq_gemv&quot; ], &quot;title&quot;: &quot;Quant Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;w_bit&quot;: { &quot;default&quot;: 4, &quot;description&quot;: &quot;W_bit quantization&quot;, &quot;title&quot;: &quot;W Bit&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;group_size&quot;: { &quot;default&quot;: 128, &quot;description&quot;: &quot;Group size quantization.&quot;, &quot;title&quot;: &quot;Group Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be replaced by LoRa layers. E.g. ['linear_values', 'linear_query'] (\\u00a74.2 in https://arxiv.org/abs/2106.09685)&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Lora Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;lora_embedding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Replace embeddings with LoRa Embeddings (\\u00a75.1)&quot;, &quot;title&quot;: &quot;Lora Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lora_rank&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;r=2 successfully tested with NLLB-200 3.3B&quot;, &quot;title&quot;: &quot;Lora Rank&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_alpha&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;\\u00a74.1 https://arxiv.org/abs/2106.09685&quot;, &quot;title&quot;: &quot;Lora Alpha&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_dropout&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rule of thumb: same value as in main model.&quot;, &quot;title&quot;: &quot;Lora Dropout&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;optim&quot;: { &quot;default&quot;: &quot;sgd&quot;, &quot;description&quot;: &quot;Optimization method.&quot;, &quot;enum&quot;: [ &quot;sgd&quot;, &quot;adagrad&quot;, &quot;adadelta&quot;, &quot;adam&quot;, &quot;adamw&quot;, &quot;sparseadam&quot;, &quot;adafactor&quot;, &quot;adamw8bit&quot;, &quot;pagedadamw8bit&quot;, &quot;pagedadamw32bit&quot; ], &quot;title&quot;: &quot;Optim&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;adagrad_accumulator_init&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Initialize the accumulator values in adagrad. Mirrors initial_accumulator_value flag from tensorflow adagrad implementation (default 0.1 there).&quot;, &quot;title&quot;: &quot;Adagrad Accumulator Init&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;adam_beta1&quot;: { &quot;default&quot;: 0.9, &quot;description&quot;: &quot;Beta1 parameter used by Adam. Almost without exception a value of 0.9 is used in the literature, seemingly giving good results, so we would discourage changing this value from the default without due consideration.&quot;, &quot;title&quot;: &quot;Adam Beta1&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;adam_beta2&quot;: { &quot;default&quot;: 0.999, &quot;description&quot;: &quot;Beta2 parameter used by Adam. Typically a value of 0.999 is recommended, as this is the value suggested by the original paper describing Adam, and is also the value adopted in other frameworks such as Tensorflow (https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and Keras (https://keras.io/optimizers/). Whereas recently the paper Attention is All You Need suggested a value of 0.98 for beta2, this parameter may not work well for normal models / default baselines.&quot;, &quot;title&quot;: &quot;Adam Beta2&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;adam_eps&quot;: { &quot;default&quot;: 1e-08, &quot;description&quot;: &quot;Adam epsilon to forward to torch Optimizer.&quot;, &quot;title&quot;: &quot;Adam Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;weight_decay&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Weight decay to forward to torch Optimizer.&quot;, &quot;title&quot;: &quot;Weight Decay&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;use_amp&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Use torch mixed precision when compute_dtype is 16-bit.&quot;, &quot;title&quot;: &quot;Use Amp&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;learning_rate&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Starting learning rate. Recommended settings: sgd=1, adagrad=0.1, adadelta=1, adam=0.001.&quot;, &quot;title&quot;: &quot;Learning Rate&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;learning_rate_decay&quot;: { &quot;default&quot;: 0.5, &quot;description&quot;: &quot;Decay learning rate by this much if steps have gone past start_decay_steps.&quot;, &quot;title&quot;: &quot;Learning Rate Decay&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;start_decay_steps&quot;: { &quot;default&quot;: 50000, &quot;description&quot;: &quot;Start decaying every decay_steps after this many steps.&quot;, &quot;title&quot;: &quot;Start Decay Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;decay_steps&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Frequency for learning rate decay, in steps.&quot;, &quot;title&quot;: &quot;Decay Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;decay_method&quot;: { &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Custom decay method to use.&quot;, &quot;enum&quot;: [ &quot;noam&quot;, &quot;noamwd&quot;, &quot;cosine&quot;, &quot;rsqrt&quot;, &quot;none&quot; ], &quot;title&quot;: &quot;Decay Method&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;warmup_steps&quot;: { &quot;default&quot;: 4000, &quot;description&quot;: &quot;Number of warmup steps for custom decay.&quot;, &quot;title&quot;: &quot;Warmup Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;reset_optim&quot;: { &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Optimization resetter when using train_from.&quot;, &quot;enum&quot;: [ &quot;none&quot;, &quot;all&quot;, &quot;states&quot;, &quot;keep_states&quot; ], &quot;title&quot;: &quot;Reset Optim&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_ranks&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of ranks for each process.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Gpu Ranks&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;world_size&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Total number of distributed processes.&quot;, &quot;title&quot;: &quot;World Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;parallel_mode&quot;: { &quot;default&quot;: &quot;data_parallel&quot;, &quot;description&quot;: &quot;Distributed mode.&quot;, &quot;enum&quot;: [ &quot;data_parallel&quot;, &quot;tensor_parallel&quot; ], &quot;title&quot;: &quot;Parallel Mode&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_backend&quot;: { &quot;default&quot;: &quot;nccl&quot;, &quot;description&quot;: &quot;Type of torch distributed backend.&quot;, &quot;title&quot;: &quot;Gpu Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_verbose_level&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Gives more info on each process per GPU.&quot;, &quot;title&quot;: &quot;Gpu Verbose Level&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;master_ip&quot;: { &quot;default&quot;: &quot;localhost&quot;, &quot;description&quot;: &quot;IP of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Ip&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;master_port&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Port of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Port&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;timeout&quot;: { &quot;default&quot;: 60, &quot;description&quot;: &quot;Timeout for one GPU to wait for the others.&quot;, &quot;title&quot;: &quot;Timeout&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;model_path&quot;: { &quot;default&quot;: &quot;model&quot;, &quot;description&quot;: &quot;Path to directory containing all model components.&quot;, &quot;title&quot;: &quot;Model Path&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;self_attn_backend&quot;: { &quot;default&quot;: &quot;flash&quot;, &quot;description&quot;: &quot;Self-attention backend.&quot;, &quot;enum&quot;: [ &quot;flash&quot;, &quot;pytorch&quot; ], &quot;title&quot;: &quot;Self Attn Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;compute_dtype&quot;: { &quot;description&quot;: &quot;Compute dtype (precision) to use for main compute. Some parameters might have other dtypes for specific cases (e.g. torch.amp -- See eole.config.training.TrainingConfig.storage_dtype) fp32 to force slow fp16 model on gtx1080, int8 to enable pytorch native 8-bit quantization (cpu only).&quot;, &quot;enum&quot;: [ &quot;fp32&quot;, &quot;fp16&quot;, &quot;int8&quot;, &quot;bf16&quot; ], &quot;title&quot;: &quot;Compute Dtype&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;torch_compile&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use torch.compile with dynamic=True.&quot;, &quot;title&quot;: &quot;Torch Compile&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;param_init&quot;: { &quot;default&quot;: 0.1, &quot;description&quot;: &quot;Support value for uniform distribution parameters initialization. Set to 0 not to use initialization.&quot;, &quot;title&quot;: &quot;Param Init&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;param_init_method&quot;: { &quot;default&quot;: &quot;uniform&quot;, &quot;description&quot;: &quot;Parameter initialization method.&quot;, &quot;enum&quot;: [ &quot;xavier_uniform&quot;, &quot;uniform&quot;, &quot;normal&quot; ], &quot;title&quot;: &quot;Param Init Method&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;freeze_encoder&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze parameters in encoder.&quot;, &quot;title&quot;: &quot;Freeze Encoder&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_decoder&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze parameters in decoder.&quot;, &quot;title&quot;: &quot;Freeze Decoder&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;pre_word_vecs_enc&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;If a valid path is specified, will load pretrained word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Pre Word Vecs Enc&quot; }, &quot;pre_word_vecs_dec&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;If a valid path is specified, will load pretrained word embeddings on the decoder side.&quot;, &quot;title&quot;: &quot;Pre Word Vecs Dec&quot; }, &quot;data_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;text&quot;, &quot;title&quot;: &quot;Data Type&quot; }, &quot;bucket_size&quot;: { &quot;default&quot;: 262144, &quot;description&quot;: &quot;A bucket is a buffer of bucket_size examples to pick from the various corpora. The dynamic iterator batches batch_size items from the bucket and shuffle them.&quot;, &quot;title&quot;: &quot;Bucket Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bucket_size_init&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Bucket size is initialized with this amount of examples (see bucket_size_increment).&quot;, &quot;title&quot;: &quot;Bucket Size Init&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bucket_size_increment&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Bucket size incremented with this amount of examples at each new bucket (up to bucket_size).&quot;, &quot;title&quot;: &quot;Bucket Size Increment&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;prefetch_factor&quot;: { &quot;default&quot;: 200, &quot;description&quot;: &quot;Number of mini-batches loaded in advance to avoid the GPU waiting during processing of next bucket.&quot;, &quot;title&quot;: &quot;Prefetch Factor&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;save_format&quot;: { &quot;default&quot;: &quot;pytorch&quot;, &quot;description&quot;: &quot;Format to save the model weights.&quot;, &quot;enum&quot;: [ &quot;pytorch&quot;, &quot;safetensors&quot; ], &quot;title&quot;: &quot;Save Format&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;save_checkpoint_steps&quot;: { &quot;default&quot;: 5000, &quot;description&quot;: &quot;Frequency of checkpoint saving (in steps).&quot;, &quot;title&quot;: &quot;Save Checkpoint Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;keep_checkpoint&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of checkpoints to retain. (-1 retains all)&quot;, &quot;title&quot;: &quot;Keep Checkpoint&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;train_from&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Pretrained model/checkpoint weights to continue training from.&quot;, &quot;title&quot;: &quot;Train From&quot; }, &quot;num_workers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of workers for pytorch.DataLoader objects.&quot;, &quot;title&quot;: &quot;Num Workers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;batch_size&quot;: { &quot;default&quot;: 64, &quot;description&quot;: &quot;Maximum batch size for training.&quot;, &quot;title&quot;: &quot;Batch Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;batch_size_multiple&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Batch size multiple for token batches.&quot;, &quot;title&quot;: &quot;Batch Size Multiple&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;batch_type&quot;: { &quot;default&quot;: &quot;sents&quot;, &quot;description&quot;: &quot;Batch grouping for batch_size.&quot;, &quot;enum&quot;: [ &quot;sents&quot;, &quot;tokens&quot; ], &quot;title&quot;: &quot;Batch Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;normalization&quot;: { &quot;default&quot;: &quot;sents&quot;, &quot;description&quot;: &quot;Normalization method of the gradient.&quot;, &quot;enum&quot;: [ &quot;sents&quot;, &quot;tokens&quot; ], &quot;title&quot;: &quot;Normalization&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;accum_count&quot;: { &quot;default&quot;: [ 1 ], &quot;description&quot;: &quot;Accumulate gradient this many times. Approximately equivalent to updating batch_size * accum_count batches at once. Recommended for transformer.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Accum Count&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;accum_steps&quot;: { &quot;default&quot;: [ 0 ], &quot;description&quot;: &quot;Steps at which accum_count values change.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Accum Steps&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;valid_steps&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Frequency of validation, in steps.&quot;, &quot;title&quot;: &quot;Valid Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;valid_batch_size&quot;: { &quot;default&quot;: 32, &quot;description&quot;: &quot;Maximum batch size for validation.&quot;, &quot;title&quot;: &quot;Valid Batch Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;train_steps&quot;: { &quot;default&quot;: 100000, &quot;description&quot;: &quot;Number of training steps.&quot;, &quot;title&quot;: &quot;Train Steps&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;single_pass&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Make a single pass over the training dataset.&quot;, &quot;title&quot;: &quot;Single Pass&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;early_stopping&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of validation steps without improving that will trigger early stop of training.&quot;, &quot;title&quot;: &quot;Early Stopping&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;early_stopping_criteria&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Criteria to use for early stopping.&quot;, &quot;title&quot;: &quot;Early Stopping Criteria&quot; }, &quot;max_grad_norm&quot;: { &quot;default&quot;: 5, &quot;description&quot;: &quot;If the norm of the gradient vector exceeds this value, renormalize it to have the norm equal to max_grad_norm.&quot;, &quot;title&quot;: &quot;Max Grad Norm&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;dropout&quot;: { &quot;default&quot;: [ 0.3 ], &quot;description&quot;: &quot;Dropout probability.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;title&quot;: &quot;Dropout&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;attention_dropout&quot;: { &quot;default&quot;: [ 0.1 ], &quot;description&quot;: &quot;Attention dropout probability.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;title&quot;: &quot;Attention Dropout&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;dropout_steps&quot;: { &quot;default&quot;: [ 0 ], &quot;description&quot;: &quot;Steps at which dropout changes.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Dropout Steps&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;label_smoothing&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Label smoothing value epsilon. Probability of all non-true labels will be smoothed by epsilon/(vocab_size-1). Set to 0 to turn off label smoothing. (https://arxiv.org/abs/1512.00567)&quot;, &quot;title&quot;: &quot;Label Smoothing&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;average_decay&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Exponential moving average decay (https://en.wikipedia.org/wiki/Moving_average). Set to other than 0 (e.g. 1e-4) to activate. Similar to Marian NMT implementation (http://www.aclweb.org/anthology/P18-4020).&quot;, &quot;title&quot;: &quot;Average Decay&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;average_every&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Step for moving average. Default is every update if average_decay is set.&quot;, &quot;title&quot;: &quot;Average Every&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;zero_out_prompt_loss&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Set the prompt loss to zero. Mostly for LLM finetuning. Will be enabled only if the `insert_mask_before_placeholder` transform is applied.&quot;, &quot;title&quot;: &quot;Zero Out Prompt Loss&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;use_ckpting&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;Use gradient checkpointing for those modules.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Use Ckpting&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;update_vocab&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Update source and target existing vocabularies.&quot;, &quot;title&quot;: &quot;Update Vocab&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lm_prior_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;LM model to use to train the TM.&quot;, &quot;title&quot;: &quot;Lm Prior Model&quot; }, &quot;lm_prior_lambda&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;LM Prior Lambda&quot;, &quot;title&quot;: &quot;Lm Prior Lambda&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;lm_prior_tau&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;LM Prior Tau&quot;, &quot;title&quot;: &quot;Lm Prior Tau&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;estim_loss_lambda&quot;: { &quot;default&quot;: [ 1.0 ], &quot;description&quot;: &quot;Weight applied to estimator loss&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;title&quot;: &quot;Estim Loss Lambda&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;estim_loss_lambda_steps&quot;: { &quot;default&quot;: [ 0 ], &quot;description&quot;: &quot;Steps at which estimator loss lambda changes&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Estim Loss Lambda Steps&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;score_threshold&quot;: { &quot;default&quot;: 0.68, &quot;description&quot;: &quot;Threshold to filterout data&quot;, &quot;title&quot;: &quot;Score Threshold&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;log_attention_entropy&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Whether to compute and log attention entropy during training.&quot;, &quot;title&quot;: &quot;Log Attention Entropy&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;attention_entropy_types&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Which attention types to compute entropy for. If None, computes for all available types (e.g., ['std', 'self', 'context']).&quot;, &quot;title&quot;: &quot;Attention Entropy Types&quot; }, &quot;attention_entropy_layers&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Which attention layer indices to include in entropy computation. If None, includes all layers.&quot;, &quot;title&quot;: &quot;Attention Entropy Layers&quot; }, &quot;attention_entropy_aggregation&quot;: { &quot;default&quot;: &quot;mean&quot;, &quot;description&quot;: &quot;How to aggregate attention entropy across different attention types/layers.&quot;, &quot;enum&quot;: [ &quot;mean&quot;, &quot;max&quot;, &quot;min&quot; ], &quot;title&quot;: &quot;Attention Entropy Aggregation&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;TrainingConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;aan_useffn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Turn on the FFN layer in the AAN decoder.&quot;, &quot;title&quot;: &quot;Aan Useffn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;alignment_layer&quot;: { &quot;default&quot;: -2, &quot;description&quot;: &quot;Layer number which has to be supervised.&quot;, &quot;title&quot;: &quot;Alignment Layer&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;alignment_heads&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of cross attention heads per layer to supervise with.&quot;, &quot;title&quot;: &quot;Alignment Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;full_context_alignment&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Whether alignment is conditioned on full target context.&quot;, &quot;title&quot;: &quot;Full Context Alignment&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_align&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)&quot;, &quot;title&quot;: &quot;Lambda Align&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;LM_type&quot;: { &quot;default&quot;: &quot;causal&quot;, &quot;description&quot;: &quot;TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)&quot;, &quot;enum&quot;: [ &quot;causal&quot;, &quot;prefix&quot; ], &quot;title&quot;: &quot;Lm Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;TransformerDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerEncoderModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Facilitate setting some transformer specific params at model level.&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot;, &quot;type&quot;: &quot;null&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;transformer_encoder&quot;, &quot;default&quot;: &quot;transformer_encoder&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerEncoderModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerLMModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Facilitate setting some transformer specific params at model level.&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot;, &quot;type&quot;: &quot;null&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;transformer_lm&quot;, &quot;default&quot;: &quot;transformer_lm&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerLMModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Facilitate setting some transformer specific params at model level.&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;UpperCaseConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;upper_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.01, &quot;description&quot;: &quot;Corpus ratio to apply uppercasing.&quot;, &quot;title&quot;: &quot;Upper Corpus Ratio&quot; } }, &quot;title&quot;: &quot;UpperCaseConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;VisionEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Based on mistral-community/pixtral-12b, might evolve later.&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;vision&quot;, &quot;default&quot;: &quot;vision&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;num_channels&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Num Channels&quot; }, &quot;image_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1024, &quot;title&quot;: &quot;Image Size&quot; }, &quot;patch_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 16, &quot;title&quot;: &quot;Patch Size&quot; }, &quot;image_token_id&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 10, &quot;title&quot;: &quot;Image Token Id&quot; }, &quot;mm_tokens_per_image&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 256, &quot;title&quot;: &quot;Mm Tokens Per Image&quot; }, &quot;layernorm_pre&quot;: { &quot;default&quot;: true, &quot;title&quot;: &quot;Layernorm Pre&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;patch_conv_bias&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Patch Conv Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;encoder_sam&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Encoder Sam&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;use_class_embedding&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Use Class Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; } }, &quot;title&quot;: &quot;VisionEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;VisionTransformerLMModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;vision_transformer_lm&quot;, &quot;default&quot;: &quot;vision_transformer_lm&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;adapter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;llava&quot;, &quot;description&quot;: &quot;Adapter type to use in the model.&quot;, &quot;title&quot;: &quot;Adapter&quot; } }, &quot;title&quot;: &quot;VisionTransformerLMModelConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false, &quot;required&quot;: [ &quot;src_vocab&quot;, &quot;data&quot; ] }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = () Fields: inference (eole.config.inference.InferenceConfig | None)model (eole.config.models.TransformerModelConfig | eole.config.models.TransformerLMModelConfig | eole.config.models.VisionTransformerLMModelConfig | eole.config.models.TransformerEncoderModelConfig | eole.config.models.RnnModelConfig | eole.config.models.CnnModelConfig | eole.config.models.CustomModelConfig | None)n_sample (int)training (eole.config.training.TrainingConfig | None)verbose (bool) Validators: _validate_train_config » all fieldsdefault_architecture » all fieldsstr_to_dict » modelstr_to_dict » training  field inference : InferenceConfig | None = None​  Validated by: _maybe_set_huggingface_model_validate_train_configdefault_architecture  field model : TransformerModelConfig | TransformerLMModelConfig | VisionTransformerLMModelConfig | TransformerEncoderModelConfig | RnnModelConfig | CnnModelConfig | CustomModelConfig | None = None​  Validated by: _maybe_set_huggingface_model_validate_train_configdefault_architecturestr_to_dict  field n_sample : int = 0​  Number of transformed samples per corpus to use to build the vocabulary. Set to -1 to use the full corpora.  Validated by: _maybe_set_huggingface_model_validate_train_configdefault_architecture  field training : TrainingConfig | None [Optional]​  Validated by: _maybe_set_huggingface_model_validate_train_configdefault_architecturestr_to_dict  field verbose : bool = False​  Print data loading and statistics for all process (default only logs the first process shard).  Validated by: _maybe_set_huggingface_model_validate_train_configdefault_architecture  validator default_architecture » all fields​  classmethod get_defaults(architecture)​  get_model_path()​  model_post_init(context: Any, /)​  This function is meant to behave like a BaseModel method to initialise private attributes.  It takes context as an argument since that’s what pydantic-core passes when calling it.  Parameters: self – The BaseModel instance.context – The context.  validator str_to_dict » training , model​  property data_type​  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.run.PredictConfig​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#pydantic-model-eoleconfigrunpredictconfig","content":" Bases: InferenceConfig, LoggingConfig, MiscConfig   Show JSON schema { &quot;title&quot;: &quot;PredictConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;seed&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Set random seed used for better reproducibility between experiments.&quot;, &quot;title&quot;: &quot;Seed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;log_file&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Output logs to a file under this path.&quot;, &quot;title&quot;: &quot;Log File&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;report_every&quot;: { &quot;default&quot;: 50, &quot;description&quot;: &quot;Print stats at this interval (in steps).&quot;, &quot;title&quot;: &quot;Report Every&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;valid_metrics&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of names of additional validation metrics.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Valid Metrics&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;scoring_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Dump src/ref/pred of the current batch.&quot;, &quot;title&quot;: &quot;Scoring Debug&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;dump_preds&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Folder to dump predictions to.&quot;, &quot;title&quot;: &quot;Dump Preds&quot; }, &quot;tensorboard&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use tensorboard for visualization during training.&quot;, &quot;title&quot;: &quot;Tensorboard&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;tensorboard_log_dir&quot;: { &quot;default&quot;: &quot;runs/eole&quot;, &quot;description&quot;: &quot;Log directory for tensorboard (also the name of the run).&quot;, &quot;title&quot;: &quot;Tensorboard Log Dir&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;tensorboard_log_dir_dated&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tensorboard Log Dir Dated&quot; }, &quot;quant_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be compressed in 4/8bit.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Quant Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;quant_type&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Type of compression.&quot;, &quot;enum&quot;: [ &quot;&quot;, &quot;bnb_8bit&quot;, &quot;bnb_FP4&quot;, &quot;bnb_NF4&quot;, &quot;awq_gemm&quot;, &quot;awq_gemv&quot; ], &quot;title&quot;: &quot;Quant Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;w_bit&quot;: { &quot;default&quot;: 4, &quot;description&quot;: &quot;W_bit quantization&quot;, &quot;title&quot;: &quot;W Bit&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;group_size&quot;: { &quot;default&quot;: 128, &quot;description&quot;: &quot;Group size quantization.&quot;, &quot;title&quot;: &quot;Group Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_layers&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of layers to be replaced by LoRa layers. E.g. ['linear_values', 'linear_query'] (\\u00a74.2 in https://arxiv.org/abs/2106.09685)&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Lora Layers&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;lora_embedding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Replace embeddings with LoRa Embeddings (\\u00a75.1)&quot;, &quot;title&quot;: &quot;Lora Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lora_rank&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;r=2 successfully tested with NLLB-200 3.3B&quot;, &quot;title&quot;: &quot;Lora Rank&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_alpha&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;\\u00a74.1 https://arxiv.org/abs/2106.09685&quot;, &quot;title&quot;: &quot;Lora Alpha&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;lora_dropout&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rule of thumb: same value as in main model.&quot;, &quot;title&quot;: &quot;Lora Dropout&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;beam_size&quot;: { &quot;default&quot;: 5, &quot;description&quot;: &quot;Beam size.&quot;, &quot;title&quot;: &quot;Beam Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;ratio&quot;: { &quot;default&quot;: -0.0, &quot;description&quot;: &quot;Ratio based beam stop condition.&quot;, &quot;title&quot;: &quot;Ratio&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;top_k&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Set this to -1 to do random sampling from full distribution. Set this to value k&gt;1 to do random sampling restricted to the k most likely next tokens. Set this to 1 to use argmax.&quot;, &quot;title&quot;: &quot;Top K&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;top_p&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Probability for top-p/nucleus sampling. Restrict tokens to the most likely until the cumulated probability is over p. In range [0,1]. (https://arxiv.org/abs/1904.09751)&quot;, &quot;lte&quot;: 1.0, &quot;minimum&quot;: 0.0, &quot;title&quot;: &quot;Top P&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;temperature&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;If doing random sampling, divide the logits by this before computing softmax during decoding.&quot;, &quot;title&quot;: &quot;Temperature&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;length_penalty&quot;: { &quot;default&quot;: &quot;avg&quot;, &quot;description&quot;: &quot;Length penalty to use.&quot;, &quot;enum&quot;: [ &quot;avg&quot;, &quot;wu&quot;, &quot;none&quot; ], &quot;title&quot;: &quot;Length Penalty&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;alpha&quot;: { &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Length penalty parameter (higher = longer generation)&quot;, &quot;title&quot;: &quot;Alpha&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;coverage_penalty&quot;: { &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Coverage penalty to use. Only available in beam search.&quot;, &quot;enum&quot;: [ &quot;none&quot;, &quot;wu&quot;, &quot;summary&quot; ], &quot;title&quot;: &quot;Coverage Penalty&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;beta&quot;: { &quot;default&quot;: -0.0, &quot;description&quot;: &quot;Coverage penalty parameter.&quot;, &quot;title&quot;: &quot;Beta&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;stepwise_penalty&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Apply coverage penalty at every decoding step. Helpful for summary penalty.&quot;, &quot;title&quot;: &quot;Stepwise Penalty&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;min_length&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Minimum prediction length.&quot;, &quot;minimum&quot;: 0, &quot;title&quot;: &quot;Min Length&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;max_length&quot;: { &quot;default&quot;: 250, &quot;description&quot;: &quot;Maximum prediction length.&quot;, &quot;title&quot;: &quot;Max Length&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;max_length_ratio&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Maximum prediction length ratio. For European languages, 2 is large enough, for target Asian charageters, need to increase to 2-3, for special languages (Burmese, Amharic) to 10.&quot;, &quot;minimum&quot;: 1, &quot;title&quot;: &quot;Max Length Ratio&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;block_ngram_repeat&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Block repetition of ngrams during decoding.&quot;, &quot;title&quot;: &quot;Block Ngram Repeat&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;ignore_when_blocking&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;Ignore these strings when blocking repeats. You want to block sentence delimiters.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Ignore When Blocking&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;replace_unk&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Replace the generated UNK tokens with the source token that had the highest attention weight. If phrase_table is provided, it will lok up the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table), then it will copy the source token.&quot;, &quot;title&quot;: &quot;Replace Unk&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ban_unk_token&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Prevent unk token generation by setting unk probability to 0.&quot;, &quot;title&quot;: &quot;Ban Unk Token&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;phrase_table&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;If phrase_table is provided (with replace_unk), it will look up the identified source token and give the corresponding target token.&quot;, &quot;title&quot;: &quot;Phrase Table&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;n_best&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Output the n_best decoded sentences.&quot;, &quot;title&quot;: &quot;N Best&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;dump_beam&quot;: { &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;File to dump beam information to.&quot;, &quot;title&quot;: &quot;Dump Beam&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;verbose&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print scores and predictions for each input.&quot;, &quot;title&quot;: &quot;Verbose&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;with_score&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add a tab separated score to each output.&quot;, &quot;title&quot;: &quot;With Score&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estim_only&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Process the input to estimator only (no decoder).&quot;, &quot;title&quot;: &quot;Estim Only&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;attn_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print best attn for each word.&quot;, &quot;title&quot;: &quot;Attn Debug&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;align_debug&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Print best align for each word.&quot;, &quot;title&quot;: &quot;Align Debug&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;gpu_ranks&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;List of ranks for each process.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;integer&quot; }, &quot;title&quot;: &quot;Gpu Ranks&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;world_size&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Total number of distributed processes.&quot;, &quot;title&quot;: &quot;World Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;parallel_mode&quot;: { &quot;default&quot;: &quot;data_parallel&quot;, &quot;description&quot;: &quot;Distributed mode.&quot;, &quot;enum&quot;: [ &quot;data_parallel&quot;, &quot;tensor_parallel&quot; ], &quot;title&quot;: &quot;Parallel Mode&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_backend&quot;: { &quot;default&quot;: &quot;nccl&quot;, &quot;description&quot;: &quot;Type of torch distributed backend.&quot;, &quot;title&quot;: &quot;Gpu Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;gpu_verbose_level&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Gives more info on each process per GPU.&quot;, &quot;title&quot;: &quot;Gpu Verbose Level&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;master_ip&quot;: { &quot;default&quot;: &quot;localhost&quot;, &quot;description&quot;: &quot;IP of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Ip&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;master_port&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Port of master for torch.distributed training.&quot;, &quot;title&quot;: &quot;Master Port&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;timeout&quot;: { &quot;default&quot;: 60, &quot;description&quot;: &quot;Timeout for one GPU to wait for the others.&quot;, &quot;title&quot;: &quot;Timeout&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;model_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; } ], &quot;description&quot;: &quot;Path to model .pt file(s). Multiple models can be specified for ensemble decoding.&quot;, &quot;title&quot;: &quot;Model Path&quot; }, &quot;self_attn_backend&quot;: { &quot;default&quot;: &quot;flash&quot;, &quot;description&quot;: &quot;Self-attention backend.&quot;, &quot;enum&quot;: [ &quot;flash&quot;, &quot;pytorch&quot; ], &quot;title&quot;: &quot;Self Attn Backend&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;compute_dtype&quot;: { &quot;description&quot;: &quot;Compute dtype (precision) to use for main compute. Some parameters might have other dtypes for specific cases (e.g. torch.amp -- See eole.config.training.TrainingConfig.storage_dtype) fp32 to force slow fp16 model on gtx1080, int8 to enable pytorch native 8-bit quantization (cpu only).&quot;, &quot;enum&quot;: [ &quot;fp32&quot;, &quot;fp16&quot;, &quot;int8&quot;, &quot;bf16&quot; ], &quot;title&quot;: &quot;Compute Dtype&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;torch_compile&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use torch.compile with dynamic=True.&quot;, &quot;title&quot;: &quot;Torch Compile&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;report_align&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report alignment for each translation.&quot;, &quot;title&quot;: &quot;Report Align&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;gold_align&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report alignment between source and gold target. Useful to test the performance of learnt alignments.&quot;, &quot;title&quot;: &quot;Gold Align&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;report_time&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report some translation time metrics.&quot;, &quot;title&quot;: &quot;Report Time&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;fuse_kvq&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Fuse K, V, Q Linear layers into a single KVQ in Self Attn.&quot;, &quot;title&quot;: &quot;Fuse Kvq&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;fuse_gate&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Fuse gate_up_proj and up_proj Linear layers into a single Linear.&quot;, &quot;title&quot;: &quot;Fuse Gate&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;profile&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Report pytorch profiling stats.&quot;, &quot;title&quot;: &quot;Profile&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;batch_size&quot;: { &quot;default&quot;: 30, &quot;description&quot;: &quot;Batch size.&quot;, &quot;title&quot;: &quot;Batch Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;batch_type&quot;: { &quot;default&quot;: &quot;sents&quot;, &quot;description&quot;: &quot;Batch grouping for batch size.&quot;, &quot;enum&quot;: [ &quot;sents&quot;, &quot;tokens&quot; ], &quot;title&quot;: &quot;Batch Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;avg_raw_probs&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;If set, during ensembling scores from different models will be combined by averaging their raw probabilities and then taking the log. Otherwise, the log probabilities will be averaged directly. Necessary for models whose output layers can assign zero probability.&quot;, &quot;title&quot;: &quot;Avg Raw Probs&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;data_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;text&quot;, &quot;title&quot;: &quot;Data Type&quot; }, &quot;chat_template&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Chat Template&quot; }, &quot;optional_eos&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;Optional EOS tokens that would stop generation, e.g. &lt;|eot_id|&gt; for Llama3&quot;, &quot;title&quot;: &quot;Optional Eos&quot; }, &quot;transforms&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;title&quot;: &quot;Transforms&quot; }, &quot;transforms_configs&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/NestedAllTransformsConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ] }, &quot;share_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Share Vocab&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;model&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnModelConfig&quot;, &quot;custom&quot;: &quot;#/$defs/CustomModelConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnModelConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerModelConfig&quot;, &quot;transformer_encoder&quot;: &quot;#/$defs/TransformerEncoderModelConfig&quot;, &quot;transformer_lm&quot;: &quot;#/$defs/TransformerLMModelConfig&quot;, &quot;vision_transformer_lm&quot;: &quot;#/$defs/VisionTransformerLMModelConfig&quot; }, &quot;propertyName&quot;: &quot;architecture&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/TransformerLMModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionTransformerLMModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnModelConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CustomModelConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Model&quot; }, &quot;src&quot;: { &quot;description&quot;: &quot;Source file to decode (one line per sequence).&quot;, &quot;title&quot;: &quot;Src&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;True target sequences, useful for scoring or prefix decoding.&quot;, &quot;title&quot;: &quot;Tgt&quot; }, &quot;tgt_file_prefix&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Generate predictions using provided tgt as prefix.&quot;, &quot;title&quot;: &quot;Tgt File Prefix&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;output&quot;: { &quot;default&quot;: &quot;pred.txt&quot;, &quot;description&quot;: &quot;Path to output the predictions (each line will be the decoded sequence).&quot;, &quot;title&quot;: &quot;Output&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;engine&quot;: { &quot;default&quot;: &quot;eole&quot;, &quot;description&quot;: &quot;engine to run inference: eole or ct2&quot;, &quot;title&quot;: &quot;Engine&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;$defs&quot;: { &quot;ActivationFunction&quot;: { &quot;enum&quot;: [ &quot;relu&quot;, &quot;gelu&quot;, &quot;silu&quot;, &quot;quick_gelu&quot;, &quot;gated-gelu&quot;, &quot;fused-gated-gelu&quot;, &quot;gated-silu&quot;, &quot;fused-gated-silu&quot;, &quot;gelu-tanh&quot;, &quot;gated-gelu-tanh&quot;, &quot;fused-gated-gelu-tanh&quot; ], &quot;title&quot;: &quot;ActivationFunction&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;BARTNoiseConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;permute_sent_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Permute this proportion of sentences (boundaries defined by ['.', '?', '!']) in all inputs.&quot;, &quot;title&quot;: &quot;Permute Sent Ratio&quot; }, &quot;rotate_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rotate this proportion of inputs.&quot;, &quot;title&quot;: &quot;Rotate Ratio&quot; }, &quot;insert_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Insert this percentage of additional random tokens.&quot;, &quot;title&quot;: &quot;Insert Ratio&quot; }, &quot;random_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Instead of using &lt;mask&gt;, use random token this often.&quot;, &quot;title&quot;: &quot;Random Ratio&quot; }, &quot;mask_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Fraction of words/subwords that will be masked.&quot;, &quot;title&quot;: &quot;Mask Ratio&quot; }, &quot;mask_length&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;subword&quot;, &quot;word&quot;, &quot;span-poisson&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;subword&quot;, &quot;description&quot;: &quot;Length of masking window to apply.&quot;, &quot;title&quot;: &quot;Mask Length&quot; }, &quot;poisson_lambda&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;Lambda for Poisson distribution to sample span length if `-mask_length` set to span-poisson.&quot;, &quot;title&quot;: &quot;Poisson Lambda&quot; }, &quot;replace_length&quot;: { &quot;anyOf&quot;: [ { &quot;maximum&quot;: 1, &quot;minimum&quot;: -1, &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: -1, &quot;description&quot;: &quot;When masking N tokens, replace with 0, 1, or N tokens. (use -1 for N)&quot;, &quot;title&quot;: &quot;Replace Length&quot; } }, &quot;title&quot;: &quot;BARTNoiseConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;BaseTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; } }, &quot;title&quot;: &quot;BaseTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CleanConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_eq_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex src==tgt&quot;, &quot;title&quot;: &quot;Src Eq Tgt&quot; }, &quot;same_char&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same char more than 4 times&quot;, &quot;title&quot;: &quot;Same Char&quot; }, &quot;same_word&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same word more than 3 times&quot;, &quot;title&quot;: &quot;Same Word&quot; }, &quot;scripts_ok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;description&quot;: &quot;list of unicodata scripts accepted&quot;, &quot;title&quot;: &quot;Scripts Ok&quot; }, &quot;scripts_nok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of unicodata scripts not accepted&quot;, &quot;title&quot;: &quot;Scripts Nok&quot; }, &quot;src_tgt_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 2.0, &quot;description&quot;: &quot;ratio between src and tgt&quot;, &quot;title&quot;: &quot;Src Tgt Ratio&quot; }, &quot;avg_tok_min&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;average length of tokens min&quot;, &quot;title&quot;: &quot;Avg Tok Min&quot; }, &quot;avg_tok_max&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 20.0, &quot;description&quot;: &quot;average length of tokens max&quot;, &quot;title&quot;: &quot;Avg Tok Max&quot; }, &quot;langid&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of languages accepted&quot;, &quot;title&quot;: &quot;Langid&quot; } }, &quot;title&quot;: &quot;CleanConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CnnModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;cnn&quot;, &quot;default&quot;: &quot;cnn&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;cnn_kernel_width&quot;: { &quot;default&quot;: 3, &quot;description&quot;: &quot;Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.&quot;, &quot;title&quot;: &quot;Cnn Kernel Width&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;CnnModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CustomModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Wrap anything that does not fit a set common architecture.&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;custom&quot;, &quot;default&quot;: &quot;custom&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;CustomModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;DocifyConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;doc_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 200, &quot;description&quot;: &quot;Number of tokens per doc.&quot;, &quot;title&quot;: &quot;Doc Length&quot; }, &quot;max_context&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Max context segments.&quot;, &quot;title&quot;: &quot;Max Context&quot; } }, &quot;title&quot;: &quot;DocifyConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;EmbeddingsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;freeze_word_vecs_enc&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Enc&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;freeze_word_vecs_dec&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Freeze word embeddings on the encoder side.&quot;, &quot;title&quot;: &quot;Freeze Word Vecs Dec&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.&quot;, &quot;title&quot;: &quot;Position Encoding&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;position_shift&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl&quot;, &quot;title&quot;: &quot;Position Shift&quot; }, &quot;normalize&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909&quot;, &quot;title&quot;: &quot;Normalize&quot; } }, &quot;title&quot;: &quot;EmbeddingsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;FilterTooLongConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;title&quot;: &quot;FilterTooLongConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;FilterTooShortConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 48, &quot;description&quot;: &quot;Minimum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 48, &quot;description&quot;: &quot;Minimum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;title&quot;: &quot;FilterTooShortConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;HuggingfaceTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;max_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Max Length&quot; } }, &quot;title&quot;: &quot;HuggingfaceTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InlineTagsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tags_dictionary_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a flat term dictionary.&quot;, &quot;title&quot;: &quot;Tags Dictionary Path&quot; }, &quot;tags_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.1, &quot;description&quot;: &quot;Ratio of corpus to augment with tags.&quot;, &quot;title&quot;: &quot;Tags Corpus Ratio&quot; }, &quot;max_tags&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 12, &quot;description&quot;: &quot;Maximum number of tags that can be added to a single sentence.&quot;, &quot;title&quot;: &quot;Max Tags&quot; }, &quot;paired_stag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;description&quot;: &quot;The format of an opening paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Stag&quot; }, &quot;paired_etag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;description&quot;: &quot;The format of a closing paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Etag&quot; }, &quot;isolated_tag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;description&quot;: &quot;The format of an isolated inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Isolated Tag&quot; }, &quot;src_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented src sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Src Delimiter&quot; } }, &quot;title&quot;: &quot;InlineTagsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InsertMaskBeforePlaceholderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;response_patterns&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ], &quot;description&quot;: &quot;Response pattern to locate the end of the prompt.&quot;, &quot;title&quot;: &quot;Response Patterns&quot; } }, &quot;title&quot;: &quot;InsertMaskBeforePlaceholderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;MeanEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;mean&quot;, &quot;default&quot;: &quot;mean&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;MeanEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;NestedAllTransformsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;huggingface_tokenize&quot;: { &quot;$ref&quot;: &quot;#/$defs/HuggingfaceTokenizerConfig&quot;, &quot;default&quot;: { &quot;path&quot;: null, &quot;huggingface_model&quot;: null, &quot;max_length&quot;: null } }, &quot;uppercase&quot;: { &quot;$ref&quot;: &quot;#/$defs/UpperCaseConfig&quot;, &quot;default&quot;: { &quot;upper_corpus_ratio&quot;: 0.01 } }, &quot;terminology&quot;: { &quot;$ref&quot;: &quot;#/$defs/TerminologyConfig&quot;, &quot;default&quot;: { &quot;termbase_path&quot;: null, &quot;src_spacy_language_model&quot;: null, &quot;tgt_spacy_language_model&quot;: null, &quot;term_corpus_ratio&quot;: 0.3, &quot;term_example_ratio&quot;: 0.2, &quot;src_term_stoken&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;tgt_term_stoken&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;tgt_term_etoken&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;term_source_delimiter&quot;: &quot;\\uff5ffuzzy\\uff60&quot; } }, &quot;sentencepiece&quot;: { &quot;$ref&quot;: &quot;#/$defs/BaseTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0 } }, &quot;bpe&quot;: { &quot;$ref&quot;: &quot;#/$defs/BaseTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0 } }, &quot;onmt_tokenize&quot;: { &quot;$ref&quot;: &quot;#/$defs/ONMTTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0, &quot;src_subword_type&quot;: &quot;none&quot;, &quot;tgt_subword_type&quot;: &quot;none&quot;, &quot;src_onmttok_kwargs&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;gpt2_pretok&quot;: false, &quot;mapped_tokens&quot;: null } }, &quot;bart&quot;: { &quot;$ref&quot;: &quot;#/$defs/BARTNoiseConfig&quot;, &quot;default&quot;: { &quot;permute_sent_ratio&quot;: 0.0, &quot;rotate_ratio&quot;: 0.0, &quot;insert_ratio&quot;: 0.0, &quot;random_ratio&quot;: 0.0, &quot;mask_ratio&quot;: 0.0, &quot;mask_length&quot;: &quot;subword&quot;, &quot;poisson_lambda&quot;: 3.0, &quot;replace_length&quot;: -1 } }, &quot;switchout&quot;: { &quot;$ref&quot;: &quot;#/$defs/SwitchOutConfig&quot;, &quot;default&quot;: { &quot;switchout_temperature&quot;: 1.0 } }, &quot;tokendrop&quot;: { &quot;$ref&quot;: &quot;#/$defs/TokenDropConfig&quot;, &quot;default&quot;: { &quot;tokendrop_temperature&quot;: 1.0 } }, &quot;tokenmask&quot;: { &quot;$ref&quot;: &quot;#/$defs/TokenMaskConfig&quot;, &quot;default&quot;: { &quot;tokenmask_temperature&quot;: 1.0 } }, &quot;normalize&quot;: { &quot;$ref&quot;: &quot;#/$defs/NormalizeConfig&quot;, &quot;default&quot;: { &quot;src_lang&quot;: &quot;&quot;, &quot;tgt_lang&quot;: &quot;&quot;, &quot;penn&quot;: true, &quot;norm_quote_commas&quot;: true, &quot;norm_numbers&quot;: true, &quot;pre_replace_unicode_punct&quot;: false, &quot;post_remove_control_chars&quot;: false } }, &quot;insert_mask_before_placeholder&quot;: { &quot;$ref&quot;: &quot;#/$defs/InsertMaskBeforePlaceholderConfig&quot;, &quot;default&quot;: { &quot;response_patterns&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ] } }, &quot;filtertooshort&quot;: { &quot;$ref&quot;: &quot;#/$defs/FilterTooShortConfig&quot;, &quot;default&quot;: { &quot;src_seq_length&quot;: 48, &quot;tgt_seq_length&quot;: 48 } }, &quot;filtertoolong&quot;: { &quot;$ref&quot;: &quot;#/$defs/FilterTooLongConfig&quot;, &quot;default&quot;: { &quot;src_seq_length&quot;: 192, &quot;tgt_seq_length&quot;: 192 } }, &quot;prefix&quot;: { &quot;$ref&quot;: &quot;#/$defs/PrefixConfig&quot;, &quot;default&quot;: { &quot;src_prefix&quot;: &quot;&quot;, &quot;tgt_prefix&quot;: &quot;&quot; } }, &quot;suffix&quot;: { &quot;$ref&quot;: &quot;#/$defs/SuffixConfig&quot;, &quot;default&quot;: { &quot;src_suffix&quot;: &quot;&quot;, &quot;tgt_suffix&quot;: &quot;&quot; } }, &quot;docify&quot;: { &quot;$ref&quot;: &quot;#/$defs/DocifyConfig&quot;, &quot;default&quot;: { &quot;doc_length&quot;: 200, &quot;max_context&quot;: 1 } }, &quot;inlinetags&quot;: { &quot;$ref&quot;: &quot;#/$defs/InlineTagsConfig&quot;, &quot;default&quot;: { &quot;tags_dictionary_path&quot;: null, &quot;tags_corpus_ratio&quot;: 0.1, &quot;max_tags&quot;: 12, &quot;paired_stag&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;paired_etag&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;isolated_tag&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;src_delimiter&quot;: &quot;\\uff5ffuzzy\\uff60&quot; } }, &quot;clean&quot;: { &quot;$ref&quot;: &quot;#/$defs/CleanConfig&quot;, &quot;default&quot;: { &quot;src_eq_tgt&quot;: false, &quot;same_char&quot;: false, &quot;same_word&quot;: false, &quot;scripts_ok&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;scripts_nok&quot;: [], &quot;src_tgt_ratio&quot;: 2.0, &quot;avg_tok_min&quot;: 3.0, &quot;avg_tok_max&quot;: 20.0, &quot;langid&quot;: [] } } }, &quot;title&quot;: &quot;NestedAllTransformsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;NormalizeConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Source language code&quot;, &quot;title&quot;: &quot;Src Lang&quot; }, &quot;tgt_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Target language code&quot;, &quot;title&quot;: &quot;Tgt Lang&quot; }, &quot;penn&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Penn substitution&quot;, &quot;title&quot;: &quot;Penn&quot; }, &quot;norm_quote_commas&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize quotations and commas&quot;, &quot;title&quot;: &quot;Norm Quote Commas&quot; }, &quot;norm_numbers&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize numbers&quot;, &quot;title&quot;: &quot;Norm Numbers&quot; }, &quot;pre_replace_unicode_punct&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Replace unicode punct&quot;, &quot;title&quot;: &quot;Pre Replace Unicode Punct&quot; }, &quot;post_remove_control_chars&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove control chars&quot;, &quot;title&quot;: &quot;Post Remove Control Chars&quot; } }, &quot;title&quot;: &quot;NormalizeConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;ONMTTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; }, &quot;src_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for src (or shared) in pyonmttok.&quot;, &quot;title&quot;: &quot;Src Subword Type&quot; }, &quot;tgt_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for tgt in pyonmttok.&quot;, &quot;title&quot;: &quot;Tgt Subword Type&quot; }, &quot;src_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for src in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Src Onmttok Kwargs&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for tgt in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Tgt Onmttok Kwargs&quot; }, &quot;gpt2_pretok&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Preprocess sentence with byte-level mapping.&quot;, &quot;title&quot;: &quot;Gpt2 Pretok&quot; }, &quot;mapped_tokens&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;maxItems&quot;: 2, &quot;minItems&quot;: 2, &quot;prefixItems&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;string&quot; } ], &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Mapped tokens for placeholders preservation&quot;, &quot;title&quot;: &quot;Mapped Tokens&quot; } }, &quot;title&quot;: &quot;ONMTTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PositionEncodingType&quot;: { &quot;enum&quot;: [ &quot;SinusoidalInterleaved&quot;, &quot;SinusoidalConcat&quot;, &quot;Learned&quot;, &quot;Relative&quot;, &quot;Rotary&quot;, &quot;Alibi&quot; ], &quot;title&quot;: &quot;PositionEncodingType&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;PrefixConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all source examples.&quot;, &quot;title&quot;: &quot;Src Prefix&quot; }, &quot;tgt_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Prefix&quot; } }, &quot;title&quot;: &quot;PrefixConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;context_gate&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Type of context gate to use.&quot;, &quot;enum&quot;: [ &quot;source&quot;, &quot;target&quot;, &quot;both&quot;, null ], &quot;title&quot;: &quot;Context Gate&quot; }, &quot;bidirectional_encoder&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Bidirectional Encoder&quot; } }, &quot;title&quot;: &quot;RnnDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;default&quot;: &quot;rnn&quot;, &quot;enum&quot;: [ &quot;rnn&quot;, &quot;brnn&quot; ], &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;RnnEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RnnModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;rnn&quot;, &quot;default&quot;: &quot;rnn&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bridge&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Have an additional layer between the last encoder state and the first decoder state (RNN specific).&quot;, &quot;title&quot;: &quot;Bridge&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rnn_type&quot;: { &quot;default&quot;: &quot;LSTM&quot;, &quot;description&quot;: &quot;The gate type to use in the RNNs.&quot;, &quot;enum&quot;: [ &quot;LSTM&quot;, &quot;GRU&quot; ], &quot;title&quot;: &quot;Rnn Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;RnnModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;RotaryPositionConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Configuration for rotary position embeddings used in transformer models.&quot;, &quot;properties&quot;: { &quot;rotary_interleave&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)&quot;, &quot;title&quot;: &quot;Rotary Interleave&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;rotary_theta&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral&quot;, &quot;title&quot;: &quot;Rotary Theta&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;rotary_dim&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Rotary dim when model requires it to be different to head dim.&quot;, &quot;title&quot;: &quot;Rotary Dim&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;scaling_type&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Specifies the type of RoPE scaling to be applied, if any.&quot;, &quot;title&quot;: &quot;Scaling Type&quot; }, &quot;alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;alpha factor by which to scale rope theta.&quot;, &quot;title&quot;: &quot;Alpha&quot; }, &quot;scaling_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8.0, &quot;description&quot;: &quot;Factor by which to scale RoPE embeddings.&quot;, &quot;title&quot;: &quot;Scaling Factor&quot; }, &quot;low_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Scaling factor applied to the lower frequency components of RoPE.&quot;, &quot;title&quot;: &quot;Low Freq Factor&quot; }, &quot;high_freq_factor&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 4.0, &quot;description&quot;: &quot;Scaling factor applied to the higher frequency components of RoPE.&quot;, &quot;title&quot;: &quot;High Freq Factor&quot; }, &quot;original_max_position_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 8192, &quot;description&quot;: &quot;Original maximum position embeddings for RoPE scaling.&quot;, &quot;title&quot;: &quot;Original Max Position Embeddings&quot; }, &quot;rotary_theta_local&quot;: { &quot;default&quot;: 10000, &quot;description&quot;: &quot;Rotary theta base length for local rotary layers&quot;, &quot;title&quot;: &quot;Rotary Theta Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;interleave_local&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Local rotary layers each 1/N layers&quot;, &quot;title&quot;: &quot;Interleave Local&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tmax_index&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;tmax indexing, 0 for all cases except gemma 3 = 1&quot;, &quot;title&quot;: &quot;Tmax Index&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;title&quot;: &quot;RotaryPositionConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;SuffixConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all source examples.&quot;, &quot;title&quot;: &quot;Src Suffix&quot; }, &quot;tgt_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Suffix&quot; } }, &quot;title&quot;: &quot;SuffixConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;SwitchOutConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;switchout_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for SwitchOut. :math:`\\\\tau^{-1}` in :cite:`DBLP:journals/corr/abs-1808-07512`. Smaller value makes data more diverse.&quot;, &quot;title&quot;: &quot;Switchout Temperature&quot; } }, &quot;title&quot;: &quot;SwitchOutConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TerminologyConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;termbase_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a dictionary file with terms.&quot;, &quot;title&quot;: &quot;Termbase Path&quot; }, &quot;src_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the source corpus.&quot;, &quot;title&quot;: &quot;Src Spacy Language Model&quot; }, &quot;tgt_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the target corpus.&quot;, &quot;title&quot;: &quot;Tgt Spacy Language Model&quot; }, &quot;term_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.3, &quot;description&quot;: &quot;Ratio of corpus to augment with terms.&quot;, &quot;title&quot;: &quot;Term Corpus Ratio&quot; }, &quot;term_example_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.2, &quot;description&quot;: &quot;Maximum terms allowed in an example.&quot;, &quot;title&quot;: &quot;Term Example Ratio&quot; }, &quot;src_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;description&quot;: &quot;The source term start token.&quot;, &quot;title&quot;: &quot;Src Term Stoken&quot; }, &quot;tgt_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;description&quot;: &quot;The target term start token.&quot;, &quot;title&quot;: &quot;Tgt Term Stoken&quot; }, &quot;tgt_term_etoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;description&quot;: &quot;The target term end token.&quot;, &quot;title&quot;: &quot;Tgt Term Etoken&quot; }, &quot;term_source_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented source sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Term Source Delimiter&quot; } }, &quot;title&quot;: &quot;TerminologyConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TokenDropConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tokendrop_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token deletion.&quot;, &quot;title&quot;: &quot;Tokendrop Temperature&quot; } }, &quot;title&quot;: &quot;TokenDropConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TokenMaskConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tokenmask_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token masking.&quot;, &quot;title&quot;: &quot;Tokenmask Temperature&quot; } }, &quot;title&quot;: &quot;TokenMaskConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerDecoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;decoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Decoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the decoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of decoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for tgt.&quot;, &quot;title&quot;: &quot;Tgt Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;coverage_attn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Train a coverage attention layer.&quot;, &quot;title&quot;: &quot;Coverage Attn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_coverage&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for coverage loss of See et al (2017)&quot;, &quot;title&quot;: &quot;Lambda Coverage&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;global_attention&quot;: { &quot;default&quot;: &quot;general&quot;, &quot;description&quot;: &quot;The attention type to use. (Luong=general, Bahdanau=MLP)&quot;, &quot;enum&quot;: [ &quot;dot&quot;, &quot;general&quot;, &quot;mlp&quot;, null ], &quot;title&quot;: &quot;Global Attention&quot; }, &quot;global_attention_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Global attention function to use.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Global Attention Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;aan_useffn&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Turn on the FFN layer in the AAN decoder.&quot;, &quot;title&quot;: &quot;Aan Useffn&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;alignment_layer&quot;: { &quot;default&quot;: -2, &quot;description&quot;: &quot;Layer number which has to be supervised.&quot;, &quot;title&quot;: &quot;Alignment Layer&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;alignment_heads&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of cross attention heads per layer to supervise with.&quot;, &quot;title&quot;: &quot;Alignment Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;full_context_alignment&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Whether alignment is conditioned on full target context.&quot;, &quot;title&quot;: &quot;Full Context Alignment&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;lambda_align&quot;: { &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)&quot;, &quot;title&quot;: &quot;Lambda Align&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;LM_type&quot;: { &quot;default&quot;: &quot;causal&quot;, &quot;description&quot;: &quot;TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)&quot;, &quot;enum&quot;: [ &quot;causal&quot;, &quot;prefix&quot; ], &quot;title&quot;: &quot;Lm Type&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;title&quot;: &quot;TransformerDecoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerEncoderModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Facilitate setting some transformer specific params at model level.&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot;, &quot;type&quot;: &quot;null&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;transformer_encoder&quot;, &quot;default&quot;: &quot;transformer_encoder&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerEncoderModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerLMModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Facilitate setting some transformer specific params at model level.&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot;, &quot;type&quot;: &quot;null&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;transformer_lm&quot;, &quot;default&quot;: &quot;transformer_lm&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerLMModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TransformerModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Facilitate setting some transformer specific params at model level.&quot;, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;transformer&quot;, &quot;default&quot;: &quot;transformer&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; } }, &quot;title&quot;: &quot;TransformerModelConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;UpperCaseConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;upper_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.01, &quot;description&quot;: &quot;Corpus ratio to apply uppercasing.&quot;, &quot;title&quot;: &quot;Upper Corpus Ratio&quot; } }, &quot;title&quot;: &quot;UpperCaseConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;VisionEncoderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Based on mistral-community/pixtral-12b, might evolve later.&quot;, &quot;properties&quot;: { &quot;encoder_type&quot;: { &quot;const&quot;: &quot;vision&quot;, &quot;default&quot;: &quot;vision&quot;, &quot;title&quot;: &quot;Encoder Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;layers&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of layers in the encoder.&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Size of encoder hidden states.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_word_vec_size&quot;: { &quot;default&quot;: 512, &quot;description&quot;: &quot;Word embedding size for src.&quot;, &quot;title&quot;: &quot;Src Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;num_channels&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Num Channels&quot; }, &quot;image_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1024, &quot;title&quot;: &quot;Image Size&quot; }, &quot;patch_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 16, &quot;title&quot;: &quot;Patch Size&quot; }, &quot;image_token_id&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 10, &quot;title&quot;: &quot;Image Token Id&quot; }, &quot;mm_tokens_per_image&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 256, &quot;title&quot;: &quot;Mm Tokens Per Image&quot; }, &quot;layernorm_pre&quot;: { &quot;default&quot;: true, &quot;title&quot;: &quot;Layernorm Pre&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;patch_conv_bias&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Patch Conv Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;encoder_sam&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Encoder Sam&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;use_class_embedding&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Use Class Embedding&quot;, &quot;type&quot;: &quot;boolean&quot; } }, &quot;title&quot;: &quot;VisionEncoderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;VisionTransformerLMModelConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;embeddings&quot;: { &quot;$ref&quot;: &quot;#/$defs/EmbeddingsConfig&quot;, &quot;description&quot;: &quot;Contains most of the args useful to build the Embeddings module.&quot; }, &quot;encoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;brnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;cnn&quot;: &quot;#/$defs/CnnEncoderConfig&quot;, &quot;mean&quot;: &quot;#/$defs/MeanEncoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnEncoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerEncoderConfig&quot;, &quot;vision&quot;: &quot;#/$defs/VisionEncoderConfig&quot; }, &quot;propertyName&quot;: &quot;encoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/MeanEncoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/VisionEncoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of an encoder.&quot;, &quot;title&quot;: &quot;Encoder&quot; }, &quot;decoder&quot;: { &quot;anyOf&quot;: [ { &quot;discriminator&quot;: { &quot;mapping&quot;: { &quot;cnn&quot;: &quot;#/$defs/CnnDecoderConfig&quot;, &quot;rnn&quot;: &quot;#/$defs/RnnDecoderConfig&quot;, &quot;transformer&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, &quot;propertyName&quot;: &quot;decoder_type&quot; }, &quot;oneOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/TransformerDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/RnnDecoderConfig&quot; }, { &quot;$ref&quot;: &quot;#/$defs/CnnDecoderConfig&quot; } ] }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Major parameters of a decoder.&quot;, &quot;title&quot;: &quot;Decoder&quot; }, &quot;hidden_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.&quot;, &quot;title&quot;: &quot;Hidden Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;word_vec_size&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Word embedding size for src and tgt.&quot;, &quot;title&quot;: &quot;Word Vec Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;layers&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).&quot;, &quot;title&quot;: &quot;Layers&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;transformer_ff&quot;: { &quot;default&quot;: 2048, &quot;description&quot;: &quot;Size of hidden transformer feed-forward.&quot;, &quot;title&quot;: &quot;Transformer Ff&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_transformer_ff&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Size of hidden moe transformer feed-forward.&quot;, &quot;title&quot;: &quot;Moe Transformer Ff&quot; }, &quot;share_decoder_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a share weight matrix for the input and output word embeddings in the decoder.&quot;, &quot;title&quot;: &quot;Share Decoder Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;share_embeddings&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.&quot;, &quot;title&quot;: &quot;Share Embeddings&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;input_feed&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.&quot;, &quot;title&quot;: &quot;Input Feed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;generator_function&quot;: { &quot;default&quot;: &quot;softmax&quot;, &quot;description&quot;: &quot;Which function to use for generating probabilities over the target vocabulary.&quot;, &quot;enum&quot;: [ &quot;softmax&quot;, &quot;sparsemax&quot; ], &quot;title&quot;: &quot;Generator Function&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;generator_bias&quot;: { &quot;default&quot;: true, &quot;description&quot;: &quot;Control whether or not the generator Linear module has bias weights.&quot;, &quot;title&quot;: &quot;Generator Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;adapter_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Control whether or not the adapter module has bias weights.&quot;, &quot;title&quot;: &quot;Adapter Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;projector_activation_fn&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in adapter projector layer.&quot; }, &quot;spatial_merge_size&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Control the presence and size of patch merger (Mistral3)&quot;, &quot;title&quot;: &quot;Spatial Merge Size&quot; }, &quot;add_estimator&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add estimator layer&quot;, &quot;title&quot;: &quot;Add Estimator&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;estimator_type&quot;: { &quot;default&quot;: &quot;average&quot;, &quot;description&quot;: &quot;Which hidden_states to use to feed the estimator&quot;, &quot;enum&quot;: [ &quot;average&quot;, &quot;last_token&quot;, &quot;first_token&quot; ], &quot;title&quot;: &quot;Estimator Type&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;left_pad&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Enable left-padding, useful for some LLMs.&quot;, &quot;title&quot;: &quot;Left Pad&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Original huggingface model.&quot;, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;eole_version&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;0.4.0&quot;, &quot;description&quot;: &quot;Eole version used to convert/train/save the model.&quot;, &quot;title&quot;: &quot;Eole Version&quot; }, &quot;architecture&quot;: { &quot;const&quot;: &quot;vision_transformer_lm&quot;, &quot;default&quot;: &quot;vision_transformer_lm&quot;, &quot;title&quot;: &quot;Architecture&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;sliding_window&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Sliding window for transformer self-attention.&quot;, &quot;title&quot;: &quot;Sliding Window&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;heads&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Number of heads for transformer self-attention.&quot;, &quot;title&quot;: &quot;Heads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;relative_positions_buckets&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).&quot;, &quot;title&quot;: &quot;Relative Positions Buckets&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;mlp_activation_fn&quot;: { &quot;$ref&quot;: &quot;#/$defs/ActivationFunction&quot;, &quot;default&quot;: &quot;relu&quot;, &quot;description&quot;: &quot;The activation function to use in MLP layer.&quot; }, &quot;layer_norm&quot;: { &quot;default&quot;: &quot;standard&quot;, &quot;description&quot;: &quot;Type of layer normalization in transformer architecture.&quot;, &quot;enum&quot;: [ &quot;standard&quot;, &quot;standardFP32&quot;, &quot;rms&quot;, &quot;gemma-rms&quot; ], &quot;title&quot;: &quot;Layer Norm&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;norm_eps&quot;: { &quot;default&quot;: 1e-05, &quot;description&quot;: &quot;Layer norm epsilon.&quot;, &quot;title&quot;: &quot;Norm Eps&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;shared_layer_norm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.&quot;, &quot;title&quot;: &quot;Shared Layer Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;ffn_layernorm&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.&quot;, &quot;title&quot;: &quot;Ffn Layernorm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_qkvbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.&quot;, &quot;title&quot;: &quot;Add Qkvbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;query_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Query Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;key_norm&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Key Norm&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;qk_norm_post_rope&quot;: { &quot;default&quot;: false, &quot;title&quot;: &quot;Qk Norm Post Rope&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;add_final_linear_bias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of final_linear in MHA.&quot;, &quot;title&quot;: &quot;Add Final Linear Bias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;heads_kv&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)&quot;, &quot;title&quot;: &quot;Heads Kv&quot; }, &quot;head_dim&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Head dimension when this needs to be different vs hidden_size // heads&quot;, &quot;title&quot;: &quot;Head Dim&quot; }, &quot;attn_scaling&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Attention scaling factor, when None uses 1/sqrt(head_dim) by default&quot;, &quot;title&quot;: &quot;Attn Scaling&quot; }, &quot;add_ffnbias&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Add bias to nn.Linear of MLP FFN.&quot;, &quot;title&quot;: &quot;Add Ffnbias&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;parallel_residual&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.&quot;, &quot;title&quot;: &quot;Parallel Residual&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of experts for MoE models.&quot;, &quot;title&quot;: &quot;Num Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_shared_experts&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of shared experts for MoE models (DeepSeekv2).&quot;, &quot;title&quot;: &quot;Num Shared Experts&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;first_k_dense_replace&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Number of layers using Dense instead of MoE&quot;, &quot;title&quot;: &quot;First K Dense Replace&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;num_experts_per_tok&quot;: { &quot;default&quot;: 2, &quot;description&quot;: &quot;Number of experts per token.&quot;, &quot;title&quot;: &quot;Num Experts Per Tok&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;moe_softmax_after&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Usually softmax is before topk, Mixtral does it after.&quot;, &quot;title&quot;: &quot;Moe Softmax After&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;position_encoding_type&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/PositionEncodingType&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;SinusoidalInterleaved&quot;, &quot;description&quot;: &quot;Type of positional encoding.&quot; }, &quot;n_positions&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative&quot;, &quot;title&quot;: &quot;N Positions&quot; }, &quot;rope_config&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/RotaryPositionConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Rotary position config, if relevant.&quot; }, &quot;adapter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;llava&quot;, &quot;description&quot;: &quot;Adapter type to use in the model.&quot;, &quot;title&quot;: &quot;Adapter&quot; } }, &quot;title&quot;: &quot;VisionTransformerLMModelConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;additionalProperties&quot;: false, &quot;required&quot;: [ &quot;model_path&quot;, &quot;src&quot; ] }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = forbidprotected_namespaces: tuple = ()arbitrary_types_allowed: bool = True Fields: engine (str)model (eole.config.models.TransformerModelConfig | eole.config.models.TransformerLMModelConfig | eole.config.models.VisionTransformerLMModelConfig | eole.config.models.TransformerEncoderModelConfig | eole.config.models.RnnModelConfig | eole.config.models.CnnModelConfig | eole.config.models.CustomModelConfig | None)model_path (str | List[str])output (str)share_vocab (bool | None)src (str)src_subword_vocab (str | None)tgt (str | None)tgt_file_prefix (bool)transforms (List[str] | None)transforms_configs (eole.config.data.NestedAllTransformsConfig | None) Validators: _validate_model_path » model_path_validate_predict_config » all fields  field engine : str = 'eole'​  engine to run inference: eole or ct2  Validated by: _validate_predict_config_validate_running_config  field model : TransformerModelConfig | TransformerLMModelConfig | VisionTransformerLMModelConfig | TransformerEncoderModelConfig | RnnModelConfig | CnnModelConfig | CustomModelConfig | None = None​  Validated by: _validate_predict_config_validate_running_config  field model_path : str | List[str] [Required]​  Path to model .pt file(s). Multiple models can be specified for ensemble decoding.  Validated by: _validate_model_path_validate_predict_config_validate_running_config  field output : str = 'pred.txt'​  Path to output the predictions (each line will be the decoded sequence).  Validated by: _validate_predict_config_validate_running_config  field share_vocab : bool | None = False​  Validated by: _validate_predict_config_validate_running_config  field src : str [Required]​  Source file to decode (one line per sequence).  Validated by: _validate_predict_config_validate_running_config  field src_subword_vocab : str | None = None​  Validated by: _validate_predict_config_validate_running_config  field tgt : str | None = None​  True target sequences, useful for scoring or prefix decoding.  Validated by: _validate_predict_config_validate_running_config  field tgt_file_prefix : bool = False​  Generate predictions using provided tgt as prefix.  Validated by: _validate_predict_config_validate_running_config  field transforms : List[str] | None = []​  Validated by: _validate_predict_config_validate_running_config  field transforms_configs : NestedAllTransformsConfig | None [Optional]​  Validated by: _validate_predict_config_validate_running_config  model_post_init(context: Any, /)​  This function is meant to behave like a BaseModel method to initialise private attributes.  It takes context as an argument since that’s what pydantic-core passes when calling it.  Parameters: self – The BaseModel instance.context – The context.  ","version":"Next","tagName":"h3"},{"title":"pydantic model eole.config.run.BuildVocabConfig​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#pydantic-model-eoleconfigrunbuildvocabconfig","content":" Bases: DataConfig, MiscConfig, BaseVocabConfig   Show JSON schema { &quot;title&quot;: &quot;BuildVocabConfig&quot;, &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: { &quot;src_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;description&quot;: &quot;Path to src (or shared) vocabulary file. Format: one &lt;word&gt; or &lt;word&gt;\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Vocab&quot; }, &quot;tgt_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to tgt vocabulary file. Format: one &lt;word&gt; or &lt;word&gt;\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Vocab&quot; }, &quot;share_vocab&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Share source and target vocabulary.&quot;, &quot;title&quot;: &quot;Share Vocab&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;decoder_start_token&quot;: { &quot;default&quot;: &quot;&amp;lt;s&amp;gt;&quot;, &quot;description&quot;: &quot;Default decoder start token. For most models it is &amp;lt;s&amp;gt; = BOS. Some fairseq models require &amp;lt;/s&amp;gt;.&quot;, &quot;title&quot;: &quot;Decoder Start Token&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;bos_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&amp;lt;s&amp;gt;&quot;, &quot;title&quot;: &quot;Bos Token&quot; }, &quot;eos_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&amp;lt;/s&amp;gt;&quot;, &quot;title&quot;: &quot;Eos Token&quot; }, &quot;unk_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&lt;unk&gt;&quot;, &quot;title&quot;: &quot;Unk Token&quot; }, &quot;pad_token&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&lt;blank&gt;&quot;, &quot;title&quot;: &quot;Pad Token&quot; }, &quot;both_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for both source and target tokens.&quot;, &quot;title&quot;: &quot;Both Embeddings&quot; }, &quot;src_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for source tokens.&quot;, &quot;title&quot;: &quot;Src Embeddings&quot; }, &quot;tgt_embeddings&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to the embeddings file to use for target tokens.&quot;, &quot;title&quot;: &quot;Tgt Embeddings&quot; }, &quot;embeddings_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;GloVe&quot;, &quot;word2vec&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Type of embeddings file.&quot;, &quot;title&quot;: &quot;Embeddings Type&quot; }, &quot;seed&quot;: { &quot;default&quot;: -1, &quot;description&quot;: &quot;Set random seed used for better reproducibility between experiments.&quot;, &quot;title&quot;: &quot;Seed&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_vocab_size&quot;: { &quot;default&quot;: 32758, &quot;description&quot;: &quot;Maximum size of the source vocabulary.&quot;, &quot;title&quot;: &quot;Src Vocab Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_vocab_size&quot;: { &quot;default&quot;: 32768, &quot;description&quot;: &quot;Maximum size of the target vocabulary.&quot;, &quot;title&quot;: &quot;Tgt Vocab Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;vocab_size_multiple&quot;: { &quot;default&quot;: 8, &quot;description&quot;: &quot;Make the vocabulary size a multiple of this value. (Adds dummy tokens if needed.)&quot;, &quot;title&quot;: &quot;Vocab Size Multiple&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;src_words_min_frequency&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Discard source words with lower frequency.&quot;, &quot;title&quot;: &quot;Src Words Min Frequency&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;tgt_words_min_frequency&quot;: { &quot;default&quot;: 0, &quot;description&quot;: &quot;Discard target words with lower frequency.&quot;, &quot;title&quot;: &quot;Tgt Words Min Frequency&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;data&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: { &quot;$ref&quot;: &quot;#/$defs/Dataset&quot; }, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;description&quot;: &quot;All datasets and their specifications. See examples/*.yaml for further details.&quot;, &quot;title&quot;: &quot;Data&quot; }, &quot;transforms&quot;: { &quot;default&quot;: [], &quot;description&quot;: &quot;Default transform pipeline to apply to data. Can be specified in each corpus of data to override.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;title&quot;: &quot;Transforms&quot;, &quot;type&quot;: &quot;array&quot; }, &quot;transforms_configs&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/$defs/NestedAllTransformsConfig&quot; }, { &quot;type&quot;: &quot;null&quot; } ] }, &quot;skip_empty_level&quot;: { &quot;default&quot;: &quot;warning&quot;, &quot;description&quot;: &quot;Logging level when encoutering empty examples. (silent: silently ignore/skip empty examples, warning: warn when ignoring/skipping empty examples, error: raise an error and stop execution when any empty example)&quot;, &quot;enum&quot;: [ &quot;silent&quot;, &quot;warning&quot;, &quot;error&quot; ], &quot;title&quot;: &quot;Skip Empty Level&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;n_sample&quot;: { &quot;default&quot;: 5000, &quot;description&quot;: &quot;Number of transformed samples per corpus to use to build the vocabulary. Set to -1 to use the full corpora.&quot;, &quot;title&quot;: &quot;N Sample&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;save_data&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Output base path for objects that will be saved (vocab, transforms, embeddings, ...)&quot;, &quot;title&quot;: &quot;Save Data&quot; }, &quot;overwrite&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Overwrite existing objects if any.&quot;, &quot;title&quot;: &quot;Overwrite&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;dump_samples&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Dump samples when building vocabulary. Warning: this may slow down the process.&quot;, &quot;title&quot;: &quot;Dump Samples&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;num_threads&quot;: { &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of parallel threads to build the vocabulary.&quot;, &quot;title&quot;: &quot;Num Threads&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;learn_subwords&quot;: { &quot;default&quot;: false, &quot;description&quot;: &quot;Learn subwords (based on defined transforms) prior to building vocabulary.&quot;, &quot;title&quot;: &quot;Learn Subwords&quot;, &quot;type&quot;: &quot;boolean&quot; }, &quot;learn_subwords_size&quot;: { &quot;default&quot;: 32000, &quot;description&quot;: &quot;Number of subwords operations to learn.&quot;, &quot;title&quot;: &quot;Learn Subwords Size&quot;, &quot;type&quot;: &quot;integer&quot; }, &quot;vocab_sample_queue_size&quot;: { &quot;default&quot;: 20, &quot;description&quot;: &quot;Size of queues used for dumping samples.&quot;, &quot;title&quot;: &quot;Vocab Sample Queue Size&quot;, &quot;type&quot;: &quot;integer&quot; } }, &quot;$defs&quot;: { &quot;BARTNoiseConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;permute_sent_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Permute this proportion of sentences (boundaries defined by ['.', '?', '!']) in all inputs.&quot;, &quot;title&quot;: &quot;Permute Sent Ratio&quot; }, &quot;rotate_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Rotate this proportion of inputs.&quot;, &quot;title&quot;: &quot;Rotate Ratio&quot; }, &quot;insert_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Insert this percentage of additional random tokens.&quot;, &quot;title&quot;: &quot;Insert Ratio&quot; }, &quot;random_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Instead of using &lt;mask&gt;, use random token this often.&quot;, &quot;title&quot;: &quot;Random Ratio&quot; }, &quot;mask_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.0, &quot;description&quot;: &quot;Fraction of words/subwords that will be masked.&quot;, &quot;title&quot;: &quot;Mask Ratio&quot; }, &quot;mask_length&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;subword&quot;, &quot;word&quot;, &quot;span-poisson&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;subword&quot;, &quot;description&quot;: &quot;Length of masking window to apply.&quot;, &quot;title&quot;: &quot;Mask Length&quot; }, &quot;poisson_lambda&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;Lambda for Poisson distribution to sample span length if `-mask_length` set to span-poisson.&quot;, &quot;title&quot;: &quot;Poisson Lambda&quot; }, &quot;replace_length&quot;: { &quot;anyOf&quot;: [ { &quot;maximum&quot;: 1, &quot;minimum&quot;: -1, &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: -1, &quot;description&quot;: &quot;When masking N tokens, replace with 0, 1, or N tokens. (use -1 for N)&quot;, &quot;title&quot;: &quot;Replace Length&quot; } }, &quot;title&quot;: &quot;BARTNoiseConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;BaseTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; } }, &quot;title&quot;: &quot;BaseTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;CleanConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_eq_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex src==tgt&quot;, &quot;title&quot;: &quot;Src Eq Tgt&quot; }, &quot;same_char&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same char more than 4 times&quot;, &quot;title&quot;: &quot;Same Char&quot; }, &quot;same_word&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove ex with same word more than 3 times&quot;, &quot;title&quot;: &quot;Same Word&quot; }, &quot;scripts_ok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;description&quot;: &quot;list of unicodata scripts accepted&quot;, &quot;title&quot;: &quot;Scripts Ok&quot; }, &quot;scripts_nok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of unicodata scripts not accepted&quot;, &quot;title&quot;: &quot;Scripts Nok&quot; }, &quot;src_tgt_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 2.0, &quot;description&quot;: &quot;ratio between src and tgt&quot;, &quot;title&quot;: &quot;Src Tgt Ratio&quot; }, &quot;avg_tok_min&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3.0, &quot;description&quot;: &quot;average length of tokens min&quot;, &quot;title&quot;: &quot;Avg Tok Min&quot; }, &quot;avg_tok_max&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 20.0, &quot;description&quot;: &quot;average length of tokens max&quot;, &quot;title&quot;: &quot;Avg Tok Max&quot; }, &quot;langid&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;description&quot;: &quot;list of languages accepted&quot;, &quot;title&quot;: &quot;Langid&quot; } }, &quot;title&quot;: &quot;CleanConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;Dataset&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;name&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Name&quot; }, &quot;weight&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;title&quot;: &quot;Weight&quot; }, &quot;transforms&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Transforms&quot; }, &quot;path_src&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Src&quot; }, &quot;path_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Tgt&quot; }, &quot;path_sco&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Sco&quot; }, &quot;path_txt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Txt&quot; }, &quot;path_align&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path Align&quot; }, &quot;src_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Prefix&quot; }, &quot;tgt_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Prefix&quot; }, &quot;src_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Suffix&quot; }, &quot;tgt_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Suffix&quot; }, &quot;src_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Src Lang&quot; }, &quot;tgt_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Tgt Lang&quot; }, &quot;penn&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Penn&quot; }, &quot;norm_quote_commas&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Norm Quote Commas&quot; }, &quot;norm_numbers&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Norm Numbers&quot; }, &quot;pre_replace_unicode_punct&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Pre Replace Unicode Punct&quot; }, &quot;post_remove_control_chars&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;title&quot;: &quot;Post Remove Control Chars&quot; }, &quot;src_eq_tgt&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Src Eq Tgt&quot; }, &quot;same_char&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Same Char&quot; }, &quot;same_word&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;title&quot;: &quot;Same Word&quot; }, &quot;scripts_ok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;title&quot;: &quot;Scripts Ok&quot; }, &quot;scripts_nok&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [], &quot;title&quot;: &quot;Scripts Nok&quot; }, &quot;src_tgt_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 2, &quot;title&quot;: &quot;Src Tgt Ratio&quot; }, &quot;avg_tok_min&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 3, &quot;title&quot;: &quot;Avg Tok Min&quot; }, &quot;avg_tok_max&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 20, &quot;title&quot;: &quot;Avg Tok Max&quot; }, &quot;lang_id&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;en&quot;, &quot;fr&quot; ], &quot;title&quot;: &quot;Lang Id&quot; } }, &quot;title&quot;: &quot;Dataset&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;DocifyConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;doc_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 200, &quot;description&quot;: &quot;Number of tokens per doc.&quot;, &quot;title&quot;: &quot;Doc Length&quot; }, &quot;max_context&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Max context segments.&quot;, &quot;title&quot;: &quot;Max Context&quot; } }, &quot;title&quot;: &quot;DocifyConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;FilterTooLongConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 192, &quot;description&quot;: &quot;Maximum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;title&quot;: &quot;FilterTooLongConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;FilterTooShortConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 48, &quot;description&quot;: &quot;Minimum source sequence length.&quot;, &quot;title&quot;: &quot;Src Seq Length&quot; }, &quot;tgt_seq_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 48, &quot;description&quot;: &quot;Minimum target sequence length.&quot;, &quot;title&quot;: &quot;Tgt Seq Length&quot; } }, &quot;title&quot;: &quot;FilterTooShortConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;HuggingfaceTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Path&quot; }, &quot;huggingface_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Huggingface Model&quot; }, &quot;max_length&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;title&quot;: &quot;Max Length&quot; } }, &quot;title&quot;: &quot;HuggingfaceTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InlineTagsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tags_dictionary_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a flat term dictionary.&quot;, &quot;title&quot;: &quot;Tags Dictionary Path&quot; }, &quot;tags_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.1, &quot;description&quot;: &quot;Ratio of corpus to augment with tags.&quot;, &quot;title&quot;: &quot;Tags Corpus Ratio&quot; }, &quot;max_tags&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 12, &quot;description&quot;: &quot;Maximum number of tags that can be added to a single sentence.&quot;, &quot;title&quot;: &quot;Max Tags&quot; }, &quot;paired_stag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;description&quot;: &quot;The format of an opening paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Stag&quot; }, &quot;paired_etag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;description&quot;: &quot;The format of a closing paired inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Paired Etag&quot; }, &quot;isolated_tag&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;description&quot;: &quot;The format of an isolated inline tag. Must include the character #.&quot;, &quot;title&quot;: &quot;Isolated Tag&quot; }, &quot;src_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented src sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Src Delimiter&quot; } }, &quot;title&quot;: &quot;InlineTagsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;InsertMaskBeforePlaceholderConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;response_patterns&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ], &quot;description&quot;: &quot;Response pattern to locate the end of the prompt.&quot;, &quot;title&quot;: &quot;Response Patterns&quot; } }, &quot;title&quot;: &quot;InsertMaskBeforePlaceholderConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;NestedAllTransformsConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;huggingface_tokenize&quot;: { &quot;$ref&quot;: &quot;#/$defs/HuggingfaceTokenizerConfig&quot;, &quot;default&quot;: { &quot;path&quot;: null, &quot;huggingface_model&quot;: null, &quot;max_length&quot;: null } }, &quot;uppercase&quot;: { &quot;$ref&quot;: &quot;#/$defs/UpperCaseConfig&quot;, &quot;default&quot;: { &quot;upper_corpus_ratio&quot;: 0.01 } }, &quot;terminology&quot;: { &quot;$ref&quot;: &quot;#/$defs/TerminologyConfig&quot;, &quot;default&quot;: { &quot;termbase_path&quot;: null, &quot;src_spacy_language_model&quot;: null, &quot;tgt_spacy_language_model&quot;: null, &quot;term_corpus_ratio&quot;: 0.3, &quot;term_example_ratio&quot;: 0.2, &quot;src_term_stoken&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;tgt_term_stoken&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;tgt_term_etoken&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;term_source_delimiter&quot;: &quot;\\uff5ffuzzy\\uff60&quot; } }, &quot;sentencepiece&quot;: { &quot;$ref&quot;: &quot;#/$defs/BaseTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0 } }, &quot;bpe&quot;: { &quot;$ref&quot;: &quot;#/$defs/BaseTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0 } }, &quot;onmt_tokenize&quot;: { &quot;$ref&quot;: &quot;#/$defs/ONMTTokenizerConfig&quot;, &quot;default&quot;: { &quot;src_subword_model&quot;: null, &quot;tgt_subword_model&quot;: null, &quot;src_subword_nbest&quot;: 1, &quot;tgt_subword_nbest&quot;: 1, &quot;src_subword_alpha&quot;: 0.0, &quot;tgt_subword_alpha&quot;: 0.0, &quot;src_subword_vocab&quot;: &quot;&quot;, &quot;tgt_subword_vocab&quot;: &quot;&quot;, &quot;src_vocab_threshold&quot;: 0, &quot;tgt_vocab_threshold&quot;: 0, &quot;src_subword_type&quot;: &quot;none&quot;, &quot;tgt_subword_type&quot;: &quot;none&quot;, &quot;src_onmttok_kwargs&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;gpt2_pretok&quot;: false, &quot;mapped_tokens&quot;: null } }, &quot;bart&quot;: { &quot;$ref&quot;: &quot;#/$defs/BARTNoiseConfig&quot;, &quot;default&quot;: { &quot;permute_sent_ratio&quot;: 0.0, &quot;rotate_ratio&quot;: 0.0, &quot;insert_ratio&quot;: 0.0, &quot;random_ratio&quot;: 0.0, &quot;mask_ratio&quot;: 0.0, &quot;mask_length&quot;: &quot;subword&quot;, &quot;poisson_lambda&quot;: 3.0, &quot;replace_length&quot;: -1 } }, &quot;switchout&quot;: { &quot;$ref&quot;: &quot;#/$defs/SwitchOutConfig&quot;, &quot;default&quot;: { &quot;switchout_temperature&quot;: 1.0 } }, &quot;tokendrop&quot;: { &quot;$ref&quot;: &quot;#/$defs/TokenDropConfig&quot;, &quot;default&quot;: { &quot;tokendrop_temperature&quot;: 1.0 } }, &quot;tokenmask&quot;: { &quot;$ref&quot;: &quot;#/$defs/TokenMaskConfig&quot;, &quot;default&quot;: { &quot;tokenmask_temperature&quot;: 1.0 } }, &quot;normalize&quot;: { &quot;$ref&quot;: &quot;#/$defs/NormalizeConfig&quot;, &quot;default&quot;: { &quot;src_lang&quot;: &quot;&quot;, &quot;tgt_lang&quot;: &quot;&quot;, &quot;penn&quot;: true, &quot;norm_quote_commas&quot;: true, &quot;norm_numbers&quot;: true, &quot;pre_replace_unicode_punct&quot;: false, &quot;post_remove_control_chars&quot;: false } }, &quot;insert_mask_before_placeholder&quot;: { &quot;$ref&quot;: &quot;#/$defs/InsertMaskBeforePlaceholderConfig&quot;, &quot;default&quot;: { &quot;response_patterns&quot;: [ &quot;Response : \\uff5fnewline\\uff60&quot; ] } }, &quot;filtertooshort&quot;: { &quot;$ref&quot;: &quot;#/$defs/FilterTooShortConfig&quot;, &quot;default&quot;: { &quot;src_seq_length&quot;: 48, &quot;tgt_seq_length&quot;: 48 } }, &quot;filtertoolong&quot;: { &quot;$ref&quot;: &quot;#/$defs/FilterTooLongConfig&quot;, &quot;default&quot;: { &quot;src_seq_length&quot;: 192, &quot;tgt_seq_length&quot;: 192 } }, &quot;prefix&quot;: { &quot;$ref&quot;: &quot;#/$defs/PrefixConfig&quot;, &quot;default&quot;: { &quot;src_prefix&quot;: &quot;&quot;, &quot;tgt_prefix&quot;: &quot;&quot; } }, &quot;suffix&quot;: { &quot;$ref&quot;: &quot;#/$defs/SuffixConfig&quot;, &quot;default&quot;: { &quot;src_suffix&quot;: &quot;&quot;, &quot;tgt_suffix&quot;: &quot;&quot; } }, &quot;docify&quot;: { &quot;$ref&quot;: &quot;#/$defs/DocifyConfig&quot;, &quot;default&quot;: { &quot;doc_length&quot;: 200, &quot;max_context&quot;: 1 } }, &quot;inlinetags&quot;: { &quot;$ref&quot;: &quot;#/$defs/InlineTagsConfig&quot;, &quot;default&quot;: { &quot;tags_dictionary_path&quot;: null, &quot;tags_corpus_ratio&quot;: 0.1, &quot;max_tags&quot;: 12, &quot;paired_stag&quot;: &quot;\\uff5fph_#_beg\\uff60&quot;, &quot;paired_etag&quot;: &quot;\\uff5fph_#_end\\uff60&quot;, &quot;isolated_tag&quot;: &quot;\\uff5fph_#_std\\uff60&quot;, &quot;src_delimiter&quot;: &quot;\\uff5ffuzzy\\uff60&quot; } }, &quot;clean&quot;: { &quot;$ref&quot;: &quot;#/$defs/CleanConfig&quot;, &quot;default&quot;: { &quot;src_eq_tgt&quot;: false, &quot;same_char&quot;: false, &quot;same_word&quot;: false, &quot;scripts_ok&quot;: [ &quot;Latin&quot;, &quot;Common&quot; ], &quot;scripts_nok&quot;: [], &quot;src_tgt_ratio&quot;: 2.0, &quot;avg_tok_min&quot;: 3.0, &quot;avg_tok_max&quot;: 20.0, &quot;langid&quot;: [] } } }, &quot;title&quot;: &quot;NestedAllTransformsConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;NormalizeConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Source language code&quot;, &quot;title&quot;: &quot;Src Lang&quot; }, &quot;tgt_lang&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Target language code&quot;, &quot;title&quot;: &quot;Tgt Lang&quot; }, &quot;penn&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Penn substitution&quot;, &quot;title&quot;: &quot;Penn&quot; }, &quot;norm_quote_commas&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize quotations and commas&quot;, &quot;title&quot;: &quot;Norm Quote Commas&quot; }, &quot;norm_numbers&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: true, &quot;description&quot;: &quot;Normalize numbers&quot;, &quot;title&quot;: &quot;Norm Numbers&quot; }, &quot;pre_replace_unicode_punct&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Replace unicode punct&quot;, &quot;title&quot;: &quot;Pre Replace Unicode Punct&quot; }, &quot;post_remove_control_chars&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Remove control chars&quot;, &quot;title&quot;: &quot;Post Remove Control Chars&quot; } }, &quot;title&quot;: &quot;NormalizeConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;ONMTTokenizerConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for src (or shared).&quot;, &quot;title&quot;: &quot;Src Subword Model&quot; }, &quot;tgt_subword_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path of subword model for tgt.&quot;, &quot;title&quot;: &quot;Tgt Subword Model&quot; }, &quot;src_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Nbest&quot; }, &quot;tgt_subword_nbest&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1, &quot;description&quot;: &quot;Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Nbest&quot; }, &quot;src_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)&quot;, &quot;title&quot;: &quot;Src Subword Alpha&quot; }, &quot;tgt_subword_alpha&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)&quot;, &quot;title&quot;: &quot;Tgt Subword Alpha&quot; }, &quot;src_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for src subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Src Subword Vocab&quot; }, &quot;tgt_subword_vocab&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;Path to the vocabulary file for tgt subword. Format: &lt;word&gt;\\\\t&lt;count&gt; per line.&quot;, &quot;title&quot;: &quot;Tgt Subword Vocab&quot; }, &quot;src_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce src subword in src_subword_vocab with frequency &gt;= src_vocab_threshold.&quot;, &quot;title&quot;: &quot;Src Vocab Threshold&quot; }, &quot;tgt_vocab_threshold&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;integer&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0, &quot;description&quot;: &quot;Only produce tgt subword in tgt_subword_vocab with frequency &gt;= tgt_vocab_threshold.&quot;, &quot;title&quot;: &quot;Tgt Vocab Threshold&quot; }, &quot;src_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for src (or shared) in pyonmttok.&quot;, &quot;title&quot;: &quot;Src Subword Type&quot; }, &quot;tgt_subword_type&quot;: { &quot;anyOf&quot;: [ { &quot;enum&quot;: [ &quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot; ], &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;none&quot;, &quot;description&quot;: &quot;Type of subword model for tgt in pyonmttok.&quot;, &quot;title&quot;: &quot;Tgt Subword Type&quot; }, &quot;src_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for src in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Src Onmttok Kwargs&quot; }, &quot;tgt_onmttok_kwargs&quot;: { &quot;anyOf&quot;: [ { &quot;additionalProperties&quot;: true, &quot;type&quot;: &quot;object&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: { &quot;mode&quot;: &quot;none&quot; }, &quot;description&quot;: &quot;Other pyonmttok options for tgt in dict string, except subword related options listed earlier.&quot;, &quot;title&quot;: &quot;Tgt Onmttok Kwargs&quot; }, &quot;gpt2_pretok&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;boolean&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: false, &quot;description&quot;: &quot;Preprocess sentence with byte-level mapping.&quot;, &quot;title&quot;: &quot;Gpt2 Pretok&quot; }, &quot;mapped_tokens&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;maxItems&quot;: 2, &quot;minItems&quot;: 2, &quot;prefixItems&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;string&quot; } ], &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Mapped tokens for placeholders preservation&quot;, &quot;title&quot;: &quot;Mapped Tokens&quot; } }, &quot;title&quot;: &quot;ONMTTokenizerConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;PrefixConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all source examples.&quot;, &quot;title&quot;: &quot;Src Prefix&quot; }, &quot;tgt_prefix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to prepend to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Prefix&quot; } }, &quot;title&quot;: &quot;PrefixConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;SuffixConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;src_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all source examples.&quot;, &quot;title&quot;: &quot;Src Suffix&quot; }, &quot;tgt_suffix&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;&quot;, &quot;description&quot;: &quot;String to append to all target examples.&quot;, &quot;title&quot;: &quot;Tgt Suffix&quot; } }, &quot;title&quot;: &quot;SuffixConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;SwitchOutConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;switchout_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for SwitchOut. :math:`\\\\tau^{-1}` in :cite:`DBLP:journals/corr/abs-1808-07512`. Smaller value makes data more diverse.&quot;, &quot;title&quot;: &quot;Switchout Temperature&quot; } }, &quot;title&quot;: &quot;SwitchOutConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TerminologyConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;termbase_path&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Path to a dictionary file with terms.&quot;, &quot;title&quot;: &quot;Termbase Path&quot; }, &quot;src_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the source corpus.&quot;, &quot;title&quot;: &quot;Src Spacy Language Model&quot; }, &quot;tgt_spacy_language_model&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: null, &quot;description&quot;: &quot;Name of the spaCy language model for the target corpus.&quot;, &quot;title&quot;: &quot;Tgt Spacy Language Model&quot; }, &quot;term_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.3, &quot;description&quot;: &quot;Ratio of corpus to augment with terms.&quot;, &quot;title&quot;: &quot;Term Corpus Ratio&quot; }, &quot;term_example_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.2, &quot;description&quot;: &quot;Maximum terms allowed in an example.&quot;, &quot;title&quot;: &quot;Term Example Ratio&quot; }, &quot;src_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5fsrc_term_start\\uff60&quot;, &quot;description&quot;: &quot;The source term start token.&quot;, &quot;title&quot;: &quot;Src Term Stoken&quot; }, &quot;tgt_term_stoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_start\\uff60&quot;, &quot;description&quot;: &quot;The target term start token.&quot;, &quot;title&quot;: &quot;Tgt Term Stoken&quot; }, &quot;tgt_term_etoken&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ftgt_term_end\\uff60&quot;, &quot;description&quot;: &quot;The target term end token.&quot;, &quot;title&quot;: &quot;Tgt Term Etoken&quot; }, &quot;term_source_delimiter&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;string&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: &quot;\\uff5ffuzzy\\uff60&quot;, &quot;description&quot;: &quot;Any special token used for augmented source sentences. The default is the fuzzy token used in the FuzzyMatch transform.&quot;, &quot;title&quot;: &quot;Term Source Delimiter&quot; } }, &quot;title&quot;: &quot;TerminologyConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TokenDropConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tokendrop_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token deletion.&quot;, &quot;title&quot;: &quot;Tokendrop Temperature&quot; } }, &quot;title&quot;: &quot;TokenDropConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;TokenMaskConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;tokenmask_temperature&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 1.0, &quot;description&quot;: &quot;Sampling temperature for token masking.&quot;, &quot;title&quot;: &quot;Tokenmask Temperature&quot; } }, &quot;title&quot;: &quot;TokenMaskConfig&quot;, &quot;type&quot;: &quot;object&quot; }, &quot;UpperCaseConfig&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;upper_corpus_ratio&quot;: { &quot;anyOf&quot;: [ { &quot;type&quot;: &quot;number&quot; }, { &quot;type&quot;: &quot;null&quot; } ], &quot;default&quot;: 0.01, &quot;description&quot;: &quot;Corpus ratio to apply uppercasing.&quot;, &quot;title&quot;: &quot;Upper Corpus Ratio&quot; } }, &quot;title&quot;: &quot;UpperCaseConfig&quot;, &quot;type&quot;: &quot;object&quot; } }, &quot;required&quot;: [ &quot;src_vocab&quot;, &quot;data&quot; ] }    Config: validate_assignment: bool = Truevalidate_default: bool = Trueuse_enum_values: bool = Trueextra: str = ignoreprotected_namespaces: tuple = () Fields: dump_samples (bool)learn_subwords (bool)learn_subwords_size (int)n_sample (int)num_threads (int)vocab_sample_queue_size (int) Validators: _validate_build_vocab_config » all fields  field dump_samples : bool = False​  Dump samples when building vocabulary. Warning: this may slow down the process.  Validated by: _maybe_set_huggingface_model_validate_build_vocab_config  field learn_subwords : bool = False​  Learn subwords (based on defined transforms) prior to building vocabulary.  Validated by: _maybe_set_huggingface_model_validate_build_vocab_config  field learn_subwords_size : int = 32000​  Number of subwords operations to learn.  Validated by: _maybe_set_huggingface_model_validate_build_vocab_config  field n_sample : int = 5000​  Number of transformed samples per corpus to use to build the vocabulary. Set to -1 to use the full corpora.  Validated by: _maybe_set_huggingface_model_validate_build_vocab_config  field num_threads : int = 1​  Number of parallel threads to build the vocabulary.  Validated by: _maybe_set_huggingface_model_validate_build_vocab_config  field vocab_sample_queue_size : int = 20​  Size of queues used for dumping samples.  Validated by: _maybe_set_huggingface_model_validate_build_vocab_config  model_post_init(context: Any, /)​  This function is meant to behave like a BaseModel method to initialise private attributes.  It takes context as an argument since that’s what pydantic-core passes when calling it.  Parameters: self – The BaseModel instance.context – The context. ","version":"Next","tagName":"h3"}],"options":{"languages":["en"],"id":"default"}}