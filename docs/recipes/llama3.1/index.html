<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-recipes/llama3.1/README" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.2.0">
<title data-rh="true">Llama3 | Eole - üë∑‚Äç‚ôÇÔ∏èüöß Work In Progress</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://eole-nlp.github.io/eole/docs/recipes/llama3.1/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Llama3 | Eole - üë∑‚Äç‚ôÇÔ∏èüöß Work In Progress"><meta data-rh="true" name="description" content="---"><meta data-rh="true" property="og:description" content="---"><link data-rh="true" rel="icon" href="/eole/img/eole-logo.ico"><link data-rh="true" rel="canonical" href="https://eole-nlp.github.io/eole/docs/recipes/llama3.1/"><link data-rh="true" rel="alternate" href="https://eole-nlp.github.io/eole/docs/recipes/llama3.1/" hreflang="en"><link data-rh="true" rel="alternate" href="https://eole-nlp.github.io/eole/docs/recipes/llama3.1/" hreflang="x-default"><link rel="stylesheet" href="/eole/assets/css/styles.0e100862.css">
<script src="/eole/assets/js/runtime~main.0169c973.js" defer="defer"></script>
<script src="/eole/assets/js/main.3a721193.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/eole/"><div class="navbar__logo"><img src="/eole/img/eole-logo.png" alt="Eole Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/eole/img/eole-logo.png" alt="Eole Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/eole/docs/">Docs</a><a class="navbar__item navbar__link" href="/eole/docs/reference/index">Reference</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/eole-nlp/eole" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/eole/docs/">EOLE</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/eole/docs/quickstart">Quickstart</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/eole/docs/category/key-concepts">Key Concepts</a><button aria-label="Expand sidebar category &#x27;Key Concepts&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/eole/docs/category/frequently-asked-questions">Frequently Asked Questions</a><button aria-label="Expand sidebar category &#x27;Frequently Asked Questions&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/eole/docs/category/recipes">Recipes</a><button aria-label="Collapse sidebar category &#x27;Recipes&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/NewsPalm-synthetic/">Example of using Hugging Face streaming dataset</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/">Recipes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/cometkiwi/">CometKiwi</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/eurollm/">Serving Eurollm in a gradio interface with Eole</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/fineweb10B/">fineweb10B</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/gpt2/">OpenAI GPT2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/llama2/">Llama2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/llama3/">Llama3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/eole/docs/recipes/llama3.1/">Llama3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/mistral/">Mistral</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/mixtral/">Mixtral</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/mmlu/">Evaluation with MMLU dataset</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/model-validator/">Model Validator</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/nllb/">NLLB</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/pixtral/">Pixtral</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/server/">Serving models with Eole</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/wiki_103/">Language Model Wiki-103</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/wmt17/">Translation WMT17 en-de</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/">TowerInstruct (Mistral)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/">TowerInstruct-(Llama2)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/eole/docs/recipes/wmt22_with_llama3.1/">Llama3</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/eole/docs/contributing">Contributors</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/eole/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/eole/docs/category/recipes"><span itemprop="name">Recipes</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Llama3</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Llama3</h1>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama31"><strong>NOTE</strong>
To make your life easier, run these commands from the recipe directory (here <code>recipes/llama3.1</code>).<a href="#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama31" class="hash-link" aria-label="Direct link to noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama31" title="Direct link to noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama31">‚Äã</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="retrieve-and-convert-model">Retrieve and convert model<a href="#retrieve-and-convert-model" class="hash-link" aria-label="Direct link to Retrieve and convert model" title="Direct link to Retrieve and convert model">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="set-environment-variables">Set environment variables<a href="#set-environment-variables" class="hash-link" aria-label="Direct link to Set environment variables" title="Direct link to Set environment variables">‚Äã</a></h3>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">export EOLE_MODEL_DIR=&lt;where_to_store_models&gt;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">export HF_TOKEN=&lt;your_hf_token&gt;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="download-and-convert-model">Download and convert model<a href="#download-and-convert-model" class="hash-link" aria-label="Direct link to Download and convert model" title="Direct link to Download and convert model">‚Äã</a></h3>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">eole convert HF --model_dir meta-llama/Meta-Llama-3.1-8B --output $EOLE_MODEL_DIR/llama3.1-8b --token $HF_TOKEN</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="inference">Inference<a href="#inference" class="hash-link" aria-label="Direct link to Inference" title="Direct link to Inference">‚Äã</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="write-test-prompt-to-text-file">Write test prompt to text file<a href="#write-test-prompt-to-text-file" class="hash-link" aria-label="Direct link to Write test prompt to text file" title="Direct link to Write test prompt to text file">‚Äã</a></h3>
<p>(Example prompt inspired from this HF PR: <a href="https://github.com/huggingface/transformers/pull/24653" target="_blank" rel="noopener noreferrer">https://github.com/huggingface/transformers/pull/24653</a>)</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">echo -e &quot;You are given this machine learning research paper, please read it carefully and answer the follow up question.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">=== BEGIN ===</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2306.15595v2 [cs.CL] 28 Jun 2023</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">arXiv</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">EXTENDING CONTEXT WINDOW OF LARGE LAN-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">GUAGE MODELS VIA POSITION INTERPOLATION</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Meta Platforms Inc.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">{chenshouyuan, shermanwong, cli, yuandong}@meta . com</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">1 INTRODUCTION</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Large language models (LLMs) typically come with a pre-defined context window size. For exam-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">context window limit is frequently exceeded in applications such as conducting long conversations,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">summarizing long documents, or executing long-term planning. For these applications, LLMs with</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">longer context windows are preferred. However, training an LLM from scratch with long context</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">windows requires significant investments. This naturally leads to a question: Can we extend the</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">context window of an existing pre-trained LLM?</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">text window. However, empirically, we found that models trained this way adapt to long context</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">windows very slowly. After training for more than 10000 batches, the effective context window</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">inefficient for extending to substantially longer context windows.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">of these techniques for extending the context window sizes of such LLMs remains limited.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">In this work, we introduce Position Interpolation to enable context window extensions for certain</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">down-scale the position indices so that the maximum position index matches the previous context</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">modate more input tokens, we interpolate the position encodings at neighboring integer positions,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">utilizing the fact that position encodings can be applied on non-integer positions, as opposed to</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">extrapolating outside the trained positions, which may lead to catastrophic values. We verify our</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">approach theoretically, by showing that the interpolated attention score has a much smaller upper</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">stable. Therefore, interpolated position encodings are easier for the model to adapt.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">very short period of fine-tuning for the model to fully adapt to greatly extended context windows.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We present experimental results for extending the context window to up to 32768 from the initial</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">The cost of fine-tuning is negligible compared to the pre-training costs. This confirms</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">our hypothesis that it is relatively easy for the models to adapt to interpolated position</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">encodings.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2. Position Interpolation generates strong models that can effectively make use of much ex-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tended context window. We show that models extended by Position Interpolation enjoy</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">significant perplexity gains from greatly extended context windows for text modeling, and</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">we show that the perplexity reduces graceful with the enlargement of context windows.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We also applied Position Interpolation in a long text summarization task, and demonstrate</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">competitive performances.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">3. Position Interpolation preserves model quality relatively well for tasks within its original</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">context window sizes. We present a variety of evaluation results for the extended LLaMA</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">models on the original LLaMA benchmark. Compared with original LLaMA models, the</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">extended LLLaM A models saw a minor degradation on several standard benchmarks within</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">a 2048 token limit.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Our results highlight the innate ability of Transformer models to ‚Äúextrapolate to sequence lengths</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">longer than the ones encountered during training‚Äù as hypothesized in the seminal work of Vaswani</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">extrapolation of positional encodings and it can be largely mitigated by interpolating position en-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">codings instead.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">window from 2K to 8K. Recently, open source community picks it up in Reddit post ! and Github</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">also give theoretical explanations why interpolation achieves much more stable results than extrap-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">olation, by showing that the upper bound of interplated attention score is much lower than that of</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">extrapolated ones.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2 METHOD</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Transformer models require explicit positional information to be injected, typically in the form of</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">positional encodings, to represent the order of inputs. We consider Rotary Position Embedding</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2023). Given a position index m ‚Ç¨ [0, ¬¢) and an embedding vector x := [zg, 71,..., 241], Where</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">follows</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Using RoPE, the self-attention score</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">is only dependent on relative position m ‚Äî 7 through trigonometric functions. Here q and k are the</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">query and key vector for a specific attention head. At each layer, RoPE is applied on both query and</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">key embeddings for computing attention scores.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2.2 DIRECT EXTRAPOLATION</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">While the attention score in RoPE only depends on the relative positions, which is what we want,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">its extrapolation performance is not great . In particular, when directly extending to larger context</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., &gt; 10%),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">comparable to untrained models.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Ideally, we want to see the model trained on a context window of size L = 2048 to still work</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">reasonably well on longer context window, but may not have the capability to leverage information</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">that appears beyond L. For example, to answer a question located at 3000, the model trained on</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">located at location 2900.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">What is the reason behind? How could this happen if the attention score a,,,‚Äî,, decays as the relative</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">distance |m ‚Äî n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m ‚Äî nl, the bound</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¬¢;(s) := #93), and</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">think about Eqn. 2 as basis expansion as the following:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j ‚Äî tk2j+1)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">are complex coefficients depending on q and k (here the definition of h; is exactly the same as the</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">region. The underlying reason is that the trigonometric family {¬¢;} (with sufficiently large d) is</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">much larger in regions beyond.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">In Fig. 2, thanks to the smoothness of bases functions ¬¢; interpolation is much more stable and will</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s &gt; L,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">how about we define an attention score a{s) = a(Ls/L‚Äô) where L‚Äô is the longer context window?</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Formally, we replace RoPE f by {‚Äô defined as follows</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We call this transformation on the position encoding Position Interpolation. In this step, we reduce</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">position indices from [0, L&#x27;) to [0, L) to match the original range of indices before computing RoPE.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">reduced from I‚Äô to L. Since we align the ranges of position indices and relative distances before</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">and after extension, we mitigate the effect on attention score computation due to context window</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">following theorem, we show that the interpolated attention score is well-behaved:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">smaller than the extrapolation bound, and thus the interpolated attention score is much more stable</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">than extrapolated one.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Notably, our method of rescaling of position indices does not introduce extra weight, or modify</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">the model architecture in any way. This makes it attractive in practical applications, since most</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">infrastructure and optimization for the original model can be reused after the extension.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">with interpolated position encodings on the extended context window size using a pre-training cor-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">is not sensitive to the choice of examples. The reason may be that the model is only adapting to the</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">new context window during the fine-tuning phase, starting from a good initialization, as opposed to</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">acquiring new knowledge.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">magnitude of query/key products. If we enforce a regularization on || during LLM training, it is</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">lated a(s) when s &gt; L can be comparable to that within [0, L]. To our knowledge, we are not aware</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">of existing LLM pre-training techniques that leverage this regularization and will leave it for future</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">work.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">3 EXPERIMENTS</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We show Position Interpolation can effectively extend context window up to 32 times of the original</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">size, and such extension can be done with only several hundreds of training steps. We show the</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">resulting models are strong LLMs with fully effective long context windows. We demonstrate its</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">performance in a number of tasks including language modeling, passkey retrieval, and long doc-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ument summarization. We also present benchmark results of the extended models on the original</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LLaMA evaluation benchmarks.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">3.1 SETUP</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Position Interpoloation method. Except for rescaling the position indices for models extended with</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ways.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Training Procedure. We fine-tune all model variants using the next token prediction objective. We</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">use AdamW (Loshchilov &amp; Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">128 global batch size. We note that the main need of using more GPUs is memory limitation during</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">et al., 2022).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RedPajama dataset (Computer, 2023).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">3.2 LONG SEQUENCE LANGUAGE MODELING</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We evaluate the long sequence language modeling performance of our extended models and base-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">dataset (Azerbayev et al., 2022).</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">subsample of 128 documents with at least 32768 SentencePiece (Kudo &amp; Richardson, 2018) tokens</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">context window size by using a sliding window approach following Press et al. (2022) with stride</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">S = 256.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">From the results, we found that models extended with our method enjoy a significantly improved</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">perplexity from longer context window sizes. By increasing the context window size from 2048 to</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-0.3 reductions of perplexity by extending to the 8192 context window size.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">In general, we observed a consistent trend of our models achieving better perplexity with longer</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">context windows. This indicates our models can effectively make use of the longer context windows</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">indicates that our method may enable extension to even longer context windows.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">In contrast, we observed that models extended via the direct fine-tuning method has shown regres-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">This indicates that models extended this way have limited capability of making use of context win-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">dows longer than their pre-trained settings.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">of performance within original evaluation context window is expected since Position Interpolation</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">forces position encodings in original context window to reside in a much narrower region, which</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">may negatively affect the language model‚Äôs performance. We present more benchmark results on</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">the original context window size in Section 3.4.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">certain language modeling capability, as indicated by &lt; 20 perplexity for extending to 8192 context</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">window (in contrast, the direct extrapolation method leads to &gt; 10% perplexity). With fine-tuning,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">we observed that the perplexity improves quickly. At 200 steps the models surpassed the original</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">model‚Äôs perplexity on 2048 context window size, indicating the models gaining ability of effectively</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">see the models have improved steadily and achieve a significantly better perplexity.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We study the effective context window size, i.e. the maximum distance of a token can effectively</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">attend to during inference, of our models after extension. To measure this, we follow a synthetic</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">evaluation task of passkey retrieval proposed by Mohtashami &amp; Jaggi (2023). In this task, the models</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">the document.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Given a language model, we estimate the upper and lower bounds of effective context windows as</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">follows. Suppose the random passkey is k tokens away from the end of the input. When a model</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">persistently fails to retrieve the correct passkey value across several independent attempts, it suggests</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">that the effective context window size of the model is less than k. Conversely, if a model consistently</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">succeeds in retrieving the correct passkey value, we deduce that the effective context window size</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">of the model is at least k.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">direct fine-tuning. For each model, we use 32 different &amp;¬£ uniformly spaced in the targeted context</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">window L‚Äô and run the above tests for 10 times for each k, where each time a random passkey of 5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We can see that models extended via Position Interpolation all successfully attain their desired ex-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tension objectives in terms of effective context window sizes, indicating by the effective context</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">that are extended via direct fine-tuning only saw a minimal increase of the effective context win-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">indication of an acceleration in the increase of window size.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We evaluate the models extended by Position Interpolation on several standard benchmark tasks</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">within the original context window size of 2048. The evaluation results are listed in Table 5. From</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">the results, we saw that models extended to 8192 produce comparable results on the original bench-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">mark which is designed for a much smaller context window, with a degradation of up to 2% on</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">performances, which may be due to the limited number of fine-tuning steps used in our method.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">The regression on benchmark tasks is consistent with our observation on perplexity regression in</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Section 3.2.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">3.5 LONG DOCUMENT SUMMARIZATION</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">In this task, we evaluate our models‚Äô performance on the long document summarization task. In</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">for training and 972 documents for evaluation. Each document comes with a human generated</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">summary. We truncate all input documents to their first 15000 tokens.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We fine-tune the LL.aMA models extended with Position Interpolation with a context window of</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">16384. Note the rescaling of position indices are still required during this fine-tuning step. We first</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">format the raw document using the prompt template in Figure 4, and then concatenate the prompt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tune the model using the next token prediction task with the above setup for 10 epochs. The losses</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">from the input prompt proportion of training examples are excluded during our fine-tuning.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">summarization of each document in the test set. The final output is truncated at 1000 tokens. We</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">the models‚Äô outputs vs the ground-truth summaries.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">In Table 6 we report our evaluation results. We have also included results from two baselines in</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">result suggests our models with 16384 context window can effectively handle the long document</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">summarization task.</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">=== END OF FILE ===</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Question: What is the paper about?</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Answer: &quot; | sed &#x27;:a;N;$!ba;s/\n/ÔΩünewlineÔΩ†/g&#x27; &gt; test_prompt.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="run-inference">Run inference<a href="#run-inference" class="hash-link" aria-label="Direct link to Run inference" title="Direct link to Run inference">‚Äã</a></h3>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">eole predict -c llama-inference.yaml -src test_prompt.txt -output test_output.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/eole-nlp/eole/tree/main/docs/docs/recipes/llama3.1/README.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/eole/docs/recipes/llama3/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Llama3</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/eole/docs/recipes/mistral/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Mistral</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama31" class="table-of-contents__link toc-highlight"><strong>NOTE</strong>
To make your life easier, run these commands from the recipe directory (here <code>recipes/llama3.1</code>).</a></li><li><a href="#retrieve-and-convert-model" class="table-of-contents__link toc-highlight">Retrieve and convert model</a><ul><li><a href="#set-environment-variables" class="table-of-contents__link toc-highlight">Set environment variables</a></li><li><a href="#download-and-convert-model" class="table-of-contents__link toc-highlight">Download and convert model</a></li></ul></li><li><a href="#inference" class="table-of-contents__link toc-highlight">Inference</a><ul><li><a href="#write-test-prompt-to-text-file" class="table-of-contents__link toc-highlight">Write test prompt to text file</a></li><li><a href="#run-inference" class="table-of-contents__link toc-highlight">Run inference</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/eole/docs/">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/eole-nlp/eole/discussions" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/eole-nlp/eole" target="_blank" rel="noopener noreferrer" class="footer__link-item">Source<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">EOLE is an open-source toolkit and is licensed under the MIT license.</div></div></div></footer></div>
</body>
</html>