"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[969],{4667:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>c,toc:()=>a});var i=t(4848),o=t(8453);const r={},s="CometKiwi",c={id:"recipes/cometkiwi/README",title:"CometKiwi",description:"---",source:"@site/docs/recipes/cometkiwi/README.md",sourceDirName:"recipes/cometkiwi",slug:"/recipes/cometkiwi/",permalink:"/eole/docs/recipes/cometkiwi/",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/recipes/cometkiwi/README.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Recipes",permalink:"/eole/docs/recipes/"},next:{title:"Serving Eurollm in a gradio interface with Eole",permalink:"/eole/docs/recipes/eurollm/"}},l={},a=[{value:"To make your life easier, run these commands from the recipe directory (here <code>recipes/cometkiwi</code>).",id:"to-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipescometkiwi",level:2},{value:"Retrieve and convert model",id:"retrieve-and-convert-model",level:2},{value:"Set environment variables",id:"set-environment-variables",level:3},{value:"Download and convert the base model",id:"download-and-convert-the-base-model",level:3},{value:"Training cometkiwi",id:"training-cometkiwi",level:2},{value:"Cometkiwi Inference",id:"cometkiwi-inference",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"cometkiwi",children:"CometKiwi"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,i.jsx)(n.p,{children:"This is NOT the exact replication of the Unbabel Comet Kiwi."}),"\n",(0,i.jsx)(n.p,{children:"What is common:\nWe use the same base model xlm-roberta-xl or xlm-roberta-xxl encoders\nWe use the same dataset (1720-da.mlqe)"}),"\n",(0,i.jsxs)(n.p,{children:["You can get the dataset to train the model below here: ",(0,i.jsx)(n.a,{href:"https://huggingface.co/eole-nlp/cometkiwi-xxl-eole/",children:"https://huggingface.co/eole-nlp/cometkiwi-xxl-eole/"})]}),"\n",(0,i.jsx)(n.p,{children:"What is different:\nwmt23-cometkiwi-da-xl(or xxl) use Layerwise attention which brings complexity without significant better accuracy\nwe use Gelu instead of Tanh in the Estimator"}),"\n",(0,i.jsx)(n.p,{children:"Our scores for XL and XXL are in the same range when they are very different for Unbabel/wmt23-cometkiwi-XL or XXL"}),"\n",(0,i.jsxs)(n.h2,{id:"to-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipescometkiwi",children:["To make your life easier, run these commands from the recipe directory (here ",(0,i.jsx)(n.code,{children:"recipes/cometkiwi"}),")."]}),"\n",(0,i.jsx)(n.h2,{id:"retrieve-and-convert-model",children:"Retrieve and convert model"}),"\n",(0,i.jsx)(n.h3,{id:"set-environment-variables",children:"Set environment variables"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"export EOLE_MODEL_DIR=<where_to_store_models>\n"})}),"\n",(0,i.jsx)(n.h3,{id:"download-and-convert-the-base-model",children:"Download and convert the base model"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"eole convert HF --model_dir facebook/xlm-roberta-xxl --output $EOLE_MODEL_DIR/xlm-roberta-xxl-eole\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"NOTE"}),"\nThe facebook original model is stored in FP32 but we convert it to FP16 at conversion."]}),"\n",(0,i.jsx)(n.p,{children:"XXL is a 10.7G params model hence will save a 21.4GB file on disk (safetensors format)\nXL is a 3.5GB params model hence will save a 7.0GB file on disk"}),"\n",(0,i.jsxs)(n.p,{children:["After conversion in ",(0,i.jsx)(n.code,{children:"$EOLE_MODEL_DIR"})," you will get the following files:\nconfig.json\nmodel.00.safetensors\nsentencepiece.bpe.model\nvocab.json\nvocab.txt"]}),"\n",(0,i.jsxs)(n.p,{children:["The vocab.txt file contains 250000 entries (from sentencepiece) but the model was trained with extra tokens\nYou need to concat the ",(0,i.jsx)(n.code,{children:"added_vocab.txt"})," file to the ",(0,i.jsx)(n.code,{children:"vocab.txt"})," file resulting in 250880 tokens"]}),"\n",(0,i.jsx)(n.h2,{id:"training-cometkiwi",children:"Training cometkiwi"}),"\n",(0,i.jsxs)(n.p,{children:["Training will happen in two steps (see cometkiwi-xxl-eole.yaml file)\nFYI the trained model can be downloaded here: ",(0,i.jsx)(n.a,{href:"https://huggingface.co/eole-nlp/cometkiwi-xxl-eole/",children:"https://huggingface.co/eole-nlp/cometkiwi-xxl-eole/"})]}),"\n",(0,i.jsxs)(n.p,{children:["STEP 1:\nWe train from the converted xlm-roberta-xxl model but we keep everything frozen.\nWe just add an Estimator layer that will be trained during 4000 steps\nTo make sure we do not lose anything we will rename the pre-trained subfolder into ",(0,i.jsx)(n.code,{children:"step_4000_pretrain"}),"\nIn this step we do NOT use LoRA but we use 4bit quant to make things easier and fit in a smaller cards"]}),"\n",(0,i.jsx)(n.p,{children:"STEP 2:\nWe release the encoder to make weights trainable hence we need to use LoRA since the model is big and gradients would not fit in consumer grade cards.\nWe train during 40000 steps"}),"\n",(0,i.jsxs)(n.p,{children:["For the two steps above, the training command is the same but make sure the yaml file is modified according to the instructions\n",(0,i.jsx)(n.code,{children:"eole train --config cometkiwi-xxl-eole.yaml"})]}),"\n",(0,i.jsx)(n.p,{children:"After this step we need to merge the LoRA weights into the original model with the following command:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:'eole model lora --base_model "./cometkiwi-xxl-eole/step_4000_pretrain/" --lora_weights "./cometkiwi-xxl-eole/" --output "./cometkiwi-xxl-eole/merged"'})}),"\n",(0,i.jsx)(n.h2,{id:"cometkiwi-inference",children:"Cometkiwi Inference"}),"\n",(0,i.jsx)(n.p,{children:"Format the source / target file you want to score in the Comet format:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"./combine.sh newstest2014.en newstest2014.de newstest2014.comb"})}),"\n",(0,i.jsx)(n.p,{children:"Score the .comb file:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"eole predict --config cometkiwi-xxl-inference.yaml --src newstest2014.comb --output newstest2014.scores --with_score"})}),"\n",(0,i.jsxs)(n.p,{children:["For now the scores are in the third column, so you can ",(0,i.jsx)(n.code,{children:"cut -f3"})," the output file"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>c});var i=t(6540);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);