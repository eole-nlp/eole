"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[5742],{4838:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>l,toc:()=>c});var s=i(4848),r=i(8453);const t={},o="EOLE",l={id:"index",title:"EOLE",description:"Documentation",source:"@site/docs/index.md",sourceDirName:".",slug:"/",permalink:"/eole/docs/",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/index.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",next:{title:"Quickstart",permalink:"/eole/docs/quickstart"}},a={},c=[{value:"Latest developments",id:"latest-developments",level:2},{value:"Work completed",id:"work-completed",level:2},{value:"Future Directions",id:"future-directions",level:3},{value:"Key Features",id:"key-features",level:2},{value:"Setup",id:"setup",level:2},{value:"Using Docker",id:"using-docker",level:3},{value:"Installing Locally",id:"installing-locally",level:3},{value:"Requirements",id:"requirements",level:4},{value:"Installation from Source",id:"installation-from-source",level:4},{value:"Installation from PyPI",id:"installation-from-pypi",level:4},{value:"Notes",id:"notes",level:4},{value:"Manual Installation of Some Dependencies",id:"manual-installation-of-some-dependencies",level:3},{value:"Flash Attention",id:"flash-attention",level:4},{value:"AWQ",id:"awq",level:4},{value:"Notes on Mixed-precision or Low precision Training",id:"notes-on-mixed-precision-or-low-precision-training",level:2},{value:"Contributing",id:"contributing",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"eole",children:"EOLE"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://eole-nlp.github.io/eole",children:(0,s.jsx)(n.img,{src:"https://img.shields.io/badge/docs-latest-blue.svg",alt:"Documentation"})})}),"\n",(0,s.jsxs)(n.p,{children:["Open language modeling toolkit based on ",(0,s.jsx)(n.a,{href:"https://pytorch.org",children:"PyTorch"})," initially spun-off of OpenNMT-py"]}),"\n",(0,s.jsx)(n.p,{children:"We aim to maintain the research-friendly approach of the original project while including latest architectures (LLMs) and various other techniques.\nOur goal is to provide a comprehensive yet compact and modular codebase for experimenting with various types of language models (encoder, decoder, seq2seq)."}),"\n",(0,s.jsx)(n.h2,{id:"latest-developments",children:"Latest developments"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mistral-3.1-24B-instruct"})," support (text and image input)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pure-BF16 Training"})," thanks to ",(0,s.jsx)(n.a,{href:"https://arxiv.org/pdf/2010.06192",children:"Kahan Summation"})," implemented ",(0,s.jsx)(n.a,{href:"https://optimi.benjaminwarner.dev/kahan_summation/",children:"here"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Web-based (Google translator-like) interface"})," featuring the latest EuroLLM-8B-Instruct LLM: read more ",(0,s.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/tree/main/recipes/eurollm",children:"here"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Estimator layer"})," which enables to rescore multiple beams in the same model. Read article ",(0,s.jsx)(n.a,{href:"https://medium.com/p/05b00b271a47",children:"here"})," and ",(0,s.jsx)(n.a,{href:"https://medium.com/p/7dccfe167814",children:"here"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Support Hugging Face Tokenizers"})," for better compatiblity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"New recipes"})," for TowerInstruct-llama2 and TowerInstruct-Mistral"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Support latest models"})," for Llama3.x, Gemma2, Pixtral"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Replicate CometKiwi(XL/XXL)"})," Encoder+Estimator models"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"work-completed",children:"Work completed"}),"\n",(0,s.jsx)(n.p,{children:"We have made significant progress in several areas:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration Management"}),": Streamlined through ",(0,s.jsx)(n.a,{href:"https://docs.pydantic.dev",children:"pydantic"})," models."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Command Line Entry Points"}),": Improved using structured subparsers for better organization."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reproducible Recipes"}),": Provided for widely used models and tasks, ensuring consistency and reliability."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Core API Simplification"}),": Refined around the new configuration objects for ease of use."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Revamped Fast API based server"}),": see above example with EuroLLM-9B-Instruct"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"future-directions",children:"Future Directions"}),"\n",(0,s.jsx)(n.p,{children:"There are still several exciting avenues to explore:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Further Simplification and Refactoring"}),": Continue enhancing the codebase for clarity and efficiency."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Documentation"}),": Enhance and expand the documentation for better user guidance."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Test Coverage"}),": Improve testing to ensure code reliability and performance."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Logging Enhancements"}),": Implement more sophisticated logging mechanisms."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Broader Model Support"}),": Extend support to include a wider range of open models, potentially multi-modal."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"key-features",children:"Key Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Versatile Training and Inference"}),": Train from scratch, finetune, and infer models of various architectures including Transformer Encoder/Decoder/EncoderDecoder and RNN EncoderDecoder."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamic Data Transforms"}),": Apply on-the-fly transformations in the dataloading logic for both training and inference."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Comprehensive LLM Support"}),": Includes converters for Llama, Mistral, Phi, Gemma ..."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Advanced Quantization"}),": Support for 8-bit and 4-bit quantization, along with LoRA adapters, with or without checkpointing, as well as mixed precision (FP16)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficient Finetuning"}),": Finetune 7B and 13B models on a single RTX 24GB GPU using 4-bit quantization."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flexible Inference"}),": Perform inference in 4-bit or 8-bit using the same layer quantization methods as in finetuning."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tensor Parallelism"}),": Enable tensor parallelism for both training and inference when models exceed the memory capacity of a single GPU."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,s.jsx)(n.h3,{id:"using-docker",children:"Using Docker"}),"\n",(0,s.jsxs)(n.p,{children:["To facilitate setup and reproducibility, we provide Docker images via the GitHub Container Registry: ",(0,s.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/pkgs/container/eole",children:"EOLE Docker Images"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["You can customize the workflow and build your own images based on specific needs using ",(0,s.jsx)(n.code,{children:"build.sh"})," and ",(0,s.jsx)(n.code,{children:"Dockerfile"})," in the ",(0,s.jsx)(n.code,{children:"docker"})," directory of the repository."]}),"\n",(0,s.jsx)(n.p,{children:"To pull the Docker image:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker pull ghcr.io/eole-nlp/eole:0.2.0-torch2.6.0-ubuntu22.04-cuda12.6\n"})}),"\n",(0,s.jsx)(n.p,{children:"Example one-liner to run a container and open a bash shell within it:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker run --rm -it --runtime=nvidia ghcr.io/eole-nlp/eole:0.2.0-torch2.6.0-ubuntu22.04-cuda12.6\n"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note"}),": Ensure you have the ",(0,s.jsx)(n.a,{href:"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html",children:"Nvidia Container Toolkit"})," (formerly nvidia-docker) installed to take advantage of CUDA/GPU features."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Depending on your needs, you can add various flags:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"-p 5000:5000"}),": Forward an exposed port from your container to your host."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"-v /some/local/directory:/some/container/directory"}),": Mount a local directory to a container directory."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"--entrypoint some_command"}),": Run a specific command as the container entry point (instead of the default bash shell)."]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"installing-locally",children:"Installing Locally"}),"\n",(0,s.jsx)(n.h4,{id:"requirements",children:"Requirements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Python >= 3.10"}),"\n",(0,s.jsx)(n.li,{children:"PyTorch >= 2.5 < 2.8"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"installation-from-source",children:"Installation from Source"}),"\n",(0,s.jsx)(n.p,{children:"To install from source:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/eole-nlp/eole\ncd eole\npip install -e .\n"})}),"\n",(0,s.jsx)(n.h4,{id:"installation-from-pypi",children:"Installation from PyPI"}),"\n",(0,s.jsx)(n.p,{children:"Installation from PyPI will be available soon."}),"\n",(0,s.jsx)(n.h4,{id:"notes",children:"Notes"}),"\n",(0,s.jsxs)(n.p,{children:["If you encounter a ",(0,s.jsx)(n.code,{children:"MemoryError"})," during installation, try using ",(0,s.jsx)(n.code,{children:"pip"})," with the ",(0,s.jsx)(n.code,{children:"--no-cache-dir"})," option."]}),"\n",(0,s.jsx)(n.p,{children:"(Optional) Some advanced features (e.g., pretrained models or specific transforms) require extra packages. Install them with:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install -r requirements.opt.txt\n"})}),"\n",(0,s.jsx)(n.h3,{id:"manual-installation-of-some-dependencies",children:"Manual Installation of Some Dependencies"}),"\n",(0,s.jsx)(n.h4,{id:"flash-attention",children:"Flash Attention"}),"\n",(0,s.jsxs)(n.p,{children:["To use ",(0,s.jsx)(n.a,{href:"https://github.com/Dao-AILab/flash-attention#installation-and-features",children:"Flash Attention"}),", install it manually:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install flash-attn --no-build-isolation\n"})}),"\n",(0,s.jsx)(n.h4,{id:"awq",children:"AWQ"}),"\n",(0,s.jsx)(n.p,{children:"For inference or quantizing an AWQ model, AutoAWQ is required. Install it with:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install autoawq\n"})}),"\n",(0,s.jsxs)(n.p,{children:["For more details, refer to ",(0,s.jsx)(n.a,{href:"https://github.com/casper-hansen/AutoAWQ",children:"AutoAWQ"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"notes-on-mixed-precision-or-low-precision-training",children:"Notes on Mixed-precision or Low precision Training"}),"\n",(0,s.jsx)(n.p,{children:'Until Feb 25, we used torch optimizers with or without AMP (mixed precision) or "fusedadam" which was an old implementation of Apex/Nvidia using FP16 with dynamic loss scaling and without FP32 master weights.\nAs of 0.2 "fusedadam" is deprecated and we implemented pure-BF16 training.'}),"\n",(0,s.jsx)(n.p,{children:"As a result, config flags are now:"}),"\n",(0,s.jsx)(n.p,{children:"For FP16-amp or BF16-amp training (using pytorch optimizers and amp implementation)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"compute_dtype: fp16 or bf16\nuse_amp: true\noptim: adam or adamw\n"})}),"\n",(0,s.jsx)(n.p,{children:"Special note: even though it may not be logical, we still use the torch GradScaler in BF16-AMP. Even if the BF16 range is similar to FP32, scaling prevents from underflowing.\nWe tested BF16-AMP without the GradScaler and it does not give good results."}),"\n",(0,s.jsx)(n.p,{children:"For pure-bf16 training (using torch-optimi and kahan summation)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"compute_dtype: bf16\nuse_amp: true\noptim: adam or adamw\n"})}),"\n",(0,s.jsx)(n.p,{children:"Pure-BF16 training is faster than AMP and the memory footprint is reduced (master weights are kept in BF16 vs FP32). However Kahan Summation is not magical, results are good but not as good as AMP.\nUse this feature mainly when memory footprint is an issue with LLMs."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"contributing",children:"Contributing"}),"\n",(0,s.jsxs)(n.p,{children:["We love contributions! Please look at issues marked with the ",(0,s.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/issues?q=is%3Aissue+is%3Aopen+label%3A%22contributions+welcome%22",children:"contributions welcome"})," tag."]}),"\n",(0,s.jsxs)(n.p,{children:["Before raising an issue, make sure you read the requirements and the ",(0,s.jsx)(n.a,{href:"https://eole-nlp.github.io/eole",children:"Full Documentation"}),". You can also check if a ",(0,s.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/tree/main/recipes",children:"Recipe"})," fits your use case."]}),"\n",(0,s.jsxs)(n.p,{children:["Unless there is a bug, please use the ",(0,s.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/discussions",children:"Discussions"})," tab to ask questions or propose new topics/features."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);