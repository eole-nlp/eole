"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[9210],{5309:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>f,frontMatter:()=>o,metadata:()=>d,toc:()=>a});var t=i(4848),r=i(8453);const o={},l="Models",d={id:"reference/Config/models",title:"Models",description:"Base Configs",source:"@site/docs/reference/Config/models.md",sourceDirName:"reference/Config",slug:"/reference/Config/models",permalink:"/eole/docs/reference/Config/models",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/reference/Config/models.md",tags:[],version:"current",frontMatter:{},sidebar:"apiSidebar",previous:{title:"Inference",permalink:"/eole/docs/reference/Config/inference"},next:{title:"Main Entrypoints",permalink:"/eole/docs/reference/Config/run"}},s={},a=[{value:"Base Configs",id:"base-configs",level:2},{value:"<em>pydantic model</em> eole.config.models.BaseModelConfig",id:"pydantic-model-eoleconfigmodelsbasemodelconfig",level:3},{value:"<em>field</em> adapter_bias <em>: bool</em> <em>= False</em>",id:"field-adapter_bias--bool--false",level:4},{value:"<em>field</em> add_estimator <em>: bool</em> <em>= False</em>",id:"field-add_estimator--bool--false",level:4},{value:"<em>field</em> architecture <em>: str | None</em> <em>= None</em>",id:"field-architecture--str--none--none",level:4},{value:"<em>field</em> decoder <em>: TransformerDecoderConfig | RnnDecoderConfig | CnnDecoderConfig | None</em> <em>= None</em>",id:"field-decoder--transformerdecoderconfig--rnndecoderconfig--cnndecoderconfig--none--none",level:4},{value:"<em>field</em> embeddings <em>: EmbeddingsConfig</em> <em>[Optional]</em>",id:"field-embeddings--embeddingsconfig-optional",level:4},{value:"<em>field</em> encoder <em>: TransformerEncoderConfig | RnnEncoderConfig | CnnEncoderConfig | MeanEncoderConfig | VisionEncoderConfig | None</em> <em>= None</em>",id:"field-encoder--transformerencoderconfig--rnnencoderconfig--cnnencoderconfig--meanencoderconfig--visionencoderconfig--none--none",level:4},{value:"<em>field</em> eole_version <em>: str | None</em> <em>= &#39;0.4.0&#39;</em>",id:"field-eole_version--str--none--040",level:4},{value:"<em>field</em> estimator_type <em>: Literal[&#39;average&#39;, &#39;last_token&#39;, &#39;first_token&#39;]</em> <em>= &#39;average&#39;</em>",id:"field-estimator_type--literalaverage-last_token-first_token--average",level:4},{value:"<em>field</em> generator_bias <em>: bool</em> <em>= True</em>",id:"field-generator_bias--bool--true",level:4},{value:"<em>field</em> generator_function <em>: Literal[&#39;softmax&#39;, &#39;sparsemax&#39;]</em> <em>= &#39;softmax&#39;</em>",id:"field-generator_function--literalsoftmax-sparsemax--softmax",level:4},{value:"<em>field</em> hidden_size <em>: int</em> <em>= -1</em>",id:"field-hidden_size--int---1",level:4},{value:"<em>field</em> huggingface_model <em>: str | None</em> <em>= None</em>",id:"field-huggingface_model--str--none--none",level:4},{value:"<em>field</em> input_feed <em>: int</em> <em>= 1</em>",id:"field-input_feed--int--1",level:4},{value:"<em>field</em> layers <em>: int</em> <em>= -1</em>",id:"field-layers--int---1",level:4},{value:"<em>field</em> left_pad <em>: bool</em> <em>= False</em>",id:"field-left_pad--bool--false",level:4},{value:"<em>field</em> moe_transformer_ff <em>: int | None</em> <em>= None</em>",id:"field-moe_transformer_ff--int--none--none",level:4},{value:"<em>field</em> projector_activation_fn <em>: ActivationFunction | None</em> <em>= ActivationFunction.relu</em>",id:"field-projector_activation_fn--activationfunction--none--activationfunctionrelu",level:4},{value:"<em>field</em> share_decoder_embeddings <em>: bool</em> <em>= False</em>",id:"field-share_decoder_embeddings--bool--false",level:4},{value:"<em>field</em> share_embeddings <em>: bool</em> <em>= False</em>",id:"field-share_embeddings--bool--false",level:4},{value:"<em>field</em> spatial_merge_size <em>: int | None</em> <em>= 1</em>",id:"field-spatial_merge_size--int--none--1",level:4},{value:"<em>field</em> transformer_ff <em>: int</em> <em>= -1</em>",id:"field-transformer_ff--int---1",level:4},{value:"<em>field</em> word_vec_size <em>: int</em> <em>= -1</em>",id:"field-word_vec_size--int---1",level:4},{value:"<em>validator</em> default_architecture  <em>\xbb</em>  <em>all fields</em>",id:"validator-default_architecture----all-fields",level:4},{value:"<em>validator</em> str_to_dict  <em>\xbb</em>  <em>encoder</em> <em>,</em> <em>decoder</em> <em>,</em> <em>embeddings</em>",id:"validator-str_to_dict----encoder--decoder--embeddings",level:4},{value:"update_model_opts()",id:"update_model_opts",level:4},{value:"<em>validator</em> validate_merge_size  <em>\xbb</em>  <em>spatial_merge_size</em>",id:"validator-validate_merge_size----spatial_merge_size",level:4},{value:"<em>property</em> model_type <em>: ModelType</em>",id:"property-model_type--modeltype",level:4},{value:"<em>pydantic model</em> eole.config.models.EmbeddingsConfig",id:"pydantic-model-eoleconfigmodelsembeddingsconfig",level:3},{value:"<em>field</em> freeze_word_vecs_dec <em>: bool</em> <em>= False</em>",id:"field-freeze_word_vecs_dec--bool--false",level:4},{value:"<em>field</em> freeze_word_vecs_enc <em>: bool</em> <em>= False</em>",id:"field-freeze_word_vecs_enc--bool--false",level:4},{value:"<em>field</em> n_positions <em>: int | None</em> <em>= None</em>",id:"field-n_positions--int--none--none",level:4},{value:"<em>field</em> normalize <em>: bool | None</em> <em>= False</em>",id:"field-normalize--bool--none--false",level:4},{value:"<em>field</em> position_encoding <em>: bool</em> <em>= False</em>",id:"field-position_encoding--bool--false",level:4},{value:"<em>field</em> position_encoding_type <em>: PositionEncodingType | None</em> <em>= PositionEncodingType.SinusoidalInterleaved</em>",id:"field-position_encoding_type--positionencodingtype--none--positionencodingtypesinusoidalinterleaved",level:4},{value:"<em>field</em> position_shift <em>: int | None</em> <em>= 0</em>",id:"field-position_shift--int--none--0",level:4},{value:"<em>field</em> src_word_vec_size <em>: int</em> <em>= 512</em>",id:"field-src_word_vec_size--int--512",level:4},{value:"<em>field</em> tgt_word_vec_size <em>: int</em> <em>= 512</em>",id:"field-tgt_word_vec_size--int--512",level:4},{value:"<em>field</em> word_vec_size <em>: int</em> <em>= -1</em>",id:"field-word_vec_size--int---1-1",level:4},{value:"<em>validator</em> validate_embeddings  <em>\xbb</em>  <em>all fields</em>",id:"validator-validate_embeddings----all-fields",level:4},{value:"<em>pydantic model</em> eole.config.models.EncoderConfig",id:"pydantic-model-eoleconfigmodelsencoderconfig",level:3},{value:"<em>field</em> encoder_type <em>: str | None</em> <em>= &#39;rnn&#39;</em>",id:"field-encoder_type--str--none--rnn",level:4},{value:"<em>field</em> hidden_size <em>: int</em> <em>= 512</em>",id:"field-hidden_size--int--512",level:4},{value:"<em>field</em> layers <em>: int</em> <em>= 2</em>",id:"field-layers--int--2",level:4},{value:"<em>field</em> src_word_vec_size <em>: int</em> <em>= 512</em>",id:"field-src_word_vec_size--int--512-1",level:4},{value:"<em>pydantic model</em> eole.config.models.DecoderConfig",id:"pydantic-model-eoleconfigmodelsdecoderconfig",level:3},{value:"<em>field</em> coverage_attn <em>: bool</em> <em>= False</em>",id:"field-coverage_attn--bool--false",level:4},{value:"<em>field</em> decoder_type <em>: str | None</em> <em>= &#39;rnn&#39;</em>",id:"field-decoder_type--str--none--rnn",level:4},{value:"<em>field</em> global_attention <em>: Literal[&#39;dot&#39;, &#39;general&#39;, &#39;mlp&#39;, None]</em> <em>= &#39;general&#39;</em>",id:"field-global_attention--literaldot-general-mlp-none--general",level:4},{value:"<em>field</em> global_attention_function <em>: Literal[&#39;softmax&#39;, &#39;sparsemax&#39;]</em> <em>= &#39;softmax&#39;</em>",id:"field-global_attention_function--literalsoftmax-sparsemax--softmax",level:4},{value:"<em>field</em> hidden_size <em>: int</em> <em>= 512</em>",id:"field-hidden_size--int--512-1",level:4},{value:"<em>field</em> lambda_coverage <em>: float</em> <em>= 0.0</em>",id:"field-lambda_coverage--float--00",level:4},{value:"<em>field</em> layers <em>: int</em> <em>= 2</em>",id:"field-layers--int--2-1",level:4},{value:"<em>field</em> tgt_word_vec_size <em>: int</em> <em>= 512</em>",id:"field-tgt_word_vec_size--int--512-1",level:4},{value:"<em>pydantic model</em> eole.config.models.CustomModelConfig",id:"pydantic-model-eoleconfigmodelscustommodelconfig",level:3},{value:"<em>field</em> architecture <em>: Literal[&#39;custom&#39;]</em> <em>= &#39;custom&#39;</em>",id:"field-architecture--literalcustom--custom",level:4},{value:"Transformer",id:"transformer",level:2},{value:"<em>pydantic model</em> eole.config.models.TransformerConfig",id:"pydantic-model-eoleconfigmodelstransformerconfig",level:3},{value:"<em>field</em> add_ffnbias <em>: bool</em> <em>= False</em>",id:"field-add_ffnbias--bool--false",level:4},{value:"<em>field</em> add_final_linear_bias <em>: bool</em> <em>= False</em>",id:"field-add_final_linear_bias--bool--false",level:4},{value:"<em>field</em> add_qkvbias <em>: bool</em> <em>= False</em>",id:"field-add_qkvbias--bool--false",level:4},{value:"<em>field</em> attn_scaling <em>: float | None</em> <em>= None</em>",id:"field-attn_scaling--float--none--none",level:4},{value:"<em>field</em> ffn_layernorm <em>: bool</em> <em>= False</em>",id:"field-ffn_layernorm--bool--false",level:4},{value:"<em>field</em> first_k_dense_replace <em>: int</em> <em>= 0</em>",id:"field-first_k_dense_replace--int--0",level:4},{value:"<em>field</em> head_dim <em>: int | None</em> <em>= None</em>",id:"field-head_dim--int--none--none",level:4},{value:"<em>field</em> heads <em>: int</em> <em>= 8</em>",id:"field-heads--int--8",level:4},{value:"<em>field</em> heads_kv <em>: int | None</em> <em>= None</em>",id:"field-heads_kv--int--none--none",level:4},{value:"<em>field</em> key_norm <em>: bool</em> <em>= False</em>",id:"field-key_norm--bool--false",level:4},{value:"<em>field</em> layer_norm <em>: Literal[&#39;standard&#39;, &#39;standardFP32&#39;, &#39;rms&#39;, &#39;gemma-rms&#39;]</em> <em>= &#39;standard&#39;</em>",id:"field-layer_norm--literalstandard-standardfp32-rms-gemma-rms--standard",level:4},{value:"<em>field</em> mlp_activation_fn <em>: ActivationFunction</em> <em>= ActivationFunction.relu</em>",id:"field-mlp_activation_fn--activationfunction--activationfunctionrelu",level:4},{value:"<em>field</em> moe_softmax_after <em>: bool</em> <em>= False</em>",id:"field-moe_softmax_after--bool--false",level:4},{value:"<em>field</em> moe_transformer_ff <em>: int | None</em> <em>= None</em>",id:"field-moe_transformer_ff--int--none--none-1",level:4},{value:"<em>field</em> n_positions <em>: int | None</em> <em>= None</em>",id:"field-n_positions--int--none--none-1",level:4},{value:"<em>field</em> norm_eps <em>: float</em> <em>= 1e-05</em>",id:"field-norm_eps--float--1e-05",level:4},{value:"<em>field</em> num_experts <em>: int</em> <em>= 0</em>",id:"field-num_experts--int--0",level:4},{value:"<em>field</em> num_experts_per_tok <em>: int</em> <em>= 2</em>",id:"field-num_experts_per_tok--int--2",level:4},{value:"<em>field</em> num_shared_experts <em>: int</em> <em>= 0</em>",id:"field-num_shared_experts--int--0",level:4},{value:"<em>field</em> parallel_residual <em>: bool</em> <em>= False</em>",id:"field-parallel_residual--bool--false",level:4},{value:"<em>field</em> position_encoding_type <em>: PositionEncodingType | None</em> <em>= PositionEncodingType.SinusoidalInterleaved</em>",id:"field-position_encoding_type--positionencodingtype--none--positionencodingtypesinusoidalinterleaved-1",level:4},{value:"<em>field</em> qk_norm_post_rope <em>: bool</em> <em>= False</em>",id:"field-qk_norm_post_rope--bool--false",level:4},{value:"<em>field</em> query_norm <em>: bool</em> <em>= False</em>",id:"field-query_norm--bool--false",level:4},{value:"<em>field</em> relative_positions_buckets <em>: int</em> <em>= 0</em>",id:"field-relative_positions_buckets--int--0",level:4},{value:"<em>field</em> rope_config <em>: RotaryPositionConfig | None</em> <em>= None</em>",id:"field-rope_config--rotarypositionconfig--none--none",level:4},{value:"<em>field</em> shared_layer_norm <em>: bool</em> <em>= False</em>",id:"field-shared_layer_norm--bool--false",level:4},{value:"<em>field</em> sliding_window <em>: int</em> <em>= 0</em>",id:"field-sliding_window--int--0",level:4},{value:"<em>field</em> transformer_ff <em>: int</em> <em>= 2048</em>",id:"field-transformer_ff--int--2048",level:4},{value:"<em>property</em> dim_per_head <em>: int</em>",id:"property-dim_per_head--int",level:4},{value:"<em>pydantic model</em> eole.config.models.TransformerEncoderConfig",id:"pydantic-model-eoleconfigmodelstransformerencoderconfig",level:3},{value:"<em>field</em> encoder_type <em>: Literal[&#39;transformer&#39;]</em> <em>= &#39;transformer&#39;</em>",id:"field-encoder_type--literaltransformer--transformer",level:4},{value:"<em>pydantic model</em> eole.config.models.TransformerDecoderConfig",id:"pydantic-model-eoleconfigmodelstransformerdecoderconfig",level:3},{value:"<em>field</em> LM_type <em>: Literal[&#39;causal&#39;, &#39;prefix&#39;]</em> <em>= &#39;causal&#39;</em>",id:"field-lm_type--literalcausal-prefix--causal",level:4},{value:"<em>field</em> aan_useffn <em>: bool</em> <em>= False</em>",id:"field-aan_useffn--bool--false",level:4},{value:"<em>field</em> alignment_heads <em>: int</em> <em>= 0</em>",id:"field-alignment_heads--int--0",level:4},{value:"<em>field</em> alignment_layer <em>: int</em> <em>= -2</em>",id:"field-alignment_layer--int---2",level:4},{value:"<em>field</em> decoder_type <em>: Literal[&#39;transformer&#39;]</em> <em>= &#39;transformer&#39;</em>",id:"field-decoder_type--literaltransformer--transformer",level:4},{value:"<em>field</em> full_context_alignment <em>: bool</em> <em>= False</em>",id:"field-full_context_alignment--bool--false",level:4},{value:"<em>field</em> lambda_align <em>: float</em> <em>= 0.0</em>",id:"field-lambda_align--float--00",level:4},{value:"<em>pydantic model</em> eole.config.models.TransformerModelConfig",id:"pydantic-model-eoleconfigmodelstransformermodelconfig",level:3},{value:"<em>field</em> architecture <em>: Literal[&#39;transformer&#39;]</em> <em>= &#39;transformer&#39;</em>",id:"field-architecture--literaltransformer--transformer",level:4},{value:"<em>validator</em> default_architecture  <em>\xbb</em>  <em>all fields</em>",id:"validator-default_architecture----all-fields-1",level:4},{value:"<em>validator</em> encoder_decoder_type  <em>\xbb</em>  <em>all fields</em>",id:"validator-encoder_decoder_type----all-fields",level:4},{value:"<em>pydantic model</em> eole.config.models.TransformerLMModelConfig",id:"pydantic-model-eoleconfigmodelstransformerlmmodelconfig",level:3},{value:"<em>field</em> architecture <em>: Literal[&#39;transformer_lm&#39;]</em> <em>= &#39;transformer_lm&#39;</em>",id:"field-architecture--literaltransformer_lm--transformer_lm",level:4},{value:"<em>field</em> encoder <em>: None</em> <em>= None</em>",id:"field-encoder--none--none",level:4},{value:"<em>validator</em> default_architecture  <em>\xbb</em>  <em>all fields</em>",id:"validator-default_architecture----all-fields-2",level:4},{value:"<em>validator</em> encoder_decoder_type  <em>\xbb</em>  <em>all fields</em>",id:"validator-encoder_decoder_type----all-fields-1",level:4},{value:"RNN",id:"rnn",level:2},{value:"<em>pydantic model</em> eole.config.models.RnnConfig",id:"pydantic-model-eoleconfigmodelsrnnconfig",level:3},{value:"<em>field</em> bridge <em>: bool</em> <em>= False</em>",id:"field-bridge--bool--false",level:4},{value:"<em>field</em> rnn_type <em>: Literal[&#39;LSTM&#39;, &#39;GRU&#39;]</em> <em>= &#39;LSTM&#39;</em>",id:"field-rnn_type--literallstm-gru--lstm",level:4},{value:"<em>pydantic model</em> eole.config.models.RnnEncoderConfig",id:"pydantic-model-eoleconfigmodelsrnnencoderconfig",level:3},{value:"<em>field</em> encoder_type <em>: Literal[&#39;rnn&#39;, &#39;brnn&#39;]</em> <em>= &#39;rnn&#39;</em>",id:"field-encoder_type--literalrnn-brnn--rnn",level:4},{value:"<em>pydantic model</em> eole.config.models.RnnDecoderConfig",id:"pydantic-model-eoleconfigmodelsrnndecoderconfig",level:3},{value:"<em>field</em> bidirectional_encoder <em>: bool | None</em> <em>= False</em>",id:"field-bidirectional_encoder--bool--none--false",level:4},{value:"<em>field</em> context_gate <em>: Literal[&#39;source&#39;, &#39;target&#39;, &#39;both&#39;, None]</em> <em>= None</em>",id:"field-context_gate--literalsource-target-both-none--none",level:4},{value:"<em>field</em> decoder_type <em>: Literal[&#39;rnn&#39;]</em> <em>= &#39;rnn&#39;</em>",id:"field-decoder_type--literalrnn--rnn",level:4},{value:"<em>pydantic model</em> eole.config.models.RnnModelConfig",id:"pydantic-model-eoleconfigmodelsrnnmodelconfig",level:3},{value:"<em>field</em> architecture <em>: Literal[&#39;rnn&#39;]</em> <em>= &#39;rnn&#39;</em>",id:"field-architecture--literalrnn--rnn",level:4},{value:"<em>validator</em> default_architecture  <em>\xbb</em>  <em>all fields</em>",id:"validator-default_architecture----all-fields-3",level:4},{value:"<em>validator</em> encoder_decoder_type  <em>\xbb</em>  <em>all fields</em>",id:"validator-encoder_decoder_type----all-fields-2",level:4},{value:"CNN",id:"cnn",level:2},{value:"<em>pydantic model</em> eole.config.models.CnnConfig",id:"pydantic-model-eoleconfigmodelscnnconfig",level:3},{value:"<em>field</em> cnn_kernel_width <em>: int</em> <em>= 3</em>",id:"field-cnn_kernel_width--int--3",level:4},{value:"<em>pydantic model</em> eole.config.models.CnnEncoderConfig",id:"pydantic-model-eoleconfigmodelscnnencoderconfig",level:3},{value:"<em>field</em> encoder_type <em>: Literal[&#39;cnn&#39;]</em> <em>= &#39;cnn&#39;</em>",id:"field-encoder_type--literalcnn--cnn",level:4},{value:"<em>pydantic model</em> eole.config.models.CnnDecoderConfig",id:"pydantic-model-eoleconfigmodelscnndecoderconfig",level:3},{value:"<em>field</em> decoder_type <em>: Literal[&#39;cnn&#39;]</em> <em>= &#39;cnn&#39;</em>",id:"field-decoder_type--literalcnn--cnn",level:4},{value:"<em>pydantic model</em> eole.config.models.CnnModelConfig",id:"pydantic-model-eoleconfigmodelscnnmodelconfig",level:3},{value:"<em>field</em> architecture <em>: Literal[&#39;cnn&#39;]</em> <em>= &#39;cnn&#39;</em>",id:"field-architecture--literalcnn--cnn",level:4},{value:"<em>validator</em> default_architecture  <em>\xbb</em>  <em>all fields</em>",id:"validator-default_architecture----all-fields-4",level:4},{value:"<em>validator</em> encoder_decoder_type  <em>\xbb</em>  <em>all fields</em>",id:"validator-encoder_decoder_type----all-fields-3",level:4}];function c(e){const n={a:"a",code:"code",details:"details",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",summary:"summary",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"models",children:"Models"}),"\n",(0,t.jsx)(n.h2,{id:"base-configs",children:"Base Configs"}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsbasemodelconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.BaseModelConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.code,{children:"Config"})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "BaseModelConfig",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "$ref": "#/$defs/EmbeddingsConfig",\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "brnn": "#/$defs/RnnEncoderConfig",\n                     "cnn": "#/$defs/CnnEncoderConfig",\n                     "mean": "#/$defs/MeanEncoderConfig",\n                     "rnn": "#/$defs/RnnEncoderConfig",\n                     "transformer": "#/$defs/TransformerEncoderConfig",\n                     "vision": "#/$defs/VisionEncoderConfig"\n                  },\n                  "propertyName": "encoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/MeanEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/VisionEncoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": -1,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "moe_transformer_ff": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Size of hidden moe transformer feed-forward.",\n         "title": "Moe Transformer Ff"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "generator_bias": {\n         "default": true,\n         "description": "Control whether or not the generator Linear module has bias weights.",\n         "title": "Generator Bias",\n         "type": "boolean"\n      },\n      "adapter_bias": {\n         "default": false,\n         "description": "Control whether or not the adapter module has bias weights.",\n         "title": "Adapter Bias",\n         "type": "boolean"\n      },\n      "projector_activation_fn": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in adapter projector layer."\n      },\n      "spatial_merge_size": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": 1,\n         "description": "Control the presence and size of patch merger (Mistral3)",\n         "title": "Spatial Merge Size"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "estimator_type": {\n         "default": "average",\n         "description": "Which hidden_states to use to feed the estimator",\n         "enum": [\n            "average",\n            "last_token",\n            "first_token"\n         ],\n         "title": "Estimator Type",\n         "type": "string"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "huggingface_model": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Original huggingface model.",\n         "title": "Huggingface Model"\n      },\n      "eole_version": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "0.4.0",\n         "description": "Eole version used to convert/train/save the model.",\n         "title": "Eole Version"\n      },\n      "architecture": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "title": "Architecture"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "quick_gelu",\n            "gated-gelu",\n            "fused-gated-gelu",\n            "gated-silu",\n            "fused-gated-silu",\n            "gelu-tanh",\n            "gated-gelu-tanh",\n            "fused-gated-gelu-tanh"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "CnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnEncoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            },\n            "normalize": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "description": "Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909",\n               "title": "Normalize"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "MeanEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "mean",\n               "default": "mean",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            }\n         },\n         "title": "MeanEncoderConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "default": "rnn",\n               "enum": [\n                  "rnn",\n                  "brnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            }\n         },\n         "title": "RnnEncoderConfig",\n         "type": "object"\n      },\n      "RotaryPositionConfig": {\n         "additionalProperties": false,\n         "description": "Configuration for rotary position embeddings used in transformer models.",\n         "properties": {\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "scaling_type": {\n               "anyOf": [\n                  {\n                     "type": "string"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Specifies the type of RoPE scaling to be applied, if any.",\n               "title": "Scaling Type"\n            },\n            "alpha": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "alpha factor by which to scale rope theta.",\n               "title": "Alpha"\n            },\n            "scaling_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8.0,\n               "description": "Factor by which to scale RoPE embeddings.",\n               "title": "Scaling Factor"\n            },\n            "low_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1.0,\n               "description": "Scaling factor applied to the lower frequency components of RoPE.",\n               "title": "Low Freq Factor"\n            },\n            "high_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 4.0,\n               "description": "Scaling factor applied to the higher frequency components of RoPE.",\n               "title": "High Freq Factor"\n            },\n            "original_max_position_embeddings": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8192,\n               "description": "Original maximum position embeddings for RoPE scaling.",\n               "title": "Original Max Position Embeddings"\n            },\n            "rotary_theta_local": {\n               "default": 10000,\n               "description": "Rotary theta base length for local rotary layers",\n               "title": "Rotary Theta Local",\n               "type": "integer"\n            },\n            "interleave_local": {\n               "default": 0,\n               "description": "Local rotary layers each 1/N layers",\n               "title": "Interleave Local",\n               "type": "integer"\n            },\n            "tmax_index": {\n               "default": 0,\n               "description": "tmax indexing, 0 for all cases except gemma 3 = 1",\n               "title": "Tmax Index",\n               "type": "integer"\n            }\n         },\n         "title": "RotaryPositionConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            },\n            "LM_type": {\n               "default": "causal",\n               "description": "TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)",\n               "enum": [\n                  "causal",\n                  "prefix"\n               ],\n               "title": "Lm Type",\n               "type": "string"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            }\n         },\n         "title": "TransformerEncoderConfig",\n         "type": "object"\n      },\n      "VisionEncoderConfig": {\n         "additionalProperties": false,\n         "description": "Based on mistral-community/pixtral-12b, might evolve later.",\n         "properties": {\n            "encoder_type": {\n               "const": "vision",\n               "default": "vision",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "num_channels": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 3,\n               "title": "Num Channels"\n            },\n            "image_size": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1024,\n               "title": "Image Size"\n            },\n            "patch_size": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 16,\n               "title": "Patch Size"\n            },\n            "image_token_id": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 10,\n               "title": "Image Token Id"\n            },\n            "mm_tokens_per_image": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 256,\n               "title": "Mm Tokens Per Image"\n            },\n            "layernorm_pre": {\n               "default": true,\n               "title": "Layernorm Pre",\n               "type": "boolean"\n            },\n            "patch_conv_bias": {\n               "default": false,\n               "title": "Patch Conv Bias",\n               "type": "boolean"\n            },\n            "encoder_sam": {\n               "default": false,\n               "title": "Encoder Sam",\n               "type": "boolean"\n            },\n            "use_class_embedding": {\n               "default": false,\n               "title": "Use Class Embedding",\n               "type": "boolean"\n            }\n         },\n         "title": "VisionEncoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.adapter_bias",children:(0,t.jsx)(n.code,{children:"adapter_bias (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.add_estimator",children:(0,t.jsx)(n.code,{children:"add_estimator (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.architecture",children:(0,t.jsx)(n.code,{children:"architecture (str | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.decoder",children:(0,t.jsx)(n.code,{children:"decoder (eole.config.models.TransformerDecoderConfig | eole.config.models.RnnDecoderConfig | eole.config.models.CnnDecoderConfig | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.embeddings",children:(0,t.jsx)(n.code,{children:"embeddings (eole.config.models.EmbeddingsConfig)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.encoder",children:(0,t.jsx)(n.code,{children:"encoder (eole.config.models.TransformerEncoderConfig | eole.config.models.RnnEncoderConfig | eole.config.models.CnnEncoderConfig | eole.config.models.MeanEncoderConfig | eole.config.models.VisionEncoderConfig | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.eole_version",children:(0,t.jsx)(n.code,{children:"eole_version (str | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.estimator_type",children:(0,t.jsx)(n.code,{children:"estimator_type (Literal['average', 'last_token', 'first_token'])"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.generator_bias",children:(0,t.jsx)(n.code,{children:"generator_bias (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.generator_function",children:(0,t.jsx)(n.code,{children:"generator_function (Literal['softmax', 'sparsemax'])"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.hidden_size",children:(0,t.jsx)(n.code,{children:"hidden_size (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.huggingface_model",children:(0,t.jsx)(n.code,{children:"huggingface_model (str | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.input_feed",children:(0,t.jsx)(n.code,{children:"input_feed (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.layers",children:(0,t.jsx)(n.code,{children:"layers (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.left_pad",children:(0,t.jsx)(n.code,{children:"left_pad (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.moe_transformer_ff",children:(0,t.jsx)(n.code,{children:"moe_transformer_ff (int | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.projector_activation_fn",children:(0,t.jsx)(n.code,{children:"projector_activation_fn (eole.constants.ActivationFunction | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.share_decoder_embeddings",children:(0,t.jsx)(n.code,{children:"share_decoder_embeddings (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.share_embeddings",children:(0,t.jsx)(n.code,{children:"share_embeddings (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.spatial_merge_size",children:(0,t.jsx)(n.code,{children:"spatial_merge_size (int | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.transformer_ff",children:(0,t.jsx)(n.code,{children:"transformer_ff (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.word_vec_size",children:(0,t.jsx)(n.code,{children:"word_vec_size (int)"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validators:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"_override_values"})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"_validate_model_config"})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,t.jsx)(n.code,{children:"str_to_dict"})})," \xbb ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.decoder",children:(0,t.jsx)(n.code,{children:"decoder"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,t.jsx)(n.code,{children:"str_to_dict"})})," \xbb ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.embeddings",children:(0,t.jsx)(n.code,{children:"embeddings"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,t.jsx)(n.code,{children:"str_to_dict"})})," \xbb ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.encoder",children:(0,t.jsx)(n.code,{children:"encoder"})})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.validate_merge_size",children:(0,t.jsx)(n.code,{children:"validate_merge_size"})})," \xbb ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.spatial_merge_size",children:(0,t.jsx)(n.code,{children:"spatial_merge_size"})})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-adapter_bias--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," adapter_bias ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Control whether or not the adapter module has bias weights."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-add_estimator--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," add_estimator ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Add estimator layer"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-architecture--str--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," architecture ",(0,t.jsx)(n.em,{children:": str | None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-decoder--transformerdecoderconfig--rnndecoderconfig--cnndecoderconfig--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," decoder ",(0,t.jsxs)(n.em,{children:[": ",(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig",children:"TransformerDecoderConfig"})," | ",(0,t.jsx)(n.a,{href:"#eole.config.models.RnnDecoderConfig",children:"RnnDecoderConfig"})," | ",(0,t.jsx)(n.a,{href:"#eole.config.models.CnnDecoderConfig",children:"CnnDecoderConfig"})," | None"]})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Major parameters of a decoder."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,t.jsx)(n.code,{children:"str_to_dict"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-embeddings--embeddingsconfig-optional",children:[(0,t.jsx)(n.em,{children:"field"})," embeddings ",(0,t.jsxs)(n.em,{children:[": ",(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig",children:"EmbeddingsConfig"})]})," ",(0,t.jsx)(n.em,{children:"[Optional]"})]}),"\n",(0,t.jsx)(n.p,{children:"Contains most of the args useful to build the Embeddings module."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,t.jsx)(n.code,{children:"str_to_dict"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-encoder--transformerencoderconfig--rnnencoderconfig--cnnencoderconfig--meanencoderconfig--visionencoderconfig--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," encoder ",(0,t.jsxs)(n.em,{children:[": ",(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerEncoderConfig",children:"TransformerEncoderConfig"})," | ",(0,t.jsx)(n.a,{href:"#eole.config.models.RnnEncoderConfig",children:"RnnEncoderConfig"})," | ",(0,t.jsx)(n.a,{href:"#eole.config.models.CnnEncoderConfig",children:"CnnEncoderConfig"})," | MeanEncoderConfig | VisionEncoderConfig | None"]})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Major parameters of an encoder."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,t.jsx)(n.code,{children:"str_to_dict"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-eole_version--str--none--040",children:[(0,t.jsx)(n.em,{children:"field"})," eole_version ",(0,t.jsx)(n.em,{children:": str | None"})," ",(0,t.jsx)(n.em,{children:"= '0.4.0'"})]}),"\n",(0,t.jsx)(n.p,{children:"Eole version used to convert/train/save the model."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-estimator_type--literalaverage-last_token-first_token--average",children:[(0,t.jsx)(n.em,{children:"field"})," estimator_type ",(0,t.jsx)(n.em,{children:": Literal['average', 'last_token', 'first_token']"})," ",(0,t.jsx)(n.em,{children:"= 'average'"})]}),"\n",(0,t.jsx)(n.p,{children:"Which hidden_states to use to feed the estimator"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-generator_bias--bool--true",children:[(0,t.jsx)(n.em,{children:"field"})," generator_bias ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= True"})]}),"\n",(0,t.jsx)(n.p,{children:"Control whether or not the generator Linear module has bias weights."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-generator_function--literalsoftmax-sparsemax--softmax",children:[(0,t.jsx)(n.em,{children:"field"})," generator_function ",(0,t.jsx)(n.em,{children:": Literal['softmax', 'sparsemax']"})," ",(0,t.jsx)(n.em,{children:"= 'softmax'"})]}),"\n",(0,t.jsx)(n.p,{children:"Which function to use for generating probabilities over the target vocabulary."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-hidden_size--int---1",children:[(0,t.jsx)(n.em,{children:"field"})," hidden_size ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= -1"})]}),"\n",(0,t.jsx)(n.p,{children:"Size of hidden states. Overwrites [encoder/decoder].hidden_size if set."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-huggingface_model--str--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," huggingface_model ",(0,t.jsx)(n.em,{children:": str | None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Original huggingface model."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-input_feed--int--1",children:[(0,t.jsx)(n.em,{children:"field"})," input_feed ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 1"})]}),"\n",(0,t.jsx)(n.p,{children:"Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-layers--int---1",children:[(0,t.jsx)(n.em,{children:"field"})," layers ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= -1"})]}),"\n",(0,t.jsx)(n.p,{children:"Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers)."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-left_pad--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," left_pad ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Enable left-padding, useful for some LLMs."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-moe_transformer_ff--int--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," moe_transformer_ff ",(0,t.jsx)(n.em,{children:": int | None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Size of hidden moe transformer feed-forward."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-projector_activation_fn--activationfunction--none--activationfunctionrelu",children:[(0,t.jsx)(n.em,{children:"field"})," projector_activation_fn ",(0,t.jsx)(n.em,{children:": ActivationFunction | None"})," ",(0,t.jsx)(n.em,{children:"= ActivationFunction.relu"})]}),"\n",(0,t.jsx)(n.p,{children:"The activation function to use in adapter projector layer."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-share_decoder_embeddings--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," share_decoder_embeddings ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Use a share weight matrix for the input and output word embeddings in the decoder."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-share_embeddings--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," share_embeddings ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-spatial_merge_size--int--none--1",children:[(0,t.jsx)(n.em,{children:"field"})," spatial_merge_size ",(0,t.jsx)(n.em,{children:": int | None"})," ",(0,t.jsx)(n.em,{children:"= 1"})]}),"\n",(0,t.jsx)(n.p,{children:"Control the presence and size of patch merger (Mistral3)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.validate_merge_size",children:(0,t.jsx)(n.code,{children:"validate_merge_size"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-transformer_ff--int---1",children:[(0,t.jsx)(n.em,{children:"field"})," transformer_ff ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= -1"})]}),"\n",(0,t.jsx)(n.p,{children:"Size of hidden transformer feed-forward."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-word_vec_size--int---1",children:[(0,t.jsx)(n.em,{children:"field"})," word_vec_size ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= -1"})]}),"\n",(0,t.jsx)(n.p,{children:"Word embedding size for src and tgt."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-default_architecture----all-fields",children:[(0,t.jsx)(n.em,{children:"validator"})," default_architecture  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.em,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-str_to_dict----encoder--decoder--embeddings",children:[(0,t.jsx)(n.em,{children:"validator"})," str_to_dict  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.encoder",children:(0,t.jsx)(n.em,{children:"encoder"})})," ",(0,t.jsx)(n.em,{children:","})," ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.decoder",children:(0,t.jsx)(n.em,{children:"decoder"})})," ",(0,t.jsx)(n.em,{children:","})," ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.embeddings",children:(0,t.jsx)(n.em,{children:"embeddings"})})]}),"\n",(0,t.jsx)(n.h4,{id:"update_model_opts",children:"update_model_opts()"}),"\n",(0,t.jsxs)(n.h4,{id:"validator-validate_merge_size----spatial_merge_size",children:[(0,t.jsx)(n.em,{children:"validator"})," validate_merge_size  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.spatial_merge_size",children:(0,t.jsx)(n.em,{children:"spatial_merge_size"})})]}),"\n",(0,t.jsxs)(n.h4,{id:"property-model_type--modeltype",children:[(0,t.jsx)(n.em,{children:"property"})," model_type ",(0,t.jsx)(n.em,{children:": ModelType"})]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsembeddingsconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.EmbeddingsConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.code,{children:"Config"})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "EmbeddingsConfig",\n   "type": "object",\n   "properties": {\n      "src_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for src.",\n         "title": "Src Word Vec Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "freeze_word_vecs_enc": {\n         "default": false,\n         "description": "Freeze word embeddings on the encoder side.",\n         "title": "Freeze Word Vecs Enc",\n         "type": "boolean"\n      },\n      "freeze_word_vecs_dec": {\n         "default": false,\n         "description": "Freeze word embeddings on the encoder side.",\n         "title": "Freeze Word Vecs Dec",\n         "type": "boolean"\n      },\n      "position_encoding": {\n         "default": false,\n         "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n         "title": "Position Encoding",\n         "type": "boolean"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      },\n      "position_shift": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": 0,\n         "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n         "title": "Position Shift"\n      },\n      "normalize": {\n         "anyOf": [\n            {\n               "type": "boolean"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": false,\n         "description": "Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909",\n         "title": "Normalize"\n      }\n   },\n   "$defs": {\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.freeze_word_vecs_dec",children:(0,t.jsx)(n.code,{children:"freeze_word_vecs_dec (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.freeze_word_vecs_enc",children:(0,t.jsx)(n.code,{children:"freeze_word_vecs_enc (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.n_positions",children:(0,t.jsx)(n.code,{children:"n_positions (int | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.normalize",children:(0,t.jsx)(n.code,{children:"normalize (bool | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.position_encoding",children:(0,t.jsx)(n.code,{children:"position_encoding (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.position_encoding_type",children:(0,t.jsx)(n.code,{children:"position_encoding_type (eole.constants.PositionEncodingType | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.position_shift",children:(0,t.jsx)(n.code,{children:"position_shift (int | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.src_word_vec_size",children:(0,t.jsx)(n.code,{children:"src_word_vec_size (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.tgt_word_vec_size",children:(0,t.jsx)(n.code,{children:"tgt_word_vec_size (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.word_vec_size",children:(0,t.jsx)(n.code,{children:"word_vec_size (int)"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validators:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-freeze_word_vecs_dec--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," freeze_word_vecs_dec ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Freeze word embeddings on the encoder side."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-freeze_word_vecs_enc--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," freeze_word_vecs_enc ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Freeze word embeddings on the encoder side."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-n_positions--int--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," n_positions ",(0,t.jsx)(n.em,{children:": int | None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-normalize--bool--none--false",children:[(0,t.jsx)(n.em,{children:"field"})," normalize ",(0,t.jsx)(n.em,{children:": bool | None"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsxs)(n.p,{children:["Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. ",(0,t.jsx)(n.a,{href:"https://datascience.stackexchange.com/a/87909",children:"https://datascience.stackexchange.com/a/87909"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-position_encoding--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," position_encoding ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-position_encoding_type--positionencodingtype--none--positionencodingtypesinusoidalinterleaved",children:[(0,t.jsx)(n.em,{children:"field"})," position_encoding_type ",(0,t.jsx)(n.em,{children:": PositionEncodingType | None"})," ",(0,t.jsx)(n.em,{children:"= PositionEncodingType.SinusoidalInterleaved"})]}),"\n",(0,t.jsx)(n.p,{children:"Type of positional encoding."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-position_shift--int--none--0",children:[(0,t.jsx)(n.em,{children:"field"})," position_shift ",(0,t.jsx)(n.em,{children:": int | None"})," ",(0,t.jsx)(n.em,{children:"= 0"})]}),"\n",(0,t.jsx)(n.p,{children:"Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-src_word_vec_size--int--512",children:[(0,t.jsx)(n.em,{children:"field"})," src_word_vec_size ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 512"})]}),"\n",(0,t.jsx)(n.p,{children:"Word embedding size for src."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-tgt_word_vec_size--int--512",children:[(0,t.jsx)(n.em,{children:"field"})," tgt_word_vec_size ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 512"})]}),"\n",(0,t.jsx)(n.p,{children:"Word embedding size for tgt."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-word_vec_size--int---1-1",children:[(0,t.jsx)(n.em,{children:"field"})," word_vec_size ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= -1"})]}),"\n",(0,t.jsx)(n.p,{children:"Word embedding size for src and tgt."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,t.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-validate_embeddings----all-fields",children:[(0,t.jsx)(n.em,{children:"validator"})," validate_embeddings  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.em,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsencoderconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.EncoderConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.code,{children:"Config"})]}),"\n",(0,t.jsx)(n.p,{children:"Abstract class for all encoders"}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "EncoderConfig",\n   "description": "Abstract class for all encoders",\n   "type": "object",\n   "properties": {\n      "encoder_type": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "rnn",\n         "description": "Type of encoder layer(s) to use.",\n         "title": "Encoder Type"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the encoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of encoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "src_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for src.",\n         "title": "Src Word Vec Size",\n         "type": "integer"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EncoderConfig.encoder_type",children:(0,t.jsx)(n.code,{children:"encoder_type (str | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EncoderConfig.hidden_size",children:(0,t.jsx)(n.code,{children:"hidden_size (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EncoderConfig.layers",children:(0,t.jsx)(n.code,{children:"layers (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.EncoderConfig.src_word_vec_size",children:(0,t.jsx)(n.code,{children:"src_word_vec_size (int)"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-encoder_type--str--none--rnn",children:[(0,t.jsx)(n.em,{children:"field"})," encoder_type ",(0,t.jsx)(n.em,{children:": str | None"})," ",(0,t.jsx)(n.em,{children:"= 'rnn'"})]}),"\n",(0,t.jsx)(n.p,{children:"Type of encoder layer(s) to use."}),"\n",(0,t.jsxs)(n.h4,{id:"field-hidden_size--int--512",children:[(0,t.jsx)(n.em,{children:"field"})," hidden_size ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 512"})]}),"\n",(0,t.jsx)(n.p,{children:"Size of encoder hidden states."}),"\n",(0,t.jsxs)(n.h4,{id:"field-layers--int--2",children:[(0,t.jsx)(n.em,{children:"field"})," layers ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 2"})]}),"\n",(0,t.jsx)(n.p,{children:"Number of layers in the encoder."}),"\n",(0,t.jsxs)(n.h4,{id:"field-src_word_vec_size--int--512-1",children:[(0,t.jsx)(n.em,{children:"field"})," src_word_vec_size ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 512"})]}),"\n",(0,t.jsx)(n.p,{children:"Word embedding size for src."}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsdecoderconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.DecoderConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.code,{children:"Config"})]}),"\n",(0,t.jsx)(n.p,{children:"Abstract class for all decoders"}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "DecoderConfig",\n   "description": "Abstract class for all decoders",\n   "type": "object",\n   "properties": {\n      "decoder_type": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "rnn",\n         "description": "Type of decoder layer(s) to use.",\n         "title": "Decoder Type"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the decoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of decoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "coverage_attn": {\n         "default": false,\n         "description": "Train a coverage attention layer.",\n         "title": "Coverage Attn",\n         "type": "boolean"\n      },\n      "lambda_coverage": {\n         "default": 0.0,\n         "description": "Lambda value for coverage loss of See et al (2017)",\n         "title": "Lambda Coverage",\n         "type": "number"\n      },\n      "global_attention": {\n         "default": "general",\n         "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n         "enum": [\n            "dot",\n            "general",\n            "mlp",\n            null\n         ],\n         "title": "Global Attention"\n      },\n      "global_attention_function": {\n         "default": "softmax",\n         "description": "Global attention function to use.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Global Attention Function",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.coverage_attn",children:(0,t.jsx)(n.code,{children:"coverage_attn (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.decoder_type",children:(0,t.jsx)(n.code,{children:"decoder_type (str | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.global_attention",children:(0,t.jsx)(n.code,{children:"global_attention (Literal['dot', 'general', 'mlp', None])"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.global_attention_function",children:(0,t.jsx)(n.code,{children:"global_attention_function (Literal['softmax', 'sparsemax'])"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.hidden_size",children:(0,t.jsx)(n.code,{children:"hidden_size (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.lambda_coverage",children:(0,t.jsx)(n.code,{children:"lambda_coverage (float)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.layers",children:(0,t.jsx)(n.code,{children:"layers (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.tgt_word_vec_size",children:(0,t.jsx)(n.code,{children:"tgt_word_vec_size (int)"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validators:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"_validate_decoder_config"})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-coverage_attn--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," coverage_attn ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Train a coverage attention layer."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-decoder_type--str--none--rnn",children:[(0,t.jsx)(n.em,{children:"field"})," decoder_type ",(0,t.jsx)(n.em,{children:": str | None"})," ",(0,t.jsx)(n.em,{children:"= 'rnn'"})]}),"\n",(0,t.jsx)(n.p,{children:"Type of decoder layer(s) to use."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-global_attention--literaldot-general-mlp-none--general",children:[(0,t.jsx)(n.em,{children:"field"})," global_attention ",(0,t.jsx)(n.em,{children:": Literal['dot', 'general', 'mlp', None]"})," ",(0,t.jsx)(n.em,{children:"= 'general'"})]}),"\n",(0,t.jsx)(n.p,{children:"The attention type to use. (Luong=general, Bahdanau=MLP)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-global_attention_function--literalsoftmax-sparsemax--softmax",children:[(0,t.jsx)(n.em,{children:"field"})," global_attention_function ",(0,t.jsx)(n.em,{children:": Literal['softmax', 'sparsemax']"})," ",(0,t.jsx)(n.em,{children:"= 'softmax'"})]}),"\n",(0,t.jsx)(n.p,{children:"Global attention function to use."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-hidden_size--int--512-1",children:[(0,t.jsx)(n.em,{children:"field"})," hidden_size ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 512"})]}),"\n",(0,t.jsx)(n.p,{children:"Size of decoder hidden states."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-lambda_coverage--float--00",children:[(0,t.jsx)(n.em,{children:"field"})," lambda_coverage ",(0,t.jsx)(n.em,{children:": float"})," ",(0,t.jsx)(n.em,{children:"= 0.0"})]}),"\n",(0,t.jsx)(n.p,{children:"Lambda value for coverage loss of See et al (2017)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-layers--int--2-1",children:[(0,t.jsx)(n.em,{children:"field"})," layers ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 2"})]}),"\n",(0,t.jsx)(n.p,{children:"Number of layers in the decoder."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-tgt_word_vec_size--int--512-1",children:[(0,t.jsx)(n.em,{children:"field"})," tgt_word_vec_size ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 512"})]}),"\n",(0,t.jsx)(n.p,{children:"Word embedding size for tgt."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelscustommodelconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.CustomModelConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig",children:(0,t.jsx)(n.code,{children:"TransformerConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig",children:(0,t.jsx)(n.code,{children:"BaseModelConfig"})})]}),"\n",(0,t.jsx)(n.p,{children:"Wrap anything that does not fit a set common architecture."}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "CustomModelConfig",\n   "description": "Wrap anything that does not fit a set common architecture.",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "$ref": "#/$defs/EmbeddingsConfig",\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "brnn": "#/$defs/RnnEncoderConfig",\n                     "cnn": "#/$defs/CnnEncoderConfig",\n                     "mean": "#/$defs/MeanEncoderConfig",\n                     "rnn": "#/$defs/RnnEncoderConfig",\n                     "transformer": "#/$defs/TransformerEncoderConfig",\n                     "vision": "#/$defs/VisionEncoderConfig"\n                  },\n                  "propertyName": "encoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/MeanEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/VisionEncoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "moe_transformer_ff": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Size of hidden moe transformer feed-forward.",\n         "title": "Moe Transformer Ff"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "generator_bias": {\n         "default": true,\n         "description": "Control whether or not the generator Linear module has bias weights.",\n         "title": "Generator Bias",\n         "type": "boolean"\n      },\n      "adapter_bias": {\n         "default": false,\n         "description": "Control whether or not the adapter module has bias weights.",\n         "title": "Adapter Bias",\n         "type": "boolean"\n      },\n      "projector_activation_fn": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in adapter projector layer."\n      },\n      "spatial_merge_size": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": 1,\n         "description": "Control the presence and size of patch merger (Mistral3)",\n         "title": "Spatial Merge Size"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "estimator_type": {\n         "default": "average",\n         "description": "Which hidden_states to use to feed the estimator",\n         "enum": [\n            "average",\n            "last_token",\n            "first_token"\n         ],\n         "title": "Estimator Type",\n         "type": "string"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "huggingface_model": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Original huggingface model.",\n         "title": "Huggingface Model"\n      },\n      "eole_version": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "0.4.0",\n         "description": "Eole version used to convert/train/save the model.",\n         "title": "Eole Version"\n      },\n      "architecture": {\n         "const": "custom",\n         "default": "custom",\n         "title": "Architecture",\n         "type": "string"\n      },\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "$ref": "#/$defs/ActivationFunction",\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "standardFP32",\n            "rms",\n            "gemma-rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-05,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "ffn_layernorm": {\n         "default": false,\n         "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n         "title": "Ffn Layernorm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "query_norm": {\n         "default": false,\n         "title": "Query Norm",\n         "type": "boolean"\n      },\n      "key_norm": {\n         "default": false,\n         "title": "Key Norm",\n         "type": "boolean"\n      },\n      "qk_norm_post_rope": {\n         "default": false,\n         "title": "Qk Norm Post Rope",\n         "type": "boolean"\n      },\n      "add_final_linear_bias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of final_linear in MHA.",\n         "title": "Add Final Linear Bias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "head_dim": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Head dimension when this needs to be different vs hidden_size // heads",\n         "title": "Head Dim"\n      },\n      "attn_scaling": {\n         "anyOf": [\n            {\n               "type": "number"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n         "title": "Attn Scaling"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_shared_experts": {\n         "default": 0,\n         "description": "Number of shared experts for MoE models (DeepSeekv2).",\n         "title": "Num Shared Experts",\n         "type": "integer"\n      },\n      "first_k_dense_replace": {\n         "default": 0,\n         "description": "Number of layers using Dense instead of MoE",\n         "title": "First K Dense Replace",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "moe_softmax_after": {\n         "default": false,\n         "description": "Usually softmax is before topk, Mixtral does it after.",\n         "title": "Moe Softmax After",\n         "type": "boolean"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      },\n      "rope_config": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/RotaryPositionConfig"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Rotary position config, if relevant."\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "quick_gelu",\n            "gated-gelu",\n            "fused-gated-gelu",\n            "gated-silu",\n            "fused-gated-silu",\n            "gelu-tanh",\n            "gated-gelu-tanh",\n            "fused-gated-gelu-tanh"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "CnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnEncoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            },\n            "normalize": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "description": "Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909",\n               "title": "Normalize"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "MeanEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "mean",\n               "default": "mean",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            }\n         },\n         "title": "MeanEncoderConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "default": "rnn",\n               "enum": [\n                  "rnn",\n                  "brnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            }\n         },\n         "title": "RnnEncoderConfig",\n         "type": "object"\n      },\n      "RotaryPositionConfig": {\n         "additionalProperties": false,\n         "description": "Configuration for rotary position embeddings used in transformer models.",\n         "properties": {\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "scaling_type": {\n               "anyOf": [\n                  {\n                     "type": "string"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Specifies the type of RoPE scaling to be applied, if any.",\n               "title": "Scaling Type"\n            },\n            "alpha": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "alpha factor by which to scale rope theta.",\n               "title": "Alpha"\n            },\n            "scaling_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8.0,\n               "description": "Factor by which to scale RoPE embeddings.",\n               "title": "Scaling Factor"\n            },\n            "low_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1.0,\n               "description": "Scaling factor applied to the lower frequency components of RoPE.",\n               "title": "Low Freq Factor"\n            },\n            "high_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 4.0,\n               "description": "Scaling factor applied to the higher frequency components of RoPE.",\n               "title": "High Freq Factor"\n            },\n            "original_max_position_embeddings": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8192,\n               "description": "Original maximum position embeddings for RoPE scaling.",\n               "title": "Original Max Position Embeddings"\n            },\n            "rotary_theta_local": {\n               "default": 10000,\n               "description": "Rotary theta base length for local rotary layers",\n               "title": "Rotary Theta Local",\n               "type": "integer"\n            },\n            "interleave_local": {\n               "default": 0,\n               "description": "Local rotary layers each 1/N layers",\n               "title": "Interleave Local",\n               "type": "integer"\n            },\n            "tmax_index": {\n               "default": 0,\n               "description": "tmax indexing, 0 for all cases except gemma 3 = 1",\n               "title": "Tmax Index",\n               "type": "integer"\n            }\n         },\n         "title": "RotaryPositionConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            },\n            "LM_type": {\n               "default": "causal",\n               "description": "TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)",\n               "enum": [\n                  "causal",\n                  "prefix"\n               ],\n               "title": "Lm Type",\n               "type": "string"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            }\n         },\n         "title": "TransformerEncoderConfig",\n         "type": "object"\n      },\n      "VisionEncoderConfig": {\n         "additionalProperties": false,\n         "description": "Based on mistral-community/pixtral-12b, might evolve later.",\n         "properties": {\n            "encoder_type": {\n               "const": "vision",\n               "default": "vision",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "num_channels": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 3,\n               "title": "Num Channels"\n            },\n            "image_size": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1024,\n               "title": "Image Size"\n            },\n            "patch_size": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 16,\n               "title": "Patch Size"\n            },\n            "image_token_id": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 10,\n               "title": "Image Token Id"\n            },\n            "mm_tokens_per_image": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 256,\n               "title": "Mm Tokens Per Image"\n            },\n            "layernorm_pre": {\n               "default": true,\n               "title": "Layernorm Pre",\n               "type": "boolean"\n            },\n            "patch_conv_bias": {\n               "default": false,\n               "title": "Patch Conv Bias",\n               "type": "boolean"\n            },\n            "encoder_sam": {\n               "default": false,\n               "title": "Encoder Sam",\n               "type": "boolean"\n            },\n            "use_class_embedding": {\n               "default": false,\n               "title": "Use Class Embedding",\n               "type": "boolean"\n            }\n         },\n         "title": "VisionEncoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.CustomModelConfig.architecture",children:(0,t.jsx)(n.code,{children:"architecture (Literal['custom'])"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Validators:"})}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-architecture--literalcustom--custom",children:[(0,t.jsx)(n.em,{children:"field"})," architecture ",(0,t.jsx)(n.em,{children:": Literal['custom']"})," ",(0,t.jsx)(n.em,{children:"= 'custom'"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"transformer",children:"Transformer"}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformerconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.code,{children:"Config"})]}),"\n",(0,t.jsx)(n.p,{children:"This base TransformerConfig class regroups parameters than can\nboth be set at model level or either encoder/decoder level.\nBaseModelConfig._override_values validator overrides\nencoder/decoder values with model values if relevant."}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerConfig",\n   "description": "This base TransformerConfig class regroups parameters than can\\nboth be set at model level or either encoder/decoder level.\\nBaseModelConfig._override_values validator overrides\\nencoder/decoder values with model values if relevant.",\n   "type": "object",\n   "properties": {\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "moe_transformer_ff": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Size of hidden moe transformer feed-forward.",\n         "title": "Moe Transformer Ff"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "$ref": "#/$defs/ActivationFunction",\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "standardFP32",\n            "rms",\n            "gemma-rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-05,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "ffn_layernorm": {\n         "default": false,\n         "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n         "title": "Ffn Layernorm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "query_norm": {\n         "default": false,\n         "title": "Query Norm",\n         "type": "boolean"\n      },\n      "key_norm": {\n         "default": false,\n         "title": "Key Norm",\n         "type": "boolean"\n      },\n      "qk_norm_post_rope": {\n         "default": false,\n         "title": "Qk Norm Post Rope",\n         "type": "boolean"\n      },\n      "add_final_linear_bias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of final_linear in MHA.",\n         "title": "Add Final Linear Bias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "head_dim": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Head dimension when this needs to be different vs hidden_size // heads",\n         "title": "Head Dim"\n      },\n      "attn_scaling": {\n         "anyOf": [\n            {\n               "type": "number"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n         "title": "Attn Scaling"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_shared_experts": {\n         "default": 0,\n         "description": "Number of shared experts for MoE models (DeepSeekv2).",\n         "title": "Num Shared Experts",\n         "type": "integer"\n      },\n      "first_k_dense_replace": {\n         "default": 0,\n         "description": "Number of layers using Dense instead of MoE",\n         "title": "First K Dense Replace",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "moe_softmax_after": {\n         "default": false,\n         "description": "Usually softmax is before topk, Mixtral does it after.",\n         "title": "Moe Softmax After",\n         "type": "boolean"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      },\n      "rope_config": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/RotaryPositionConfig"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Rotary position config, if relevant."\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "quick_gelu",\n            "gated-gelu",\n            "fused-gated-gelu",\n            "gated-silu",\n            "fused-gated-silu",\n            "gelu-tanh",\n            "gated-gelu-tanh",\n            "fused-gated-gelu-tanh"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RotaryPositionConfig": {\n         "additionalProperties": false,\n         "description": "Configuration for rotary position embeddings used in transformer models.",\n         "properties": {\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "scaling_type": {\n               "anyOf": [\n                  {\n                     "type": "string"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Specifies the type of RoPE scaling to be applied, if any.",\n               "title": "Scaling Type"\n            },\n            "alpha": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "alpha factor by which to scale rope theta.",\n               "title": "Alpha"\n            },\n            "scaling_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8.0,\n               "description": "Factor by which to scale RoPE embeddings.",\n               "title": "Scaling Factor"\n            },\n            "low_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1.0,\n               "description": "Scaling factor applied to the lower frequency components of RoPE.",\n               "title": "Low Freq Factor"\n            },\n            "high_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 4.0,\n               "description": "Scaling factor applied to the higher frequency components of RoPE.",\n               "title": "High Freq Factor"\n            },\n            "original_max_position_embeddings": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8192,\n               "description": "Original maximum position embeddings for RoPE scaling.",\n               "title": "Original Max Position Embeddings"\n            },\n            "rotary_theta_local": {\n               "default": 10000,\n               "description": "Rotary theta base length for local rotary layers",\n               "title": "Rotary Theta Local",\n               "type": "integer"\n            },\n            "interleave_local": {\n               "default": 0,\n               "description": "Local rotary layers each 1/N layers",\n               "title": "Interleave Local",\n               "type": "integer"\n            },\n            "tmax_index": {\n               "default": 0,\n               "description": "tmax indexing, 0 for all cases except gemma 3 = 1",\n               "title": "Tmax Index",\n               "type": "integer"\n            }\n         },\n         "title": "RotaryPositionConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.add_ffnbias",children:(0,t.jsx)(n.code,{children:"add_ffnbias (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.add_final_linear_bias",children:(0,t.jsx)(n.code,{children:"add_final_linear_bias (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.add_qkvbias",children:(0,t.jsx)(n.code,{children:"add_qkvbias (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.attn_scaling",children:(0,t.jsx)(n.code,{children:"attn_scaling (float | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.ffn_layernorm",children:(0,t.jsx)(n.code,{children:"ffn_layernorm (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.first_k_dense_replace",children:(0,t.jsx)(n.code,{children:"first_k_dense_replace (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.head_dim",children:(0,t.jsx)(n.code,{children:"head_dim (int | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.heads",children:(0,t.jsx)(n.code,{children:"heads (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.heads_kv",children:(0,t.jsx)(n.code,{children:"heads_kv (int | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.key_norm",children:(0,t.jsx)(n.code,{children:"key_norm (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.layer_norm",children:(0,t.jsx)(n.code,{children:"layer_norm (Literal['standard', 'standardFP32', 'rms', 'gemma-rms'])"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.mlp_activation_fn",children:(0,t.jsx)(n.code,{children:"mlp_activation_fn (eole.constants.ActivationFunction)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.moe_softmax_after",children:(0,t.jsx)(n.code,{children:"moe_softmax_after (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.moe_transformer_ff",children:(0,t.jsx)(n.code,{children:"moe_transformer_ff (int | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.n_positions",children:(0,t.jsx)(n.code,{children:"n_positions (int | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.norm_eps",children:(0,t.jsx)(n.code,{children:"norm_eps (float)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.num_experts",children:(0,t.jsx)(n.code,{children:"num_experts (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.num_experts_per_tok",children:(0,t.jsx)(n.code,{children:"num_experts_per_tok (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.num_shared_experts",children:(0,t.jsx)(n.code,{children:"num_shared_experts (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.parallel_residual",children:(0,t.jsx)(n.code,{children:"parallel_residual (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.position_encoding_type",children:(0,t.jsx)(n.code,{children:"position_encoding_type (eole.constants.PositionEncodingType | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.qk_norm_post_rope",children:(0,t.jsx)(n.code,{children:"qk_norm_post_rope (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.query_norm",children:(0,t.jsx)(n.code,{children:"query_norm (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.relative_positions_buckets",children:(0,t.jsx)(n.code,{children:"relative_positions_buckets (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.rope_config",children:(0,t.jsx)(n.code,{children:"rope_config (eole.config.models.RotaryPositionConfig | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.shared_layer_norm",children:(0,t.jsx)(n.code,{children:"shared_layer_norm (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.sliding_window",children:(0,t.jsx)(n.code,{children:"sliding_window (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.transformer_ff",children:(0,t.jsx)(n.code,{children:"transformer_ff (int)"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validators:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"_validate_transformer_config"})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-add_ffnbias--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," add_ffnbias ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Add bias to nn.Linear of MLP FFN."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-add_final_linear_bias--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," add_final_linear_bias ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Add bias to nn.Linear of final_linear in MHA."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-add_qkvbias--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," add_qkvbias ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with add_final_linear_bias."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-attn_scaling--float--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," attn_scaling ",(0,t.jsx)(n.em,{children:": float | None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Attention scaling factor, when None uses 1/sqrt(head_dim) by default"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-ffn_layernorm--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," ffn_layernorm ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-first_k_dense_replace--int--0",children:[(0,t.jsx)(n.em,{children:"field"})," first_k_dense_replace ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 0"})]}),"\n",(0,t.jsx)(n.p,{children:"Number of layers using Dense instead of MoE"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-head_dim--int--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," head_dim ",(0,t.jsx)(n.em,{children:": int | None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Head dimension when this needs to be different vs hidden_size // heads"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-heads--int--8",children:[(0,t.jsx)(n.em,{children:"field"})," heads ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 8"})]}),"\n",(0,t.jsx)(n.p,{children:"Number of heads for transformer self-attention."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-heads_kv--int--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," heads_kv ",(0,t.jsx)(n.em,{children:": int | None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-key_norm--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," key_norm ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-layer_norm--literalstandard-standardfp32-rms-gemma-rms--standard",children:[(0,t.jsx)(n.em,{children:"field"})," layer_norm ",(0,t.jsx)(n.em,{children:": Literal['standard', 'standardFP32', 'rms', 'gemma-rms']"})," ",(0,t.jsx)(n.em,{children:"= 'standard'"})]}),"\n",(0,t.jsx)(n.p,{children:"Type of layer normalization in transformer architecture."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-mlp_activation_fn--activationfunction--activationfunctionrelu",children:[(0,t.jsx)(n.em,{children:"field"})," mlp_activation_fn ",(0,t.jsx)(n.em,{children:": ActivationFunction"})," ",(0,t.jsx)(n.em,{children:"= ActivationFunction.relu"})]}),"\n",(0,t.jsx)(n.p,{children:"The activation function to use in MLP layer."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-moe_softmax_after--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," moe_softmax_after ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Usually softmax is before topk, Mixtral does it after."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-moe_transformer_ff--int--none--none-1",children:[(0,t.jsx)(n.em,{children:"field"})," moe_transformer_ff ",(0,t.jsx)(n.em,{children:": int | None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Size of hidden moe transformer feed-forward."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-n_positions--int--none--none-1",children:[(0,t.jsx)(n.em,{children:"field"})," n_positions ",(0,t.jsx)(n.em,{children:": int | None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-norm_eps--float--1e-05",children:[(0,t.jsx)(n.em,{children:"field"})," norm_eps ",(0,t.jsx)(n.em,{children:": float"})," ",(0,t.jsx)(n.em,{children:"= 1e-05"})]}),"\n",(0,t.jsx)(n.p,{children:"Layer norm epsilon."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-num_experts--int--0",children:[(0,t.jsx)(n.em,{children:"field"})," num_experts ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 0"})]}),"\n",(0,t.jsx)(n.p,{children:"Number of experts for MoE models."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-num_experts_per_tok--int--2",children:[(0,t.jsx)(n.em,{children:"field"})," num_experts_per_tok ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 2"})]}),"\n",(0,t.jsx)(n.p,{children:"Number of experts per token."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-num_shared_experts--int--0",children:[(0,t.jsx)(n.em,{children:"field"})," num_shared_experts ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 0"})]}),"\n",(0,t.jsx)(n.p,{children:"Number of shared experts for MoE models (DeepSeekv2)."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-parallel_residual--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," parallel_residual ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-position_encoding_type--positionencodingtype--none--positionencodingtypesinusoidalinterleaved-1",children:[(0,t.jsx)(n.em,{children:"field"})," position_encoding_type ",(0,t.jsx)(n.em,{children:": PositionEncodingType | None"})," ",(0,t.jsx)(n.em,{children:"= PositionEncodingType.SinusoidalInterleaved"})]}),"\n",(0,t.jsx)(n.p,{children:"Type of positional encoding."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-qk_norm_post_rope--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," qk_norm_post_rope ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-query_norm--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," query_norm ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-relative_positions_buckets--int--0",children:[(0,t.jsx)(n.em,{children:"field"})," relative_positions_buckets ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 0"})]}),"\n",(0,t.jsxs)(n.p,{children:["Enable relative position bias (",(0,t.jsx)(n.a,{href:"https://github.com/google-research/text-to-text-transfer-transformer",children:"https://github.com/google-research/text-to-text-transfer-transformer"}),")."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-rope_config--rotarypositionconfig--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," rope_config ",(0,t.jsx)(n.em,{children:": RotaryPositionConfig | None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Rotary position config, if relevant."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-shared_layer_norm--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," shared_layer_norm ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-sliding_window--int--0",children:[(0,t.jsx)(n.em,{children:"field"})," sliding_window ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 0"})]}),"\n",(0,t.jsx)(n.p,{children:"Sliding window for transformer self-attention."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-transformer_ff--int--2048",children:[(0,t.jsx)(n.em,{children:"field"})," transformer_ff ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 2048"})]}),"\n",(0,t.jsx)(n.p,{children:"Size of hidden transformer feed-forward."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"property-dim_per_head--int",children:[(0,t.jsx)(n.em,{children:"property"})," dim_per_head ",(0,t.jsx)(n.em,{children:": int"})]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformerencoderconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerEncoderConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig",children:(0,t.jsx)(n.code,{children:"TransformerConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.EncoderConfig",children:(0,t.jsx)(n.code,{children:"EncoderConfig"})})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerEncoderConfig",\n   "type": "object",\n   "properties": {\n      "encoder_type": {\n         "const": "transformer",\n         "default": "transformer",\n         "title": "Encoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the encoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of encoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "src_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for src.",\n         "title": "Src Word Vec Size",\n         "type": "integer"\n      },\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "moe_transformer_ff": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Size of hidden moe transformer feed-forward.",\n         "title": "Moe Transformer Ff"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "$ref": "#/$defs/ActivationFunction",\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "standardFP32",\n            "rms",\n            "gemma-rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-05,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "ffn_layernorm": {\n         "default": false,\n         "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n         "title": "Ffn Layernorm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "query_norm": {\n         "default": false,\n         "title": "Query Norm",\n         "type": "boolean"\n      },\n      "key_norm": {\n         "default": false,\n         "title": "Key Norm",\n         "type": "boolean"\n      },\n      "qk_norm_post_rope": {\n         "default": false,\n         "title": "Qk Norm Post Rope",\n         "type": "boolean"\n      },\n      "add_final_linear_bias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of final_linear in MHA.",\n         "title": "Add Final Linear Bias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "head_dim": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Head dimension when this needs to be different vs hidden_size // heads",\n         "title": "Head Dim"\n      },\n      "attn_scaling": {\n         "anyOf": [\n            {\n               "type": "number"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n         "title": "Attn Scaling"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_shared_experts": {\n         "default": 0,\n         "description": "Number of shared experts for MoE models (DeepSeekv2).",\n         "title": "Num Shared Experts",\n         "type": "integer"\n      },\n      "first_k_dense_replace": {\n         "default": 0,\n         "description": "Number of layers using Dense instead of MoE",\n         "title": "First K Dense Replace",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "moe_softmax_after": {\n         "default": false,\n         "description": "Usually softmax is before topk, Mixtral does it after.",\n         "title": "Moe Softmax After",\n         "type": "boolean"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      },\n      "rope_config": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/RotaryPositionConfig"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Rotary position config, if relevant."\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "quick_gelu",\n            "gated-gelu",\n            "fused-gated-gelu",\n            "gated-silu",\n            "fused-gated-silu",\n            "gelu-tanh",\n            "gated-gelu-tanh",\n            "fused-gated-gelu-tanh"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RotaryPositionConfig": {\n         "additionalProperties": false,\n         "description": "Configuration for rotary position embeddings used in transformer models.",\n         "properties": {\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "scaling_type": {\n               "anyOf": [\n                  {\n                     "type": "string"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Specifies the type of RoPE scaling to be applied, if any.",\n               "title": "Scaling Type"\n            },\n            "alpha": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "alpha factor by which to scale rope theta.",\n               "title": "Alpha"\n            },\n            "scaling_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8.0,\n               "description": "Factor by which to scale RoPE embeddings.",\n               "title": "Scaling Factor"\n            },\n            "low_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1.0,\n               "description": "Scaling factor applied to the lower frequency components of RoPE.",\n               "title": "Low Freq Factor"\n            },\n            "high_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 4.0,\n               "description": "Scaling factor applied to the higher frequency components of RoPE.",\n               "title": "High Freq Factor"\n            },\n            "original_max_position_embeddings": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8192,\n               "description": "Original maximum position embeddings for RoPE scaling.",\n               "title": "Original Max Position Embeddings"\n            },\n            "rotary_theta_local": {\n               "default": 10000,\n               "description": "Rotary theta base length for local rotary layers",\n               "title": "Rotary Theta Local",\n               "type": "integer"\n            },\n            "interleave_local": {\n               "default": 0,\n               "description": "Local rotary layers each 1/N layers",\n               "title": "Interleave Local",\n               "type": "integer"\n            },\n            "tmax_index": {\n               "default": 0,\n               "description": "tmax indexing, 0 for all cases except gemma 3 = 1",\n               "title": "Tmax Index",\n               "type": "integer"\n            }\n         },\n         "title": "RotaryPositionConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerEncoderConfig.encoder_type",children:(0,t.jsx)(n.code,{children:"encoder_type (Literal['transformer'])"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Validators:"})}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-encoder_type--literaltransformer--transformer",children:[(0,t.jsx)(n.em,{children:"field"})," encoder_type ",(0,t.jsx)(n.em,{children:": Literal['transformer']"})," ",(0,t.jsx)(n.em,{children:"= 'transformer'"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformerdecoderconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerDecoderConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig",children:(0,t.jsx)(n.code,{children:"TransformerConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig",children:(0,t.jsx)(n.code,{children:"DecoderConfig"})})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerDecoderConfig",\n   "type": "object",\n   "properties": {\n      "decoder_type": {\n         "const": "transformer",\n         "default": "transformer",\n         "title": "Decoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the decoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of decoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "coverage_attn": {\n         "default": false,\n         "description": "Train a coverage attention layer.",\n         "title": "Coverage Attn",\n         "type": "boolean"\n      },\n      "lambda_coverage": {\n         "default": 0.0,\n         "description": "Lambda value for coverage loss of See et al (2017)",\n         "title": "Lambda Coverage",\n         "type": "number"\n      },\n      "global_attention": {\n         "default": "general",\n         "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n         "enum": [\n            "dot",\n            "general",\n            "mlp",\n            null\n         ],\n         "title": "Global Attention"\n      },\n      "global_attention_function": {\n         "default": "softmax",\n         "description": "Global attention function to use.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Global Attention Function",\n         "type": "string"\n      },\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "moe_transformer_ff": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Size of hidden moe transformer feed-forward.",\n         "title": "Moe Transformer Ff"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "$ref": "#/$defs/ActivationFunction",\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "standardFP32",\n            "rms",\n            "gemma-rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-05,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "ffn_layernorm": {\n         "default": false,\n         "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n         "title": "Ffn Layernorm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "query_norm": {\n         "default": false,\n         "title": "Query Norm",\n         "type": "boolean"\n      },\n      "key_norm": {\n         "default": false,\n         "title": "Key Norm",\n         "type": "boolean"\n      },\n      "qk_norm_post_rope": {\n         "default": false,\n         "title": "Qk Norm Post Rope",\n         "type": "boolean"\n      },\n      "add_final_linear_bias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of final_linear in MHA.",\n         "title": "Add Final Linear Bias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "head_dim": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Head dimension when this needs to be different vs hidden_size // heads",\n         "title": "Head Dim"\n      },\n      "attn_scaling": {\n         "anyOf": [\n            {\n               "type": "number"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n         "title": "Attn Scaling"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_shared_experts": {\n         "default": 0,\n         "description": "Number of shared experts for MoE models (DeepSeekv2).",\n         "title": "Num Shared Experts",\n         "type": "integer"\n      },\n      "first_k_dense_replace": {\n         "default": 0,\n         "description": "Number of layers using Dense instead of MoE",\n         "title": "First K Dense Replace",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "moe_softmax_after": {\n         "default": false,\n         "description": "Usually softmax is before topk, Mixtral does it after.",\n         "title": "Moe Softmax After",\n         "type": "boolean"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      },\n      "rope_config": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/RotaryPositionConfig"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Rotary position config, if relevant."\n      },\n      "aan_useffn": {\n         "default": false,\n         "description": "Turn on the FFN layer in the AAN decoder.",\n         "title": "Aan Useffn",\n         "type": "boolean"\n      },\n      "alignment_layer": {\n         "default": -2,\n         "description": "Layer number which has to be supervised.",\n         "title": "Alignment Layer",\n         "type": "integer"\n      },\n      "alignment_heads": {\n         "default": 0,\n         "description": "Number of cross attention heads per layer to supervise with.",\n         "title": "Alignment Heads",\n         "type": "integer"\n      },\n      "full_context_alignment": {\n         "default": false,\n         "description": "Whether alignment is conditioned on full target context.",\n         "title": "Full Context Alignment",\n         "type": "boolean"\n      },\n      "lambda_align": {\n         "default": 0.0,\n         "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n         "title": "Lambda Align",\n         "type": "number"\n      },\n      "LM_type": {\n         "default": "causal",\n         "description": "TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)",\n         "enum": [\n            "causal",\n            "prefix"\n         ],\n         "title": "Lm Type",\n         "type": "string"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "quick_gelu",\n            "gated-gelu",\n            "fused-gated-gelu",\n            "gated-silu",\n            "fused-gated-silu",\n            "gelu-tanh",\n            "gated-gelu-tanh",\n            "fused-gated-gelu-tanh"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RotaryPositionConfig": {\n         "additionalProperties": false,\n         "description": "Configuration for rotary position embeddings used in transformer models.",\n         "properties": {\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "scaling_type": {\n               "anyOf": [\n                  {\n                     "type": "string"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Specifies the type of RoPE scaling to be applied, if any.",\n               "title": "Scaling Type"\n            },\n            "alpha": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "alpha factor by which to scale rope theta.",\n               "title": "Alpha"\n            },\n            "scaling_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8.0,\n               "description": "Factor by which to scale RoPE embeddings.",\n               "title": "Scaling Factor"\n            },\n            "low_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1.0,\n               "description": "Scaling factor applied to the lower frequency components of RoPE.",\n               "title": "Low Freq Factor"\n            },\n            "high_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 4.0,\n               "description": "Scaling factor applied to the higher frequency components of RoPE.",\n               "title": "High Freq Factor"\n            },\n            "original_max_position_embeddings": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8192,\n               "description": "Original maximum position embeddings for RoPE scaling.",\n               "title": "Original Max Position Embeddings"\n            },\n            "rotary_theta_local": {\n               "default": 10000,\n               "description": "Rotary theta base length for local rotary layers",\n               "title": "Rotary Theta Local",\n               "type": "integer"\n            },\n            "interleave_local": {\n               "default": 0,\n               "description": "Local rotary layers each 1/N layers",\n               "title": "Interleave Local",\n               "type": "integer"\n            },\n            "tmax_index": {\n               "default": 0,\n               "description": "tmax indexing, 0 for all cases except gemma 3 = 1",\n               "title": "Tmax Index",\n               "type": "integer"\n            }\n         },\n         "title": "RotaryPositionConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.LM_type",children:(0,t.jsx)(n.code,{children:"LM_type (Literal['causal', 'prefix'])"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.aan_useffn",children:(0,t.jsx)(n.code,{children:"aan_useffn (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.alignment_heads",children:(0,t.jsx)(n.code,{children:"alignment_heads (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.alignment_layer",children:(0,t.jsx)(n.code,{children:"alignment_layer (int)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.decoder_type",children:(0,t.jsx)(n.code,{children:"decoder_type (Literal['transformer'])"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.full_context_alignment",children:(0,t.jsx)(n.code,{children:"full_context_alignment (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.lambda_align",children:(0,t.jsx)(n.code,{children:"lambda_align (float)"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validators:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"_validate_transformer_decoder_config"})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-lm_type--literalcausal-prefix--causal",children:[(0,t.jsx)(n.em,{children:"field"})," LM_type ",(0,t.jsx)(n.em,{children:": Literal['causal', 'prefix']"})," ",(0,t.jsx)(n.em,{children:"= 'causal'"})]}),"\n",(0,t.jsxs)(n.p,{children:["TransformerDecoder LM type (causal = classic, or prefix LM ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2308.06912",children:"https://arxiv.org/pdf/2308.06912"}),")"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-aan_useffn--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," aan_useffn ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Turn on the FFN layer in the AAN decoder."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-alignment_heads--int--0",children:[(0,t.jsx)(n.em,{children:"field"})," alignment_heads ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 0"})]}),"\n",(0,t.jsx)(n.p,{children:"Number of cross attention heads per layer to supervise with."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-alignment_layer--int---2",children:[(0,t.jsx)(n.em,{children:"field"})," alignment_layer ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= -2"})]}),"\n",(0,t.jsx)(n.p,{children:"Layer number which has to be supervised."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-decoder_type--literaltransformer--transformer",children:[(0,t.jsx)(n.em,{children:"field"})," decoder_type ",(0,t.jsx)(n.em,{children:": Literal['transformer']"})," ",(0,t.jsx)(n.em,{children:"= 'transformer'"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-full_context_alignment--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," full_context_alignment ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Whether alignment is conditioned on full target context."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-lambda_align--float--00",children:[(0,t.jsx)(n.em,{children:"field"})," lambda_align ",(0,t.jsx)(n.em,{children:": float"})," ",(0,t.jsx)(n.em,{children:"= 0.0"})]}),"\n",(0,t.jsxs)(n.p,{children:["Lambda value for alignement loss of Garg et al, 2019 (",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1909.02074",children:"https://arxiv.org/abs/1909.02074"}),")"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformermodelconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerModelConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig",children:(0,t.jsx)(n.code,{children:"TransformerConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig",children:(0,t.jsx)(n.code,{children:"BaseModelConfig"})})]}),"\n",(0,t.jsx)(n.p,{children:"Facilitate setting some transformer specific params at model level."}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerModelConfig",\n   "description": "Facilitate setting some transformer specific params at model level.",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "$ref": "#/$defs/EmbeddingsConfig",\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "brnn": "#/$defs/RnnEncoderConfig",\n                     "cnn": "#/$defs/CnnEncoderConfig",\n                     "mean": "#/$defs/MeanEncoderConfig",\n                     "rnn": "#/$defs/RnnEncoderConfig",\n                     "transformer": "#/$defs/TransformerEncoderConfig",\n                     "vision": "#/$defs/VisionEncoderConfig"\n                  },\n                  "propertyName": "encoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/MeanEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/VisionEncoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "moe_transformer_ff": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Size of hidden moe transformer feed-forward.",\n         "title": "Moe Transformer Ff"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "generator_bias": {\n         "default": true,\n         "description": "Control whether or not the generator Linear module has bias weights.",\n         "title": "Generator Bias",\n         "type": "boolean"\n      },\n      "adapter_bias": {\n         "default": false,\n         "description": "Control whether or not the adapter module has bias weights.",\n         "title": "Adapter Bias",\n         "type": "boolean"\n      },\n      "projector_activation_fn": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in adapter projector layer."\n      },\n      "spatial_merge_size": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": 1,\n         "description": "Control the presence and size of patch merger (Mistral3)",\n         "title": "Spatial Merge Size"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "estimator_type": {\n         "default": "average",\n         "description": "Which hidden_states to use to feed the estimator",\n         "enum": [\n            "average",\n            "last_token",\n            "first_token"\n         ],\n         "title": "Estimator Type",\n         "type": "string"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "huggingface_model": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Original huggingface model.",\n         "title": "Huggingface Model"\n      },\n      "eole_version": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "0.4.0",\n         "description": "Eole version used to convert/train/save the model.",\n         "title": "Eole Version"\n      },\n      "architecture": {\n         "const": "transformer",\n         "default": "transformer",\n         "title": "Architecture",\n         "type": "string"\n      },\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "$ref": "#/$defs/ActivationFunction",\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "standardFP32",\n            "rms",\n            "gemma-rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-05,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "ffn_layernorm": {\n         "default": false,\n         "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n         "title": "Ffn Layernorm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "query_norm": {\n         "default": false,\n         "title": "Query Norm",\n         "type": "boolean"\n      },\n      "key_norm": {\n         "default": false,\n         "title": "Key Norm",\n         "type": "boolean"\n      },\n      "qk_norm_post_rope": {\n         "default": false,\n         "title": "Qk Norm Post Rope",\n         "type": "boolean"\n      },\n      "add_final_linear_bias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of final_linear in MHA.",\n         "title": "Add Final Linear Bias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "head_dim": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Head dimension when this needs to be different vs hidden_size // heads",\n         "title": "Head Dim"\n      },\n      "attn_scaling": {\n         "anyOf": [\n            {\n               "type": "number"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n         "title": "Attn Scaling"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_shared_experts": {\n         "default": 0,\n         "description": "Number of shared experts for MoE models (DeepSeekv2).",\n         "title": "Num Shared Experts",\n         "type": "integer"\n      },\n      "first_k_dense_replace": {\n         "default": 0,\n         "description": "Number of layers using Dense instead of MoE",\n         "title": "First K Dense Replace",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "moe_softmax_after": {\n         "default": false,\n         "description": "Usually softmax is before topk, Mixtral does it after.",\n         "title": "Moe Softmax After",\n         "type": "boolean"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      },\n      "rope_config": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/RotaryPositionConfig"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Rotary position config, if relevant."\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "quick_gelu",\n            "gated-gelu",\n            "fused-gated-gelu",\n            "gated-silu",\n            "fused-gated-silu",\n            "gelu-tanh",\n            "gated-gelu-tanh",\n            "fused-gated-gelu-tanh"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "CnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnEncoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            },\n            "normalize": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "description": "Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909",\n               "title": "Normalize"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "MeanEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "mean",\n               "default": "mean",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            }\n         },\n         "title": "MeanEncoderConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "default": "rnn",\n               "enum": [\n                  "rnn",\n                  "brnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            }\n         },\n         "title": "RnnEncoderConfig",\n         "type": "object"\n      },\n      "RotaryPositionConfig": {\n         "additionalProperties": false,\n         "description": "Configuration for rotary position embeddings used in transformer models.",\n         "properties": {\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "scaling_type": {\n               "anyOf": [\n                  {\n                     "type": "string"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Specifies the type of RoPE scaling to be applied, if any.",\n               "title": "Scaling Type"\n            },\n            "alpha": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "alpha factor by which to scale rope theta.",\n               "title": "Alpha"\n            },\n            "scaling_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8.0,\n               "description": "Factor by which to scale RoPE embeddings.",\n               "title": "Scaling Factor"\n            },\n            "low_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1.0,\n               "description": "Scaling factor applied to the lower frequency components of RoPE.",\n               "title": "Low Freq Factor"\n            },\n            "high_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 4.0,\n               "description": "Scaling factor applied to the higher frequency components of RoPE.",\n               "title": "High Freq Factor"\n            },\n            "original_max_position_embeddings": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8192,\n               "description": "Original maximum position embeddings for RoPE scaling.",\n               "title": "Original Max Position Embeddings"\n            },\n            "rotary_theta_local": {\n               "default": 10000,\n               "description": "Rotary theta base length for local rotary layers",\n               "title": "Rotary Theta Local",\n               "type": "integer"\n            },\n            "interleave_local": {\n               "default": 0,\n               "description": "Local rotary layers each 1/N layers",\n               "title": "Interleave Local",\n               "type": "integer"\n            },\n            "tmax_index": {\n               "default": 0,\n               "description": "tmax indexing, 0 for all cases except gemma 3 = 1",\n               "title": "Tmax Index",\n               "type": "integer"\n            }\n         },\n         "title": "RotaryPositionConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            },\n            "LM_type": {\n               "default": "causal",\n               "description": "TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)",\n               "enum": [\n                  "causal",\n                  "prefix"\n               ],\n               "title": "Lm Type",\n               "type": "string"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            }\n         },\n         "title": "TransformerEncoderConfig",\n         "type": "object"\n      },\n      "VisionEncoderConfig": {\n         "additionalProperties": false,\n         "description": "Based on mistral-community/pixtral-12b, might evolve later.",\n         "properties": {\n            "encoder_type": {\n               "const": "vision",\n               "default": "vision",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "num_channels": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 3,\n               "title": "Num Channels"\n            },\n            "image_size": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1024,\n               "title": "Image Size"\n            },\n            "patch_size": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 16,\n               "title": "Patch Size"\n            },\n            "image_token_id": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 10,\n               "title": "Image Token Id"\n            },\n            "mm_tokens_per_image": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 256,\n               "title": "Mm Tokens Per Image"\n            },\n            "layernorm_pre": {\n               "default": true,\n               "title": "Layernorm Pre",\n               "type": "boolean"\n            },\n            "patch_conv_bias": {\n               "default": false,\n               "title": "Patch Conv Bias",\n               "type": "boolean"\n            },\n            "encoder_sam": {\n               "default": false,\n               "title": "Encoder Sam",\n               "type": "boolean"\n            },\n            "use_class_embedding": {\n               "default": false,\n               "title": "Use Class Embedding",\n               "type": "boolean"\n            }\n         },\n         "title": "VisionEncoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerModelConfig.architecture",children:(0,t.jsx)(n.code,{children:"architecture (Literal['transformer'])"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validators:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"_validate_transformer"})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerModelConfig.encoder_decoder_type",children:(0,t.jsx)(n.code,{children:"encoder_decoder_type"})})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-architecture--literaltransformer--transformer",children:[(0,t.jsx)(n.em,{children:"field"})," architecture ",(0,t.jsx)(n.em,{children:": Literal['transformer']"})," ",(0,t.jsx)(n.em,{children:"= 'transformer'"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerModelConfig.encoder_decoder_type",children:(0,t.jsx)(n.code,{children:"encoder_decoder_type"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-default_architecture----all-fields-1",children:[(0,t.jsx)(n.em,{children:"validator"})," default_architecture  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.em,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-encoder_decoder_type----all-fields",children:[(0,t.jsx)(n.em,{children:"validator"})," encoder_decoder_type  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.em,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformerlmmodelconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerLMModelConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerConfig",children:(0,t.jsx)(n.code,{children:"TransformerConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig",children:(0,t.jsx)(n.code,{children:"BaseModelConfig"})})]}),"\n",(0,t.jsx)(n.p,{children:"Facilitate setting some transformer specific params at model level."}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerLMModelConfig",\n   "description": "Facilitate setting some transformer specific params at model level.",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "$ref": "#/$defs/EmbeddingsConfig",\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder",\n         "type": "null"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "moe_transformer_ff": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Size of hidden moe transformer feed-forward.",\n         "title": "Moe Transformer Ff"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "generator_bias": {\n         "default": true,\n         "description": "Control whether or not the generator Linear module has bias weights.",\n         "title": "Generator Bias",\n         "type": "boolean"\n      },\n      "adapter_bias": {\n         "default": false,\n         "description": "Control whether or not the adapter module has bias weights.",\n         "title": "Adapter Bias",\n         "type": "boolean"\n      },\n      "projector_activation_fn": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in adapter projector layer."\n      },\n      "spatial_merge_size": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": 1,\n         "description": "Control the presence and size of patch merger (Mistral3)",\n         "title": "Spatial Merge Size"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "estimator_type": {\n         "default": "average",\n         "description": "Which hidden_states to use to feed the estimator",\n         "enum": [\n            "average",\n            "last_token",\n            "first_token"\n         ],\n         "title": "Estimator Type",\n         "type": "string"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "huggingface_model": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Original huggingface model.",\n         "title": "Huggingface Model"\n      },\n      "eole_version": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "0.4.0",\n         "description": "Eole version used to convert/train/save the model.",\n         "title": "Eole Version"\n      },\n      "architecture": {\n         "const": "transformer_lm",\n         "default": "transformer_lm",\n         "title": "Architecture",\n         "type": "string"\n      },\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "$ref": "#/$defs/ActivationFunction",\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "standardFP32",\n            "rms",\n            "gemma-rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-05,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "ffn_layernorm": {\n         "default": false,\n         "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n         "title": "Ffn Layernorm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "query_norm": {\n         "default": false,\n         "title": "Query Norm",\n         "type": "boolean"\n      },\n      "key_norm": {\n         "default": false,\n         "title": "Key Norm",\n         "type": "boolean"\n      },\n      "qk_norm_post_rope": {\n         "default": false,\n         "title": "Qk Norm Post Rope",\n         "type": "boolean"\n      },\n      "add_final_linear_bias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of final_linear in MHA.",\n         "title": "Add Final Linear Bias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "head_dim": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Head dimension when this needs to be different vs hidden_size // heads",\n         "title": "Head Dim"\n      },\n      "attn_scaling": {\n         "anyOf": [\n            {\n               "type": "number"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n         "title": "Attn Scaling"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_shared_experts": {\n         "default": 0,\n         "description": "Number of shared experts for MoE models (DeepSeekv2).",\n         "title": "Num Shared Experts",\n         "type": "integer"\n      },\n      "first_k_dense_replace": {\n         "default": 0,\n         "description": "Number of layers using Dense instead of MoE",\n         "title": "First K Dense Replace",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "moe_softmax_after": {\n         "default": false,\n         "description": "Usually softmax is before topk, Mixtral does it after.",\n         "title": "Moe Softmax After",\n         "type": "boolean"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      },\n      "rope_config": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/RotaryPositionConfig"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Rotary position config, if relevant."\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "quick_gelu",\n            "gated-gelu",\n            "fused-gated-gelu",\n            "gated-silu",\n            "fused-gated-silu",\n            "gelu-tanh",\n            "gated-gelu-tanh",\n            "fused-gated-gelu-tanh"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            },\n            "normalize": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "description": "Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909",\n               "title": "Normalize"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RotaryPositionConfig": {\n         "additionalProperties": false,\n         "description": "Configuration for rotary position embeddings used in transformer models.",\n         "properties": {\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "scaling_type": {\n               "anyOf": [\n                  {\n                     "type": "string"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Specifies the type of RoPE scaling to be applied, if any.",\n               "title": "Scaling Type"\n            },\n            "alpha": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "alpha factor by which to scale rope theta.",\n               "title": "Alpha"\n            },\n            "scaling_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8.0,\n               "description": "Factor by which to scale RoPE embeddings.",\n               "title": "Scaling Factor"\n            },\n            "low_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1.0,\n               "description": "Scaling factor applied to the lower frequency components of RoPE.",\n               "title": "Low Freq Factor"\n            },\n            "high_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 4.0,\n               "description": "Scaling factor applied to the higher frequency components of RoPE.",\n               "title": "High Freq Factor"\n            },\n            "original_max_position_embeddings": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8192,\n               "description": "Original maximum position embeddings for RoPE scaling.",\n               "title": "Original Max Position Embeddings"\n            },\n            "rotary_theta_local": {\n               "default": 10000,\n               "description": "Rotary theta base length for local rotary layers",\n               "title": "Rotary Theta Local",\n               "type": "integer"\n            },\n            "interleave_local": {\n               "default": 0,\n               "description": "Local rotary layers each 1/N layers",\n               "title": "Interleave Local",\n               "type": "integer"\n            },\n            "tmax_index": {\n               "default": 0,\n               "description": "tmax indexing, 0 for all cases except gemma 3 = 1",\n               "title": "Tmax Index",\n               "type": "integer"\n            }\n         },\n         "title": "RotaryPositionConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            },\n            "LM_type": {\n               "default": "causal",\n               "description": "TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)",\n               "enum": [\n                  "causal",\n                  "prefix"\n               ],\n               "title": "Lm Type",\n               "type": "string"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.architecture",children:(0,t.jsx)(n.code,{children:"architecture (Literal['transformer_lm'])"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.encoder",children:(0,t.jsx)(n.code,{children:"encoder (None)"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validators:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"_validate_transformer"})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.encoder_decoder_type",children:(0,t.jsx)(n.code,{children:"encoder_decoder_type"})})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,t.jsx)(n.code,{children:"str_to_dict"})})," \xbb ",(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.encoder",children:(0,t.jsx)(n.code,{children:"encoder"})})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-architecture--literaltransformer_lm--transformer_lm",children:[(0,t.jsx)(n.em,{children:"field"})," architecture ",(0,t.jsx)(n.em,{children:": Literal['transformer_lm']"})," ",(0,t.jsx)(n.em,{children:"= 'transformer_lm'"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.encoder_decoder_type",children:(0,t.jsx)(n.code,{children:"encoder_decoder_type"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-encoder--none--none",children:[(0,t.jsx)(n.em,{children:"field"})," encoder ",(0,t.jsx)(n.em,{children:": None"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Major parameters of an encoder."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_transformer_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.encoder_decoder_type",children:(0,t.jsx)(n.code,{children:"encoder_decoder_type"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,t.jsx)(n.code,{children:"str_to_dict"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-default_architecture----all-fields-2",children:[(0,t.jsx)(n.em,{children:"validator"})," default_architecture  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.em,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-encoder_decoder_type----all-fields-1",children:[(0,t.jsx)(n.em,{children:"validator"})," encoder_decoder_type  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.em,{children:"all fields"})]}),"\n",(0,t.jsx)(n.h2,{id:"rnn",children:"RNN"}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsrnnconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.RnnConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.code,{children:"Config"})]}),"\n",(0,t.jsx)(n.p,{children:"Just to facilitate testing discriminator stuff."}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "RnnConfig",\n   "description": "Just to facilitate testing discriminator stuff.",\n   "type": "object",\n   "properties": {\n      "bridge": {\n         "default": false,\n         "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n         "title": "Bridge",\n         "type": "boolean"\n      },\n      "rnn_type": {\n         "default": "LSTM",\n         "description": "The gate type to use in the RNNs.",\n         "enum": [\n            "LSTM",\n            "GRU"\n         ],\n         "title": "Rnn Type",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.RnnConfig.bridge",children:(0,t.jsx)(n.code,{children:"bridge (bool)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.RnnConfig.rnn_type",children:(0,t.jsx)(n.code,{children:"rnn_type (Literal['LSTM', 'GRU'])"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-bridge--bool--false",children:[(0,t.jsx)(n.em,{children:"field"})," bridge ",(0,t.jsx)(n.em,{children:": bool"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsx)(n.p,{children:"Have an additional layer between the last encoder state and the first decoder state (RNN specific)."}),"\n",(0,t.jsxs)(n.h4,{id:"field-rnn_type--literallstm-gru--lstm",children:[(0,t.jsx)(n.em,{children:"field"})," rnn_type ",(0,t.jsx)(n.em,{children:": Literal['LSTM', 'GRU']"})," ",(0,t.jsx)(n.em,{children:"= 'LSTM'"})]}),"\n",(0,t.jsx)(n.p,{children:"The gate type to use in the RNNs."}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsrnnencoderconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.RnnEncoderConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.RnnConfig",children:(0,t.jsx)(n.code,{children:"RnnConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.EncoderConfig",children:(0,t.jsx)(n.code,{children:"EncoderConfig"})})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "RnnEncoderConfig",\n   "type": "object",\n   "properties": {\n      "encoder_type": {\n         "default": "rnn",\n         "enum": [\n            "rnn",\n            "brnn"\n         ],\n         "title": "Encoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the encoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of encoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "src_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for src.",\n         "title": "Src Word Vec Size",\n         "type": "integer"\n      },\n      "bridge": {\n         "default": false,\n         "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n         "title": "Bridge",\n         "type": "boolean"\n      },\n      "rnn_type": {\n         "default": "LSTM",\n         "description": "The gate type to use in the RNNs.",\n         "enum": [\n            "LSTM",\n            "GRU"\n         ],\n         "title": "Rnn Type",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.RnnEncoderConfig.encoder_type",children:(0,t.jsx)(n.code,{children:"encoder_type (Literal['rnn', 'brnn'])"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-encoder_type--literalrnn-brnn--rnn",children:[(0,t.jsx)(n.em,{children:"field"})," encoder_type ",(0,t.jsx)(n.em,{children:": Literal['rnn', 'brnn']"})," ",(0,t.jsx)(n.em,{children:"= 'rnn'"})]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsrnndecoderconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.RnnDecoderConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.RnnConfig",children:(0,t.jsx)(n.code,{children:"RnnConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig",children:(0,t.jsx)(n.code,{children:"DecoderConfig"})})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "RnnDecoderConfig",\n   "type": "object",\n   "properties": {\n      "decoder_type": {\n         "const": "rnn",\n         "default": "rnn",\n         "title": "Decoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the decoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of decoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "coverage_attn": {\n         "default": false,\n         "description": "Train a coverage attention layer.",\n         "title": "Coverage Attn",\n         "type": "boolean"\n      },\n      "lambda_coverage": {\n         "default": 0.0,\n         "description": "Lambda value for coverage loss of See et al (2017)",\n         "title": "Lambda Coverage",\n         "type": "number"\n      },\n      "global_attention": {\n         "default": "general",\n         "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n         "enum": [\n            "dot",\n            "general",\n            "mlp",\n            null\n         ],\n         "title": "Global Attention"\n      },\n      "global_attention_function": {\n         "default": "softmax",\n         "description": "Global attention function to use.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Global Attention Function",\n         "type": "string"\n      },\n      "bridge": {\n         "default": false,\n         "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n         "title": "Bridge",\n         "type": "boolean"\n      },\n      "rnn_type": {\n         "default": "LSTM",\n         "description": "The gate type to use in the RNNs.",\n         "enum": [\n            "LSTM",\n            "GRU"\n         ],\n         "title": "Rnn Type",\n         "type": "string"\n      },\n      "context_gate": {\n         "default": null,\n         "description": "Type of context gate to use.",\n         "enum": [\n            "source",\n            "target",\n            "both",\n            null\n         ],\n         "title": "Context Gate"\n      },\n      "bidirectional_encoder": {\n         "anyOf": [\n            {\n               "type": "boolean"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": false,\n         "title": "Bidirectional Encoder"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.RnnDecoderConfig.bidirectional_encoder",children:(0,t.jsx)(n.code,{children:"bidirectional_encoder (bool | None)"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.RnnDecoderConfig.context_gate",children:(0,t.jsx)(n.code,{children:"context_gate (Literal['source', 'target', 'both', None])"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.RnnDecoderConfig.decoder_type",children:(0,t.jsx)(n.code,{children:"decoder_type (Literal['rnn'])"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Validators:"})}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-bidirectional_encoder--bool--none--false",children:[(0,t.jsx)(n.em,{children:"field"})," bidirectional_encoder ",(0,t.jsx)(n.em,{children:": bool | None"})," ",(0,t.jsx)(n.em,{children:"= False"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-context_gate--literalsource-target-both-none--none",children:[(0,t.jsx)(n.em,{children:"field"})," context_gate ",(0,t.jsx)(n.em,{children:": Literal['source', 'target', 'both', None]"})," ",(0,t.jsx)(n.em,{children:"= None"})]}),"\n",(0,t.jsx)(n.p,{children:"Type of context gate to use."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-decoder_type--literalrnn--rnn",children:[(0,t.jsx)(n.em,{children:"field"})," decoder_type ",(0,t.jsx)(n.em,{children:": Literal['rnn']"})," ",(0,t.jsx)(n.em,{children:"= 'rnn'"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsrnnmodelconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.RnnModelConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.RnnConfig",children:(0,t.jsx)(n.code,{children:"RnnConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig",children:(0,t.jsx)(n.code,{children:"BaseModelConfig"})})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "RnnModelConfig",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "$ref": "#/$defs/EmbeddingsConfig",\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "brnn": "#/$defs/RnnEncoderConfig",\n                     "cnn": "#/$defs/CnnEncoderConfig",\n                     "mean": "#/$defs/MeanEncoderConfig",\n                     "rnn": "#/$defs/RnnEncoderConfig",\n                     "transformer": "#/$defs/TransformerEncoderConfig",\n                     "vision": "#/$defs/VisionEncoderConfig"\n                  },\n                  "propertyName": "encoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/MeanEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/VisionEncoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": -1,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "moe_transformer_ff": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Size of hidden moe transformer feed-forward.",\n         "title": "Moe Transformer Ff"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "generator_bias": {\n         "default": true,\n         "description": "Control whether or not the generator Linear module has bias weights.",\n         "title": "Generator Bias",\n         "type": "boolean"\n      },\n      "adapter_bias": {\n         "default": false,\n         "description": "Control whether or not the adapter module has bias weights.",\n         "title": "Adapter Bias",\n         "type": "boolean"\n      },\n      "projector_activation_fn": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in adapter projector layer."\n      },\n      "spatial_merge_size": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": 1,\n         "description": "Control the presence and size of patch merger (Mistral3)",\n         "title": "Spatial Merge Size"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "estimator_type": {\n         "default": "average",\n         "description": "Which hidden_states to use to feed the estimator",\n         "enum": [\n            "average",\n            "last_token",\n            "first_token"\n         ],\n         "title": "Estimator Type",\n         "type": "string"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "huggingface_model": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Original huggingface model.",\n         "title": "Huggingface Model"\n      },\n      "eole_version": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "0.4.0",\n         "description": "Eole version used to convert/train/save the model.",\n         "title": "Eole Version"\n      },\n      "architecture": {\n         "const": "rnn",\n         "default": "rnn",\n         "title": "Architecture",\n         "type": "string"\n      },\n      "bridge": {\n         "default": false,\n         "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n         "title": "Bridge",\n         "type": "boolean"\n      },\n      "rnn_type": {\n         "default": "LSTM",\n         "description": "The gate type to use in the RNNs.",\n         "enum": [\n            "LSTM",\n            "GRU"\n         ],\n         "title": "Rnn Type",\n         "type": "string"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "quick_gelu",\n            "gated-gelu",\n            "fused-gated-gelu",\n            "gated-silu",\n            "fused-gated-silu",\n            "gelu-tanh",\n            "gated-gelu-tanh",\n            "fused-gated-gelu-tanh"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "CnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnEncoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            },\n            "normalize": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "description": "Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909",\n               "title": "Normalize"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "MeanEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "mean",\n               "default": "mean",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            }\n         },\n         "title": "MeanEncoderConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "default": "rnn",\n               "enum": [\n                  "rnn",\n                  "brnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            }\n         },\n         "title": "RnnEncoderConfig",\n         "type": "object"\n      },\n      "RotaryPositionConfig": {\n         "additionalProperties": false,\n         "description": "Configuration for rotary position embeddings used in transformer models.",\n         "properties": {\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "scaling_type": {\n               "anyOf": [\n                  {\n                     "type": "string"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Specifies the type of RoPE scaling to be applied, if any.",\n               "title": "Scaling Type"\n            },\n            "alpha": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "alpha factor by which to scale rope theta.",\n               "title": "Alpha"\n            },\n            "scaling_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8.0,\n               "description": "Factor by which to scale RoPE embeddings.",\n               "title": "Scaling Factor"\n            },\n            "low_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1.0,\n               "description": "Scaling factor applied to the lower frequency components of RoPE.",\n               "title": "Low Freq Factor"\n            },\n            "high_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 4.0,\n               "description": "Scaling factor applied to the higher frequency components of RoPE.",\n               "title": "High Freq Factor"\n            },\n            "original_max_position_embeddings": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8192,\n               "description": "Original maximum position embeddings for RoPE scaling.",\n               "title": "Original Max Position Embeddings"\n            },\n            "rotary_theta_local": {\n               "default": 10000,\n               "description": "Rotary theta base length for local rotary layers",\n               "title": "Rotary Theta Local",\n               "type": "integer"\n            },\n            "interleave_local": {\n               "default": 0,\n               "description": "Local rotary layers each 1/N layers",\n               "title": "Interleave Local",\n               "type": "integer"\n            },\n            "tmax_index": {\n               "default": 0,\n               "description": "tmax indexing, 0 for all cases except gemma 3 = 1",\n               "title": "Tmax Index",\n               "type": "integer"\n            }\n         },\n         "title": "RotaryPositionConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            },\n            "LM_type": {\n               "default": "causal",\n               "description": "TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)",\n               "enum": [\n                  "causal",\n                  "prefix"\n               ],\n               "title": "Lm Type",\n               "type": "string"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            }\n         },\n         "title": "TransformerEncoderConfig",\n         "type": "object"\n      },\n      "VisionEncoderConfig": {\n         "additionalProperties": false,\n         "description": "Based on mistral-community/pixtral-12b, might evolve later.",\n         "properties": {\n            "encoder_type": {\n               "const": "vision",\n               "default": "vision",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "num_channels": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 3,\n               "title": "Num Channels"\n            },\n            "image_size": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1024,\n               "title": "Image Size"\n            },\n            "patch_size": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 16,\n               "title": "Patch Size"\n            },\n            "image_token_id": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 10,\n               "title": "Image Token Id"\n            },\n            "mm_tokens_per_image": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 256,\n               "title": "Mm Tokens Per Image"\n            },\n            "layernorm_pre": {\n               "default": true,\n               "title": "Layernorm Pre",\n               "type": "boolean"\n            },\n            "patch_conv_bias": {\n               "default": false,\n               "title": "Patch Conv Bias",\n               "type": "boolean"\n            },\n            "encoder_sam": {\n               "default": false,\n               "title": "Encoder Sam",\n               "type": "boolean"\n            },\n            "use_class_embedding": {\n               "default": false,\n               "title": "Use Class Embedding",\n               "type": "boolean"\n            }\n         },\n         "title": "VisionEncoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.RnnModelConfig.architecture",children:(0,t.jsx)(n.code,{children:"architecture (Literal['rnn'])"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validators:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.RnnModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.RnnModelConfig.encoder_decoder_type",children:(0,t.jsx)(n.code,{children:"encoder_decoder_type"})})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-architecture--literalrnn--rnn",children:[(0,t.jsx)(n.em,{children:"field"})," architecture ",(0,t.jsx)(n.em,{children:": Literal['rnn']"})," ",(0,t.jsx)(n.em,{children:"= 'rnn'"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.RnnModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.RnnModelConfig.encoder_decoder_type",children:(0,t.jsx)(n.code,{children:"encoder_decoder_type"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-default_architecture----all-fields-3",children:[(0,t.jsx)(n.em,{children:"validator"})," default_architecture  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.em,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-encoder_decoder_type----all-fields-2",children:[(0,t.jsx)(n.em,{children:"validator"})," encoder_decoder_type  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.em,{children:"all fields"})]}),"\n",(0,t.jsx)(n.h2,{id:"cnn",children:"CNN"}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelscnnconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.CnnConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.code,{children:"Config"})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "CnnConfig",\n   "type": "object",\n   "properties": {\n      "cnn_kernel_width": {\n         "default": 3,\n         "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n         "title": "Cnn Kernel Width",\n         "type": "integer"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.CnnConfig.cnn_kernel_width",children:(0,t.jsx)(n.code,{children:"cnn_kernel_width (int)"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-cnn_kernel_width--int--3",children:[(0,t.jsx)(n.em,{children:"field"})," cnn_kernel_width ",(0,t.jsx)(n.em,{children:": int"})," ",(0,t.jsx)(n.em,{children:"= 3"})]}),"\n",(0,t.jsx)(n.p,{children:"Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers."}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelscnnencoderconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.CnnEncoderConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.CnnConfig",children:(0,t.jsx)(n.code,{children:"CnnConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.EncoderConfig",children:(0,t.jsx)(n.code,{children:"EncoderConfig"})})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "CnnEncoderConfig",\n   "type": "object",\n   "properties": {\n      "encoder_type": {\n         "const": "cnn",\n         "default": "cnn",\n         "title": "Encoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the encoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of encoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "src_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for src.",\n         "title": "Src Word Vec Size",\n         "type": "integer"\n      },\n      "cnn_kernel_width": {\n         "default": 3,\n         "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n         "title": "Cnn Kernel Width",\n         "type": "integer"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.CnnEncoderConfig.encoder_type",children:(0,t.jsx)(n.code,{children:"encoder_type (Literal['cnn'])"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-encoder_type--literalcnn--cnn",children:[(0,t.jsx)(n.em,{children:"field"})," encoder_type ",(0,t.jsx)(n.em,{children:": Literal['cnn']"})," ",(0,t.jsx)(n.em,{children:"= 'cnn'"})]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelscnndecoderconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.CnnDecoderConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.CnnConfig",children:(0,t.jsx)(n.code,{children:"CnnConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.DecoderConfig",children:(0,t.jsx)(n.code,{children:"DecoderConfig"})})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "CnnDecoderConfig",\n   "type": "object",\n   "properties": {\n      "decoder_type": {\n         "const": "cnn",\n         "default": "cnn",\n         "title": "Decoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the decoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of decoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "coverage_attn": {\n         "default": false,\n         "description": "Train a coverage attention layer.",\n         "title": "Coverage Attn",\n         "type": "boolean"\n      },\n      "lambda_coverage": {\n         "default": 0.0,\n         "description": "Lambda value for coverage loss of See et al (2017)",\n         "title": "Lambda Coverage",\n         "type": "number"\n      },\n      "global_attention": {\n         "default": "general",\n         "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n         "enum": [\n            "dot",\n            "general",\n            "mlp",\n            null\n         ],\n         "title": "Global Attention"\n      },\n      "global_attention_function": {\n         "default": "softmax",\n         "description": "Global attention function to use.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Global Attention Function",\n         "type": "string"\n      },\n      "cnn_kernel_width": {\n         "default": 3,\n         "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n         "title": "Cnn Kernel Width",\n         "type": "integer"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.CnnDecoderConfig.decoder_type",children:(0,t.jsx)(n.code,{children:"decoder_type (Literal['cnn'])"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Validators:"})}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-decoder_type--literalcnn--cnn",children:[(0,t.jsx)(n.em,{children:"field"})," decoder_type ",(0,t.jsx)(n.em,{children:": Literal['cnn']"})," ",(0,t.jsx)(n.em,{children:"= 'cnn'"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelscnnmodelconfig",children:[(0,t.jsx)(n.em,{children:"pydantic model"})," eole.config.models.CnnModelConfig"]}),"\n",(0,t.jsxs)(n.p,{children:["Bases: ",(0,t.jsx)(n.a,{href:"#eole.config.models.CnnConfig",children:(0,t.jsx)(n.code,{children:"CnnConfig"})}),", ",(0,t.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig",children:(0,t.jsx)(n.code,{children:"BaseModelConfig"})})]}),"\n",(0,t.jsx)(n.p,{}),(0,t.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,t.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n   "title": "CnnModelConfig",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "$ref": "#/$defs/EmbeddingsConfig",\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "brnn": "#/$defs/RnnEncoderConfig",\n                     "cnn": "#/$defs/CnnEncoderConfig",\n                     "mean": "#/$defs/MeanEncoderConfig",\n                     "rnn": "#/$defs/RnnEncoderConfig",\n                     "transformer": "#/$defs/TransformerEncoderConfig",\n                     "vision": "#/$defs/VisionEncoderConfig"\n                  },\n                  "propertyName": "encoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/MeanEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/VisionEncoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": -1,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "moe_transformer_ff": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Size of hidden moe transformer feed-forward.",\n         "title": "Moe Transformer Ff"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "generator_bias": {\n         "default": true,\n         "description": "Control whether or not the generator Linear module has bias weights.",\n         "title": "Generator Bias",\n         "type": "boolean"\n      },\n      "adapter_bias": {\n         "default": false,\n         "description": "Control whether or not the adapter module has bias weights.",\n         "title": "Adapter Bias",\n         "type": "boolean"\n      },\n      "projector_activation_fn": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in adapter projector layer."\n      },\n      "spatial_merge_size": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": 1,\n         "description": "Control the presence and size of patch merger (Mistral3)",\n         "title": "Spatial Merge Size"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "estimator_type": {\n         "default": "average",\n         "description": "Which hidden_states to use to feed the estimator",\n         "enum": [\n            "average",\n            "last_token",\n            "first_token"\n         ],\n         "title": "Estimator Type",\n         "type": "string"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "huggingface_model": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Original huggingface model.",\n         "title": "Huggingface Model"\n      },\n      "eole_version": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "0.4.0",\n         "description": "Eole version used to convert/train/save the model.",\n         "title": "Eole Version"\n      },\n      "architecture": {\n         "const": "cnn",\n         "default": "cnn",\n         "title": "Architecture",\n         "type": "string"\n      },\n      "cnn_kernel_width": {\n         "default": 3,\n         "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n         "title": "Cnn Kernel Width",\n         "type": "integer"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "quick_gelu",\n            "gated-gelu",\n            "fused-gated-gelu",\n            "gated-silu",\n            "fused-gated-silu",\n            "gelu-tanh",\n            "gated-gelu-tanh",\n            "fused-gated-gelu-tanh"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "CnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnEncoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            },\n            "normalize": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "description": "Enable embeddings scaling. Not always necessary, but useful for some model compatibility, e.g. gemma. https://datascience.stackexchange.com/a/87909",\n               "title": "Normalize"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "MeanEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "mean",\n               "default": "mean",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            }\n         },\n         "title": "MeanEncoderConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "default": "rnn",\n               "enum": [\n                  "rnn",\n                  "brnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            }\n         },\n         "title": "RnnEncoderConfig",\n         "type": "object"\n      },\n      "RotaryPositionConfig": {\n         "additionalProperties": false,\n         "description": "Configuration for rotary position embeddings used in transformer models.",\n         "properties": {\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True= Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "scaling_type": {\n               "anyOf": [\n                  {\n                     "type": "string"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Specifies the type of RoPE scaling to be applied, if any.",\n               "title": "Scaling Type"\n            },\n            "alpha": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "alpha factor by which to scale rope theta.",\n               "title": "Alpha"\n            },\n            "scaling_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8.0,\n               "description": "Factor by which to scale RoPE embeddings.",\n               "title": "Scaling Factor"\n            },\n            "low_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1.0,\n               "description": "Scaling factor applied to the lower frequency components of RoPE.",\n               "title": "Low Freq Factor"\n            },\n            "high_freq_factor": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 4.0,\n               "description": "Scaling factor applied to the higher frequency components of RoPE.",\n               "title": "High Freq Factor"\n            },\n            "original_max_position_embeddings": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 8192,\n               "description": "Original maximum position embeddings for RoPE scaling.",\n               "title": "Original Max Position Embeddings"\n            },\n            "rotary_theta_local": {\n               "default": 10000,\n               "description": "Rotary theta base length for local rotary layers",\n               "title": "Rotary Theta Local",\n               "type": "integer"\n            },\n            "interleave_local": {\n               "default": 0,\n               "description": "Local rotary layers each 1/N layers",\n               "title": "Interleave Local",\n               "type": "integer"\n            },\n            "tmax_index": {\n               "default": 0,\n               "description": "tmax indexing, 0 for all cases except gemma 3 = 1",\n               "title": "Tmax Index",\n               "type": "integer"\n            }\n         },\n         "title": "RotaryPositionConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            },\n            "LM_type": {\n               "default": "causal",\n               "description": "TransformerDecoder LM type (causal = classic, or prefix LM https://arxiv.org/pdf/2308.06912)",\n               "enum": [\n                  "causal",\n                  "prefix"\n               ],\n               "title": "Lm Type",\n               "type": "string"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            }\n         },\n         "title": "TransformerEncoderConfig",\n         "type": "object"\n      },\n      "VisionEncoderConfig": {\n         "additionalProperties": false,\n         "description": "Based on mistral-community/pixtral-12b, might evolve later.",\n         "properties": {\n            "encoder_type": {\n               "const": "vision",\n               "default": "vision",\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "moe_transformer_ff": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Size of hidden moe transformer feed-forward.",\n               "title": "Moe Transformer Ff"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "$ref": "#/$defs/ActivationFunction",\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "standardFP32",\n                  "rms",\n                  "gemma-rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-05,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "ffn_layernorm": {\n               "default": false,\n               "description": "Add pre/post_feedforward_layernorm around MLP forward. Note: introduced for gemma2 support.",\n               "title": "Ffn Layernorm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too by default. Can be disabled with `add_final_linear_bias`.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "query_norm": {\n               "default": false,\n               "title": "Query Norm",\n               "type": "boolean"\n            },\n            "key_norm": {\n               "default": false,\n               "title": "Key Norm",\n               "type": "boolean"\n            },\n            "qk_norm_post_rope": {\n               "default": false,\n               "title": "Qk Norm Post Rope",\n               "type": "boolean"\n            },\n            "add_final_linear_bias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of final_linear in MHA.",\n               "title": "Add Final Linear Bias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "head_dim": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Head dimension when this needs to be different vs hidden_size // heads",\n               "title": "Head Dim"\n            },\n            "attn_scaling": {\n               "anyOf": [\n                  {\n                     "type": "number"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Attention scaling factor, when None uses 1/sqrt(head_dim) by default",\n               "title": "Attn Scaling"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_shared_experts": {\n               "default": 0,\n               "description": "Number of shared experts for MoE models (DeepSeekv2).",\n               "title": "Num Shared Experts",\n               "type": "integer"\n            },\n            "first_k_dense_replace": {\n               "default": 0,\n               "description": "Number of layers using Dense instead of MoE",\n               "title": "First K Dense Replace",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "moe_softmax_after": {\n               "default": false,\n               "description": "Usually softmax is before topk, Mixtral does it after.",\n               "title": "Moe Softmax After",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "rope_config": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/RotaryPositionConfig"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Rotary position config, if relevant."\n            },\n            "num_channels": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 3,\n               "title": "Num Channels"\n            },\n            "image_size": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 1024,\n               "title": "Image Size"\n            },\n            "patch_size": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 16,\n               "title": "Patch Size"\n            },\n            "image_token_id": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 10,\n               "title": "Image Token Id"\n            },\n            "mm_tokens_per_image": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 256,\n               "title": "Mm Tokens Per Image"\n            },\n            "layernorm_pre": {\n               "default": true,\n               "title": "Layernorm Pre",\n               "type": "boolean"\n            },\n            "patch_conv_bias": {\n               "default": false,\n               "title": "Patch Conv Bias",\n               "type": "boolean"\n            },\n            "encoder_sam": {\n               "default": false,\n               "title": "Encoder Sam",\n               "type": "boolean"\n            },\n            "use_class_embedding": {\n               "default": false,\n               "title": "Use Class Embedding",\n               "type": "boolean"\n            }\n         },\n         "title": "VisionEncoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,t.jsx)(n.p,{}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Config:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"validate_default"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,t.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"extra"}),": ",(0,t.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,t.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fields:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.CnnModelConfig.architecture",children:(0,t.jsx)(n.code,{children:"architecture (Literal['cnn'])"})})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validators:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.CnnModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#eole.config.models.CnnModelConfig.encoder_decoder_type",children:(0,t.jsx)(n.code,{children:"encoder_decoder_type"})})," \xbb ",(0,t.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"field-architecture--literalcnn--cnn",children:[(0,t.jsx)(n.em,{children:"field"})," architecture ",(0,t.jsx)(n.em,{children:": Literal['cnn']"})," ",(0,t.jsx)(n.em,{children:"= 'cnn'"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_override_values"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.CnnModelConfig.default_architecture",children:(0,t.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#eole.config.models.CnnModelConfig.encoder_decoder_type",children:(0,t.jsx)(n.code,{children:"encoder_decoder_type"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-default_architecture----all-fields-4",children:[(0,t.jsx)(n.em,{children:"validator"})," default_architecture  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.em,{children:"all fields"})]}),"\n",(0,t.jsxs)(n.h4,{id:"validator-encoder_decoder_type----all-fields-3",children:[(0,t.jsx)(n.em,{children:"validator"})," encoder_decoder_type  ",(0,t.jsx)(n.em,{children:"\xbb"}),"  ",(0,t.jsx)(n.em,{children:"all fields"})]})]})}function f(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>d});var t=i(6540);const r={},o=t.createContext(r);function l(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);