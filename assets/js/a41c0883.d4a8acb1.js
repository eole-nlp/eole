"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[1795],{4583:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>c,toc:()=>a});var t=s(4848),o=s(8453);const r={},i="Translation WMT17 en-de",c={id:"recipes/wmt17/README",title:"Translation WMT17 en-de",description:"---",source:"@site/docs/recipes/wmt17/README.md",sourceDirName:"recipes/wmt17",slug:"/recipes/wmt17/",permalink:"/eole/docs/recipes/wmt17/",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/recipes/wmt17/README.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Language Model Wiki-103",permalink:"/eole/docs/recipes/wiki_103/"},next:{title:"TowerInstruct (Mistral)",permalink:"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/"}},d={},a=[{value:"<strong>NOTE</strong>\nTo make your life easier, run these commands from the recipe directory (here <code>recipes/wmt17</code>).",id:"noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipeswmt17",level:2},{value:"Tokenization methods",id:"tokenization-methods",level:3},{value:"Get Data and prepare",id:"get-data-and-prepare",level:3},{value:"Train",id:"train",level:3}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"translation-wmt17-en-de",children:"Translation WMT17 en-de"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.h2,{id:"noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipeswmt17",children:[(0,t.jsx)(n.strong,{children:"NOTE"}),"\nTo make your life easier, run these commands from the recipe directory (here ",(0,t.jsx)(n.code,{children:"recipes/wmt17"}),")."]}),"\n",(0,t.jsx)(n.h3,{id:"tokenization-methods",children:"Tokenization methods"}),"\n",(0,t.jsx)(n.p,{children:"The following configurations as provided as example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"wmt17_ende_yaml"}),': "legacy" configuration, using already tokenized data;']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"wmt17_ende_bpe.yaml"}),': on-the-fly bpe tokenization, using the "official" ',(0,t.jsx)(n.code,{children:"subword-nmt"})," based ",(0,t.jsx)(n.code,{children:"bpe"})," transform;"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"wmt17_ende_bpe_onmt_tokenize.yaml"}),": on-the-fly bpe tokenization, using the ",(0,t.jsx)(n.code,{children:"pyonmttok"})," based ",(0,t.jsx)(n.code,{children:"onmt_tokenize"})," transform;"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"wmt17_ende_spm.yaml"}),": on-the-fly sentencepiece tokenization, using the official ",(0,t.jsx)(n.code,{children:"sentencepiece"})," based ",(0,t.jsx)(n.code,{children:"sentencepiece"})," transform;"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"wmt17_ende_spm_onmt_tokenize.yaml"}),": on-the-fly sentencepiece tokenization, using the ",(0,t.jsx)(n.code,{children:"pyonmttok"})," based ",(0,t.jsx)(n.code,{children:"onmt_tokenize"})," transform;"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"get-data-and-prepare",children:"Get Data and prepare"}),"\n",(0,t.jsx)(n.p,{children:"WMT17 English-German data set:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd recipes/wmt17\nbash prepare_wmt_ende_data.sh\n"})}),"\n",(0,t.jsx)(n.p,{children:"Options:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--method"}),": ",(0,t.jsx)(n.code,{children:"bpe"}),"/",(0,t.jsx)(n.code,{children:"sentencepiece"})," (subwords method to use)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"--encode"}),": ",(0,t.jsx)(n.code,{children:"true"}),"/",(0,t.jsx)(n.code,{children:"false"})," (tokenize all datasets, not necessary if using on the fly transforms)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["If you want to use one of the aforementioned configurations with on-the-fly transforms, set ",(0,t.jsx)(n.code,{children:"--encode false"}),", and either of ",(0,t.jsx)(n.code,{children:"--method bpe"}),"/",(0,t.jsx)(n.code,{children:"--method sentecepiece"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"train",children:"Train"}),"\n",(0,t.jsx)(n.p,{children:"Choose the config you want to run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'export CONFIG="wmt17_ende_bpe.yaml"\n'})}),"\n",(0,t.jsx)(n.p,{children:"Training the following big transformer for 50K steps takes less than 10 hours on a single RTX 4090"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"eole build_vocab --config $CONFIG --n_sample -1 # --num_threads 4\neole train --config $CONFIG\n"})}),"\n",(0,t.jsxs)(n.p,{children:['Note: if you need to perform some visual checks on the "transformed" data, you can enable the ',(0,t.jsx)(n.code,{children:"dump_samples"})," flag at the ",(0,t.jsx)(n.code,{children:"build_vocab"})," stage (and specify a smaller ",(0,t.jsx)(n.code,{children:"-n_sample"})," for efficiency)."]}),"\n",(0,t.jsx)(n.p,{children:"Translate test sets with various settings on local GPU and CPUs."}),"\n",(0,t.jsx)(n.p,{children:"Notes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"the exact model path depends on the config you chose. You can check your logs for the exact path."}),"\n",(0,t.jsxs)(n.li,{children:['the "root" model links to the last saved step, but you can choose any step subfolder if needed (e.g. ',(0,t.jsx)(n.code,{children:"--model_path wmt17_en_de/transformer_big_bpe/step_10000"}),")"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"eole predict --src wmt17_en_de/test.src.bpe --model_path wmt17_en_de/transformer_big_bpe --beam_size 5 --batch_size 4096 --batch_type tokens --output wmt17_en_de/pred.trg.bpe --gpu 0\nsed -re 's/@@( |$)//g' < wmt17_en_de/pred.trg.bpe > wmt17_en_de/pred.trg.tok\nsacrebleu -tok none wmt17_en_de/test.trg < wmt17_en_de/pred.trg.tok\n"})}),"\n",(0,t.jsx)(n.p,{children:"BLEU scored at 40K, 45K, 50K steps on the test set (Newstest2016)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'{\n "name": "BLEU",\n "score": 35.4,\n "signature": "nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.0.0",\n "verbose_score": "66.2/41.3/28.5/20.3 (BP = 0.998 ratio = 0.998 hyp_len = 64244 ref_len = 64379)",\n "nrefs": "1",\n "case": "mixed",\n "eff": "no",\n "tok": "none",\n "smooth": "exp",\n "version": "2.0.0"\n}\n{\n "name": "BLEU",\n "score": 35.2,\n "signature": "nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.0.0",\n "verbose_score": "65.9/41.0/28.3/20.2 (BP = 1.000 ratio = 1.000 hyp_len = 64357 ref_len = 64379)",\n "nrefs": "1",\n "case": "mixed",\n "eff": "no",\n "tok": "none",\n "smooth": "exp",\n "version": "2.0.0"\n}\n{\n "name": "BLEU",\n "score": 35.1,\n "signature": "nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.0.0",\n "verbose_score": "66.2/41.2/28.4/20.3 (BP = 0.992 ratio = 0.992 hyp_len = 63885 ref_len = 64379)",\n "nrefs": "1",\n "case": "mixed",\n "eff": "no",\n "tok": "none",\n "smooth": "exp",\n "version": "2.0.0"\n}\n\n'})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>c});var t=s(6540);const o={},r=t.createContext(o);function i(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);