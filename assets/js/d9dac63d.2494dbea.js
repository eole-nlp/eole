"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[7881],{3084:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>a});var l=i(4848),t=i(8453);const r={},o="Inference",s={id:"reference/Config/inference",title:"Inference",description:"pydantic model eole.config.inference.DecodingConfig[source]",source:"@site/docs/reference/Config/inference.md",sourceDirName:"reference/Config",slug:"/reference/Config/inference",permalink:"/eole/docs/reference/Config/inference",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/reference/Config/inference.md",tags:[],version:"current",frontMatter:{},sidebar:"apiSidebar",previous:{title:"Data",permalink:"/eole/docs/reference/Config/data"},next:{title:"Models",permalink:"/eole/docs/reference/Config/models"}},d={},a=[{value:"<em>pydantic model</em> eole.config.inference.DecodingConfig[source]",id:"pydantic-model-eoleconfiginferencedecodingconfigsource",level:3},{value:"<em>field</em> align_debug <em>: bool</em> <em>= False</em>",id:"field-align_debug--bool--false",level:4},{value:"<em>field</em> alpha <em>: float</em> <em>= 1.0</em>",id:"field-alpha--float--10",level:4},{value:"<em>field</em> attn_debug <em>: bool</em> <em>= False</em>",id:"field-attn_debug--bool--false",level:4},{value:"<em>field</em> ban_unk_token <em>: bool</em> <em>= False</em>",id:"field-ban_unk_token--bool--false",level:4},{value:"<em>field</em> beam_size <em>: int</em> <em>= 5</em>",id:"field-beam_size--int--5",level:4},{value:"<em>field</em> beta <em>: float</em> <em>= -0.0</em>",id:"field-beta--float---00",level:4},{value:"<em>field</em> block_ngram_repeat <em>: int</em> <em>= 0</em>",id:"field-block_ngram_repeat--int--0",level:4},{value:"<em>field</em> coverage_penalty <em>: Literal[&#39;none&#39;, &#39;wu&#39;, &#39;summary&#39;]</em> <em>= &#39;none&#39;</em>",id:"field-coverage_penalty--literalnone-wu-summary--none",level:4},{value:"<em>field</em> dump_beam <em>: str</em> <em>= &#39;&#39;</em>",id:"field-dump_beam--str--",level:4},{value:"<em>field</em> ignore_when_blocking <em>: List[str]</em> <em>= []</em>",id:"field-ignore_when_blocking--liststr--",level:4},{value:"<em>field</em> length_penalty <em>: Literal[&#39;avg&#39;, &#39;wu&#39;, &#39;none&#39;]</em> <em>= &#39;avg&#39;</em>",id:"field-length_penalty--literalavg-wu-none--avg",level:4},{value:"<em>field</em> max_length <em>: int</em> <em>= 250</em>",id:"field-max_length--int--250",level:4},{value:"<em>field</em> max_length_ratio <em>: float</em> <em>= 2</em>",id:"field-max_length_ratio--float--2",level:4},{value:"<em>field</em> min_length <em>: int</em> <em>= 0</em>",id:"field-min_length--int--0",level:4},{value:"<em>field</em> n_best <em>: int</em> <em>= 1</em>",id:"field-n_best--int--1",level:4},{value:"<em>field</em> phrase_table <em>: str</em> <em>= &#39;&#39;</em>",id:"field-phrase_table--str--",level:4},{value:"<em>field</em> ratio <em>: float</em> <em>= -0.0</em>",id:"field-ratio--float---00",level:4},{value:"<em>field</em> replace_unk <em>: bool</em> <em>= False</em>",id:"field-replace_unk--bool--false",level:4},{value:"<em>field</em> stepwise_penalty <em>: bool</em> <em>= False</em>",id:"field-stepwise_penalty--bool--false",level:4},{value:"<em>field</em> temperature <em>: float</em> <em>= 1.0</em>",id:"field-temperature--float--10",level:4},{value:"<em>field</em> top_k <em>: int</em> <em>= 0</em>",id:"field-top_k--int--0",level:4},{value:"<em>field</em> top_p <em>: float</em> <em>= 0.0</em>",id:"field-top_p--float--00",level:4},{value:"<em>field</em> verbose <em>: bool</em> <em>= False</em>",id:"field-verbose--bool--false",level:4},{value:"<em>field</em> with_score <em>: bool</em> <em>= False</em>",id:"field-with_score--bool--false",level:4},{value:"<em>pydantic model</em> eole.config.inference.InferenceConfig[source]",id:"pydantic-model-eoleconfiginferenceinferenceconfigsource",level:3},{value:"<em>field</em> avg_raw_probs <em>: bool</em> <em>= False</em>",id:"field-avg_raw_probs--bool--false",level:4},{value:"<em>field</em> batch_size <em>: int</em> <em>= 30</em>",id:"field-batch_size--int--30",level:4},{value:"<em>field</em> batch_type <em>: Literal[&#39;sents&#39;, &#39;tokens&#39;]</em> <em>= &#39;sents&#39;</em>",id:"field-batch_type--literalsents-tokens--sents",level:4},{value:"<em>field</em> chat_template <em>: str | None</em> <em>= None</em>",id:"field-chat_template--str--none--none",level:4},{value:"<em>field</em> data_type <em>: str | None</em> <em>= &#39;text&#39;</em>",id:"field-data_type--str--none--text",level:4},{value:"<em>field</em> gold_align <em>: bool</em> <em>= False</em>",id:"field-gold_align--bool--false",level:4},{value:"<em>field</em> optional_eos <em>: List[str] | None</em> <em>= []</em>",id:"field-optional_eos--liststr--none--",level:4},{value:"<em>field</em> profile <em>: bool</em> <em>= False</em>",id:"field-profile--bool--false",level:4},{value:"<em>field</em> report_align <em>: bool</em> <em>= False</em>",id:"field-report_align--bool--false",level:4},{value:"<em>field</em> report_time <em>: bool</em> <em>= False</em>",id:"field-report_time--bool--false",level:4},{value:"get_model_path()[source]",id:"get_model_pathsource",level:4},{value:"<em>property</em> storage_dtype <em>: dtype</em>[source]",id:"property-storage_dtype--dtypesource",level:4}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",details:"details",em:"em",h1:"h1",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",summary:"summary",ul:"ul",...(0,t.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.h1,{id:"inference",children:"Inference"}),"\n",(0,l.jsxs)(n.h3,{id:"pydantic-model-eoleconfiginferencedecodingconfigsource",children:[(0,l.jsx)(n.em,{children:"pydantic model"})," eole.config.inference.DecodingConfig",(0,l.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/inference.py#L9-L99",children:"[source]"})]}),"\n",(0,l.jsxs)(n.p,{children:["Bases: ",(0,l.jsx)(n.code,{children:"Config"})]}),"\n",(0,l.jsx)(n.p,{}),(0,l.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,l.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-json",children:'{\n   "title": "DecodingConfig",\n   "type": "object",\n   "properties": {\n      "beam_size": {\n         "default": 5,\n         "description": "Beam size.",\n         "title": "Beam Size",\n         "type": "integer"\n      },\n      "ratio": {\n         "default": -0.0,\n         "description": "Ratio based beam stop condition.",\n         "title": "Ratio",\n         "type": "number"\n      },\n      "top_k": {\n         "default": 0,\n         "description": "Set this to -1 to do random sampling from full distribution. Set this to value k>1 to do random sampling restricted to the k most likely next tokens. Set this to 1 to use argmax.",\n         "title": "Top K",\n         "type": "integer"\n      },\n      "top_p": {\n         "default": 0.0,\n         "description": "Probability for top-p/nucleus sampling. Restrict tokens to the most likely until the cumulated probability is over p. In range [0,1]. (https://arxiv.org/abs/1904.09751)",\n         "lte": 1.0,\n         "minimum": 0.0,\n         "title": "Top P",\n         "type": "number"\n      },\n      "temperature": {\n         "default": 1.0,\n         "description": "If doing random sampling, divide the logits by this before computing softmax during decoding.",\n         "title": "Temperature",\n         "type": "number"\n      },\n      "length_penalty": {\n         "default": "avg",\n         "description": "Length penalty to use.",\n         "enum": [\n            "avg",\n            "wu",\n            "none"\n         ],\n         "title": "Length Penalty",\n         "type": "string"\n      },\n      "alpha": {\n         "default": 1.0,\n         "description": "Length penalty parameter (higher = longer generation)",\n         "title": "Alpha",\n         "type": "number"\n      },\n      "coverage_penalty": {\n         "default": "none",\n         "description": "Coverage penalty to use. Only available in beam search.",\n         "enum": [\n            "none",\n            "wu",\n            "summary"\n         ],\n         "title": "Coverage Penalty",\n         "type": "string"\n      },\n      "beta": {\n         "default": -0.0,\n         "description": "Coverage penalty parameter.",\n         "title": "Beta",\n         "type": "number"\n      },\n      "stepwise_penalty": {\n         "default": false,\n         "description": "Apply coverage penalty at every decoding step. Helpful for summary penalty.",\n         "title": "Stepwise Penalty",\n         "type": "boolean"\n      },\n      "min_length": {\n         "default": 0,\n         "description": "Minimum prediction length.",\n         "minimum": 0,\n         "title": "Min Length",\n         "type": "integer"\n      },\n      "max_length": {\n         "default": 250,\n         "description": "Maximum prediction length.",\n         "title": "Max Length",\n         "type": "integer"\n      },\n      "max_length_ratio": {\n         "default": 2,\n         "description": "Maximum prediction length ratio. For European languages, 2 is large enough, for target Asian charageters, need to increase to 2-3, for special languages (Burmese, Amharic) to 10.",\n         "minimum": 1.0,\n         "title": "Max Length Ratio",\n         "type": "number"\n      },\n      "block_ngram_repeat": {\n         "default": 0,\n         "description": "Block repetition of ngrams during decoding.",\n         "title": "Block Ngram Repeat",\n         "type": "integer"\n      },\n      "ignore_when_blocking": {\n         "default": [],\n         "description": "Ignore these strings when blocking repeats. You want to block sentence delimiters.",\n         "items": {\n            "type": "string"\n         },\n         "title": "Ignore When Blocking",\n         "type": "array"\n      },\n      "replace_unk": {\n         "default": false,\n         "description": "Replace the generated UNK tokens with the source token that had the highest attention weight. If phrase_table is provided, it will lok up the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table), then it will copy the source token.",\n         "title": "Replace Unk",\n         "type": "boolean"\n      },\n      "ban_unk_token": {\n         "default": false,\n         "description": "Prevent unk token generation by setting unk probability to 0.",\n         "title": "Ban Unk Token",\n         "type": "boolean"\n      },\n      "phrase_table": {\n         "default": "",\n         "description": "If phrase_table is provided (with replace_unk), it will look up the identified source token and give the corresponding target token.",\n         "title": "Phrase Table",\n         "type": "string"\n      },\n      "n_best": {\n         "default": 1,\n         "description": "Output the n_best decoded sentences.",\n         "title": "N Best",\n         "type": "integer"\n      },\n      "dump_beam": {\n         "default": "",\n         "description": "File to dump beam information to.",\n         "title": "Dump Beam",\n         "type": "string"\n      },\n      "verbose": {\n         "default": false,\n         "description": "Print scores and predictions for each input.",\n         "title": "Verbose",\n         "type": "boolean"\n      },\n      "with_score": {\n         "default": false,\n         "description": "Add a tab separated score to each output.",\n         "title": "With Score",\n         "type": "boolean"\n      },\n      "attn_debug": {\n         "default": false,\n         "description": "Print best attn for each word.",\n         "title": "Attn Debug",\n         "type": "boolean"\n      },\n      "align_debug": {\n         "default": false,\n         "description": "Print best align for each word.",\n         "title": "Align Debug",\n         "type": "boolean"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,l.jsx)(n.p,{}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Config:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,l.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"validate_default"}),": ",(0,l.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,l.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"extra"}),": ",(0,l.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,l.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Fields:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.align_debug",children:(0,l.jsx)(n.code,{children:"align_debug (bool)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.alpha",children:(0,l.jsx)(n.code,{children:"alpha (float)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.attn_debug",children:(0,l.jsx)(n.code,{children:"attn_debug (bool)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.ban_unk_token",children:(0,l.jsx)(n.code,{children:"ban_unk_token (bool)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.beam_size",children:(0,l.jsx)(n.code,{children:"beam_size (int)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.beta",children:(0,l.jsx)(n.code,{children:"beta (float)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.block_ngram_repeat",children:(0,l.jsx)(n.code,{children:"block_ngram_repeat (int)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.coverage_penalty",children:(0,l.jsx)(n.code,{children:"coverage_penalty (Literal['none', 'wu', 'summary'])"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.dump_beam",children:(0,l.jsx)(n.code,{children:"dump_beam (str)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.ignore_when_blocking",children:(0,l.jsx)(n.code,{children:"ignore_when_blocking (List[str])"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.length_penalty",children:(0,l.jsx)(n.code,{children:"length_penalty (Literal['avg', 'wu', 'none'])"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.max_length",children:(0,l.jsx)(n.code,{children:"max_length (int)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.max_length_ratio",children:(0,l.jsx)(n.code,{children:"max_length_ratio (float)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.min_length",children:(0,l.jsx)(n.code,{children:"min_length (int)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.n_best",children:(0,l.jsx)(n.code,{children:"n_best (int)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.phrase_table",children:(0,l.jsx)(n.code,{children:"phrase_table (str)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.ratio",children:(0,l.jsx)(n.code,{children:"ratio (float)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.replace_unk",children:(0,l.jsx)(n.code,{children:"replace_unk (bool)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.stepwise_penalty",children:(0,l.jsx)(n.code,{children:"stepwise_penalty (bool)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.temperature",children:(0,l.jsx)(n.code,{children:"temperature (float)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.top_k",children:(0,l.jsx)(n.code,{children:"top_k (int)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.top_p",children:(0,l.jsx)(n.code,{children:"top_p (float)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.verbose",children:(0,l.jsx)(n.code,{children:"verbose (bool)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig.with_score",children:(0,l.jsx)(n.code,{children:"with_score (bool)"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-align_debug--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," align_debug ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Print best align for each word."}),"\n",(0,l.jsxs)(n.h4,{id:"field-alpha--float--10",children:[(0,l.jsx)(n.em,{children:"field"})," alpha ",(0,l.jsx)(n.em,{children:": float"})," ",(0,l.jsx)(n.em,{children:"= 1.0"})]}),"\n",(0,l.jsx)(n.p,{children:"Length penalty parameter (higher = longer generation)"}),"\n",(0,l.jsxs)(n.h4,{id:"field-attn_debug--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," attn_debug ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Print best attn for each word."}),"\n",(0,l.jsxs)(n.h4,{id:"field-ban_unk_token--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," ban_unk_token ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Prevent unk token generation by setting unk probability to 0."}),"\n",(0,l.jsxs)(n.h4,{id:"field-beam_size--int--5",children:[(0,l.jsx)(n.em,{children:"field"})," beam_size ",(0,l.jsx)(n.em,{children:": int"})," ",(0,l.jsx)(n.em,{children:"= 5"})]}),"\n",(0,l.jsx)(n.p,{children:"Beam size."}),"\n",(0,l.jsxs)(n.h4,{id:"field-beta--float---00",children:[(0,l.jsx)(n.em,{children:"field"})," beta ",(0,l.jsx)(n.em,{children:": float"})," ",(0,l.jsx)(n.em,{children:"= -0.0"})]}),"\n",(0,l.jsx)(n.p,{children:"Coverage penalty parameter."}),"\n",(0,l.jsxs)(n.h4,{id:"field-block_ngram_repeat--int--0",children:[(0,l.jsx)(n.em,{children:"field"})," block_ngram_repeat ",(0,l.jsx)(n.em,{children:": int"})," ",(0,l.jsx)(n.em,{children:"= 0"})]}),"\n",(0,l.jsx)(n.p,{children:"Block repetition of ngrams during decoding."}),"\n",(0,l.jsxs)(n.h4,{id:"field-coverage_penalty--literalnone-wu-summary--none",children:[(0,l.jsx)(n.em,{children:"field"})," coverage_penalty ",(0,l.jsx)(n.em,{children:": Literal['none', 'wu', 'summary']"})," ",(0,l.jsx)(n.em,{children:"= 'none'"})]}),"\n",(0,l.jsx)(n.p,{children:"Coverage penalty to use. Only available in beam search."}),"\n",(0,l.jsxs)(n.h4,{id:"field-dump_beam--str--",children:[(0,l.jsx)(n.em,{children:"field"})," dump_beam ",(0,l.jsx)(n.em,{children:": str"})," ",(0,l.jsx)(n.em,{children:"= ''"})]}),"\n",(0,l.jsx)(n.p,{children:"File to dump beam information to."}),"\n",(0,l.jsxs)(n.h4,{id:"field-ignore_when_blocking--liststr--",children:[(0,l.jsx)(n.em,{children:"field"})," ignore_when_blocking ",(0,l.jsx)(n.em,{children:": List[str]"})," ",(0,l.jsx)(n.em,{children:"= []"})]}),"\n",(0,l.jsx)(n.p,{children:"Ignore these strings when blocking repeats. You want to block sentence delimiters."}),"\n",(0,l.jsxs)(n.h4,{id:"field-length_penalty--literalavg-wu-none--avg",children:[(0,l.jsx)(n.em,{children:"field"})," length_penalty ",(0,l.jsx)(n.em,{children:": Literal['avg', 'wu', 'none']"})," ",(0,l.jsx)(n.em,{children:"= 'avg'"})]}),"\n",(0,l.jsx)(n.p,{children:"Length penalty to use."}),"\n",(0,l.jsxs)(n.h4,{id:"field-max_length--int--250",children:[(0,l.jsx)(n.em,{children:"field"})," max_length ",(0,l.jsx)(n.em,{children:": int"})," ",(0,l.jsx)(n.em,{children:"= 250"})]}),"\n",(0,l.jsx)(n.p,{children:"Maximum prediction length."}),"\n",(0,l.jsxs)(n.h4,{id:"field-max_length_ratio--float--2",children:[(0,l.jsx)(n.em,{children:"field"})," max_length_ratio ",(0,l.jsx)(n.em,{children:": float"})," ",(0,l.jsx)(n.em,{children:"= 2"})]}),"\n",(0,l.jsx)(n.p,{children:"Maximum prediction length ratio. For European languages, 2 is large enough, for target Asian charageters, need to increase to 2-3, for special languages (Burmese, Amharic) to 10."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Constraints:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"ge"})," = 1"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-min_length--int--0",children:[(0,l.jsx)(n.em,{children:"field"})," min_length ",(0,l.jsx)(n.em,{children:": int"})," ",(0,l.jsx)(n.em,{children:"= 0"})]}),"\n",(0,l.jsx)(n.p,{children:"Minimum prediction length."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Constraints:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"ge"})," = 0"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-n_best--int--1",children:[(0,l.jsx)(n.em,{children:"field"})," n_best ",(0,l.jsx)(n.em,{children:": int"})," ",(0,l.jsx)(n.em,{children:"= 1"})]}),"\n",(0,l.jsx)(n.p,{children:"Output the n_best decoded sentences."}),"\n",(0,l.jsxs)(n.h4,{id:"field-phrase_table--str--",children:[(0,l.jsx)(n.em,{children:"field"})," phrase_table ",(0,l.jsx)(n.em,{children:": str"})," ",(0,l.jsx)(n.em,{children:"= ''"})]}),"\n",(0,l.jsx)(n.p,{children:"If phrase_table is provided (with replace_unk), it will look up the identified source token and give the corresponding target token."}),"\n",(0,l.jsxs)(n.h4,{id:"field-ratio--float---00",children:[(0,l.jsx)(n.em,{children:"field"})," ratio ",(0,l.jsx)(n.em,{children:": float"})," ",(0,l.jsx)(n.em,{children:"= -0.0"})]}),"\n",(0,l.jsx)(n.p,{children:"Ratio based beam stop condition."}),"\n",(0,l.jsxs)(n.h4,{id:"field-replace_unk--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," replace_unk ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Replace the generated UNK tokens with the source token that had the highest attention weight. If phrase_table is provided, it will lok up the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table), then it will copy the source token."}),"\n",(0,l.jsxs)(n.h4,{id:"field-stepwise_penalty--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," stepwise_penalty ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Apply coverage penalty at every decoding step. Helpful for summary penalty."}),"\n",(0,l.jsxs)(n.h4,{id:"field-temperature--float--10",children:[(0,l.jsx)(n.em,{children:"field"})," temperature ",(0,l.jsx)(n.em,{children:": float"})," ",(0,l.jsx)(n.em,{children:"= 1.0"})]}),"\n",(0,l.jsx)(n.p,{children:"If doing random sampling, divide the logits by this before computing softmax during decoding."}),"\n",(0,l.jsxs)(n.h4,{id:"field-top_k--int--0",children:[(0,l.jsx)(n.em,{children:"field"})," top_k ",(0,l.jsx)(n.em,{children:": int"})," ",(0,l.jsx)(n.em,{children:"= 0"})]}),"\n",(0,l.jsx)(n.p,{children:"Set this to -1 to do random sampling from full distribution. Set this to value k>1 to do random sampling restricted to the k most likely next tokens. Set this to 1 to use argmax."}),"\n",(0,l.jsxs)(n.h4,{id:"field-top_p--float--00",children:[(0,l.jsx)(n.em,{children:"field"})," top_p ",(0,l.jsx)(n.em,{children:": float"})," ",(0,l.jsx)(n.em,{children:"= 0.0"})]}),"\n",(0,l.jsxs)(n.p,{children:["Probability for top-p/nucleus sampling. Restrict tokens to the most likely until the cumulated probability is over p. In range [0,1]. (",(0,l.jsx)(n.a,{href:"https://arxiv.org/abs/1904.09751",children:"https://arxiv.org/abs/1904.09751"}),")"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Constraints:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"ge"})," = 0.0"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-verbose--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," verbose ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Print scores and predictions for each input."}),"\n",(0,l.jsxs)(n.h4,{id:"field-with_score--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," with_score ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Add a tab separated score to each output."}),"\n",(0,l.jsxs)(n.h3,{id:"pydantic-model-eoleconfiginferenceinferenceconfigsource",children:[(0,l.jsx)(n.em,{children:"pydantic model"})," eole.config.inference.InferenceConfig",(0,l.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/inference.py#L103-L173",children:"[source]"})]}),"\n",(0,l.jsxs)(n.p,{children:["Bases: ",(0,l.jsx)(n.code,{children:"RunningConfig"}),", ",(0,l.jsx)(n.a,{href:"#eole.config.inference.DecodingConfig",children:(0,l.jsx)(n.code,{children:"DecodingConfig"})}),", ",(0,l.jsx)(n.a,{href:"/eole/docs/reference/Config/run#eole.config.common.LoRaConfig",children:(0,l.jsx)(n.code,{children:"LoRaConfig"})}),", ",(0,l.jsx)(n.a,{href:"/eole/docs/reference/Config/run#eole.config.common.QuantizeConfig",children:(0,l.jsx)(n.code,{children:"QuantizeConfig"})})]}),"\n",(0,l.jsx)(n.p,{}),(0,l.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,l.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-json",children:'{\n   "title": "InferenceConfig",\n   "type": "object",\n   "properties": {\n      "quant_layers": {\n         "default": [],\n         "description": "List of layers to be compressed in 4/8bit.",\n         "items": {\n            "type": "string"\n         },\n         "title": "Quant Layers",\n         "type": "array"\n      },\n      "quant_type": {\n         "default": "",\n         "description": "Type of compression.",\n         "enum": [\n            "",\n            "bnb_8bit",\n            "bnb_FP4",\n            "bnb_NF4",\n            "awq_gemm",\n            "awq_gemv"\n         ],\n         "title": "Quant Type",\n         "type": "string"\n      },\n      "w_bit": {\n         "default": 4,\n         "description": "W_bit quantization",\n         "title": "W Bit",\n         "type": "integer"\n      },\n      "group_size": {\n         "default": 128,\n         "description": "Group size quantization.",\n         "title": "Group Size",\n         "type": "integer"\n      },\n      "lora_layers": {\n         "default": [],\n         "description": "List of layers to be replaced by LoRa layers. E.g. [\'linear_values\', \'linear_query\'] (\\u00a74.2 in https://arxiv.org/abs/2106.09685)",\n         "items": {\n            "type": "string"\n         },\n         "title": "Lora Layers",\n         "type": "array"\n      },\n      "lora_embedding": {\n         "default": false,\n         "description": "Replace embeddings with LoRa Embeddings (\\u00a75.1)",\n         "title": "Lora Embedding",\n         "type": "boolean"\n      },\n      "lora_rank": {\n         "default": 2,\n         "description": "r=2 successfully tested with NLLB-200 3.3B",\n         "title": "Lora Rank",\n         "type": "integer"\n      },\n      "lora_alpha": {\n         "default": 1,\n         "description": "\\u00a74.1 https://arxiv.org/abs/2106.09685",\n         "title": "Lora Alpha",\n         "type": "integer"\n      },\n      "lora_dropout": {\n         "default": 0.0,\n         "description": "Rule of thumb: same value as in main model.",\n         "title": "Lora Dropout",\n         "type": "number"\n      },\n      "beam_size": {\n         "default": 5,\n         "description": "Beam size.",\n         "title": "Beam Size",\n         "type": "integer"\n      },\n      "ratio": {\n         "default": -0.0,\n         "description": "Ratio based beam stop condition.",\n         "title": "Ratio",\n         "type": "number"\n      },\n      "top_k": {\n         "default": 0,\n         "description": "Set this to -1 to do random sampling from full distribution. Set this to value k>1 to do random sampling restricted to the k most likely next tokens. Set this to 1 to use argmax.",\n         "title": "Top K",\n         "type": "integer"\n      },\n      "top_p": {\n         "default": 0.0,\n         "description": "Probability for top-p/nucleus sampling. Restrict tokens to the most likely until the cumulated probability is over p. In range [0,1]. (https://arxiv.org/abs/1904.09751)",\n         "lte": 1.0,\n         "minimum": 0.0,\n         "title": "Top P",\n         "type": "number"\n      },\n      "temperature": {\n         "default": 1.0,\n         "description": "If doing random sampling, divide the logits by this before computing softmax during decoding.",\n         "title": "Temperature",\n         "type": "number"\n      },\n      "length_penalty": {\n         "default": "avg",\n         "description": "Length penalty to use.",\n         "enum": [\n            "avg",\n            "wu",\n            "none"\n         ],\n         "title": "Length Penalty",\n         "type": "string"\n      },\n      "alpha": {\n         "default": 1.0,\n         "description": "Length penalty parameter (higher = longer generation)",\n         "title": "Alpha",\n         "type": "number"\n      },\n      "coverage_penalty": {\n         "default": "none",\n         "description": "Coverage penalty to use. Only available in beam search.",\n         "enum": [\n            "none",\n            "wu",\n            "summary"\n         ],\n         "title": "Coverage Penalty",\n         "type": "string"\n      },\n      "beta": {\n         "default": -0.0,\n         "description": "Coverage penalty parameter.",\n         "title": "Beta",\n         "type": "number"\n      },\n      "stepwise_penalty": {\n         "default": false,\n         "description": "Apply coverage penalty at every decoding step. Helpful for summary penalty.",\n         "title": "Stepwise Penalty",\n         "type": "boolean"\n      },\n      "min_length": {\n         "default": 0,\n         "description": "Minimum prediction length.",\n         "minimum": 0,\n         "title": "Min Length",\n         "type": "integer"\n      },\n      "max_length": {\n         "default": 250,\n         "description": "Maximum prediction length.",\n         "title": "Max Length",\n         "type": "integer"\n      },\n      "max_length_ratio": {\n         "default": 2,\n         "description": "Maximum prediction length ratio. For European languages, 2 is large enough, for target Asian charageters, need to increase to 2-3, for special languages (Burmese, Amharic) to 10.",\n         "minimum": 1.0,\n         "title": "Max Length Ratio",\n         "type": "number"\n      },\n      "block_ngram_repeat": {\n         "default": 0,\n         "description": "Block repetition of ngrams during decoding.",\n         "title": "Block Ngram Repeat",\n         "type": "integer"\n      },\n      "ignore_when_blocking": {\n         "default": [],\n         "description": "Ignore these strings when blocking repeats. You want to block sentence delimiters.",\n         "items": {\n            "type": "string"\n         },\n         "title": "Ignore When Blocking",\n         "type": "array"\n      },\n      "replace_unk": {\n         "default": false,\n         "description": "Replace the generated UNK tokens with the source token that had the highest attention weight. If phrase_table is provided, it will lok up the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table), then it will copy the source token.",\n         "title": "Replace Unk",\n         "type": "boolean"\n      },\n      "ban_unk_token": {\n         "default": false,\n         "description": "Prevent unk token generation by setting unk probability to 0.",\n         "title": "Ban Unk Token",\n         "type": "boolean"\n      },\n      "phrase_table": {\n         "default": "",\n         "description": "If phrase_table is provided (with replace_unk), it will look up the identified source token and give the corresponding target token.",\n         "title": "Phrase Table",\n         "type": "string"\n      },\n      "n_best": {\n         "default": 1,\n         "description": "Output the n_best decoded sentences.",\n         "title": "N Best",\n         "type": "integer"\n      },\n      "dump_beam": {\n         "default": "",\n         "description": "File to dump beam information to.",\n         "title": "Dump Beam",\n         "type": "string"\n      },\n      "verbose": {\n         "default": false,\n         "description": "Print scores and predictions for each input.",\n         "title": "Verbose",\n         "type": "boolean"\n      },\n      "with_score": {\n         "default": false,\n         "description": "Add a tab separated score to each output.",\n         "title": "With Score",\n         "type": "boolean"\n      },\n      "attn_debug": {\n         "default": false,\n         "description": "Print best attn for each word.",\n         "title": "Attn Debug",\n         "type": "boolean"\n      },\n      "align_debug": {\n         "default": false,\n         "description": "Print best align for each word.",\n         "title": "Align Debug",\n         "type": "boolean"\n      },\n      "gpu_ranks": {\n         "default": [],\n         "description": "List of ranks for each process.",\n         "items": {\n            "type": "integer"\n         },\n         "title": "Gpu Ranks",\n         "type": "array"\n      },\n      "world_size": {\n         "default": 1,\n         "description": "Total number of distributed processes.",\n         "title": "World Size",\n         "type": "integer"\n      },\n      "parallel_mode": {\n         "default": "data_parallel",\n         "description": "Distributed mode.",\n         "enum": [\n            "data_parallel",\n            "tensor_parallel"\n         ],\n         "title": "Parallel Mode",\n         "type": "string"\n      },\n      "gpu_backend": {\n         "default": "nccl",\n         "description": "Type of torch distributed backend.",\n         "title": "Gpu Backend",\n         "type": "string"\n      },\n      "gpu_verbose_level": {\n         "default": 0,\n         "description": "Gives more info on each process per GPU.",\n         "title": "Gpu Verbose Level",\n         "type": "integer"\n      },\n      "master_ip": {\n         "default": "localhost",\n         "description": "IP of master for torch.distributed training.",\n         "title": "Master Ip",\n         "type": "string"\n      },\n      "master_port": {\n         "default": 10000,\n         "description": "Port of master for torch.distributed training.",\n         "title": "Master Port",\n         "type": "integer"\n      },\n      "timeout": {\n         "default": 60,\n         "description": "Timeout for one GPU to wait for the others.",\n         "title": "Timeout",\n         "type": "integer"\n      },\n      "model_path": {\n         "default": "model",\n         "description": "Path to directory containing all model components.",\n         "title": "Model Path",\n         "type": "string"\n      },\n      "self_attn_backend": {\n         "default": "flash",\n         "description": "Self-attention backend.",\n         "enum": [\n            "flash",\n            "pytorch"\n         ],\n         "title": "Self Attn Backend",\n         "type": "string"\n      },\n      "compute_dtype": {\n         "description": "Compute dtype (precision) to use for main compute. Some parameters might have other dtypes for specific cases (e.g. torch.amp -- See eole.config.training.TrainingConfig.storage_dtype) fp32 to force slow fp16 model on gtx1080, int8 to enable pytorch native 8-bit quantization (cpu only).",\n         "enum": [\n            "fp32",\n            "fp16",\n            "int8",\n            "bf16"\n         ],\n         "title": "Compute Dtype",\n         "type": "string"\n      },\n      "torch_compile": {\n         "default": false,\n         "description": "Use torch.compile with dynamic=True.",\n         "title": "Torch Compile",\n         "type": "boolean"\n      },\n      "report_align": {\n         "default": false,\n         "description": "Report alignment for each translation.",\n         "title": "Report Align",\n         "type": "boolean"\n      },\n      "gold_align": {\n         "default": false,\n         "description": "Report alignment between source and gold target. Useful to test the performance of learnt alignments.",\n         "title": "Gold Align",\n         "type": "boolean"\n      },\n      "report_time": {\n         "default": false,\n         "description": "Report some translation time metrics.",\n         "title": "Report Time",\n         "type": "boolean"\n      },\n      "profile": {\n         "default": false,\n         "description": "Report pytorch profiling stats.",\n         "title": "Profile",\n         "type": "boolean"\n      },\n      "batch_size": {\n         "default": 30,\n         "description": "Batch size.",\n         "title": "Batch Size",\n         "type": "integer"\n      },\n      "batch_type": {\n         "default": "sents",\n         "description": "Batch grouping for batch size.",\n         "enum": [\n            "sents",\n            "tokens"\n         ],\n         "title": "Batch Type",\n         "type": "string"\n      },\n      "avg_raw_probs": {\n         "default": false,\n         "description": "If set, during ensembling scores from different models will be combined by averaging their raw probabilities and then taking the log. Otherwise, the log probabilities will be averaged directly. Necessary for models whose output layers can assign zero probability.",\n         "title": "Avg Raw Probs",\n         "type": "boolean"\n      },\n      "data_type": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "text",\n         "title": "Data Type"\n      },\n      "chat_template": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "title": "Chat Template"\n      },\n      "optional_eos": {\n         "anyOf": [\n            {\n               "items": {\n                  "type": "string"\n               },\n               "type": "array"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": [],\n         "description": "Optional EOS tokens that would stop generation, e.g. <|eot_id|> for Llama3",\n         "title": "Optional Eos"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,l.jsx)(n.p,{}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Config:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,l.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"validate_default"}),": ",(0,l.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,l.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"extra"}),": ",(0,l.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,l.jsx)(n.em,{children:"tuple = ()"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"arbitrary_types_allowed"}),": ",(0,l.jsx)(n.em,{children:"bool = True"})]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Fields:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.InferenceConfig.avg_raw_probs",children:(0,l.jsx)(n.code,{children:"avg_raw_probs (bool)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.InferenceConfig.batch_size",children:(0,l.jsx)(n.code,{children:"batch_size (int)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.InferenceConfig.batch_type",children:(0,l.jsx)(n.code,{children:"batch_type (Literal['sents', 'tokens'])"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.InferenceConfig.chat_template",children:(0,l.jsx)(n.code,{children:"chat_template (str | None)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.InferenceConfig.data_type",children:(0,l.jsx)(n.code,{children:"data_type (str | None)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.InferenceConfig.gold_align",children:(0,l.jsx)(n.code,{children:"gold_align (bool)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.InferenceConfig.optional_eos",children:(0,l.jsx)(n.code,{children:"optional_eos (List[str] | None)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.InferenceConfig.profile",children:(0,l.jsx)(n.code,{children:"profile (bool)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.InferenceConfig.report_align",children:(0,l.jsx)(n.code,{children:"report_align (bool)"})})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"#eole.config.inference.InferenceConfig.report_time",children:(0,l.jsx)(n.code,{children:"report_time (bool)"})})}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validators:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"_validate_model_path"})," \xbb ",(0,l.jsx)(n.code,{children:"model_path"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"_validate_running_config"})," \xbb ",(0,l.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-avg_raw_probs--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," avg_raw_probs ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"If set, during ensembling scores from different models will be combined by averaging their raw probabilities and then taking the log. Otherwise, the log probabilities will be averaged directly. Necessary for models whose output layers can assign zero probability."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"_validate_running_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-batch_size--int--30",children:[(0,l.jsx)(n.em,{children:"field"})," batch_size ",(0,l.jsx)(n.em,{children:": int"})," ",(0,l.jsx)(n.em,{children:"= 30"})]}),"\n",(0,l.jsx)(n.p,{children:"Batch size."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"_validate_running_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-batch_type--literalsents-tokens--sents",children:[(0,l.jsx)(n.em,{children:"field"})," batch_type ",(0,l.jsx)(n.em,{children:": Literal['sents', 'tokens']"})," ",(0,l.jsx)(n.em,{children:"= 'sents'"})]}),"\n",(0,l.jsx)(n.p,{children:"Batch grouping for batch size."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"_validate_running_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-chat_template--str--none--none",children:[(0,l.jsx)(n.em,{children:"field"})," chat_template ",(0,l.jsx)(n.em,{children:": str | None"})," ",(0,l.jsx)(n.em,{children:"= None"})]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"_validate_running_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-data_type--str--none--text",children:[(0,l.jsx)(n.em,{children:"field"})," data_type ",(0,l.jsx)(n.em,{children:": str | None"})," ",(0,l.jsx)(n.em,{children:"= 'text'"})]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"_validate_running_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-gold_align--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," gold_align ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Report alignment between source and gold target. Useful to test the performance of learnt alignments."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"_validate_running_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-optional_eos--liststr--none--",children:[(0,l.jsx)(n.em,{children:"field"})," optional_eos ",(0,l.jsx)(n.em,{children:": List[str] | None"})," ",(0,l.jsx)(n.em,{children:"= []"})]}),"\n",(0,l.jsx)(n.p,{children:"Optional EOS tokens that would stop generation, e.g. <"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{children:"|eot_id|\n"})}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsx)(n.p,{children:"for Llama3"}),"\n"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"_validate_running_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-profile--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," profile ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Report pytorch profiling stats."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"_validate_running_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-report_align--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," report_align ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Report alignment for each translation."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"_validate_running_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"field-report_time--bool--false",children:[(0,l.jsx)(n.em,{children:"field"})," report_time ",(0,l.jsx)(n.em,{children:": bool"})," ",(0,l.jsx)(n.em,{children:"= False"})]}),"\n",(0,l.jsx)(n.p,{children:"Report some translation time metrics."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.code,{children:"_validate_running_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h4,{id:"get_model_pathsource",children:["get_model_path()",(0,l.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/inference.py#L140-L141",children:"[source]"})]}),"\n",(0,l.jsxs)(n.h4,{id:"property-storage_dtype--dtypesource",children:[(0,l.jsx)(n.em,{children:"property"})," storage_dtype ",(0,l.jsx)(n.em,{children:": dtype"}),(0,l.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/inference.py#L165-L173",children:"[source]"})]}),"\n",(0,l.jsx)(n.p,{children:"Deduce which dtype to use for main model parameters."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>s});var l=i(6540);const t={},r=l.createContext(t);function o(e){const n=l.useContext(r);return l.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),l.createElement(r.Provider,{value:n},e.children)}}}]);