"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[4883],{9775:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>a,contentTitle:()=>d,default:()=>c,frontMatter:()=>r,metadata:()=>s,toc:()=>l});var o=n(4848),i=n(8453);const r={},d="HunyuanOCR",s={id:"recipes/hunyuanocr/README",title:"HunyuanOCR",description:"Set environment variables",source:"@site/docs/recipes/hunyuanocr/README.md",sourceDirName:"recipes/hunyuanocr",slug:"/recipes/hunyuanocr/",permalink:"/eole/docs/recipes/hunyuanocr/",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/recipes/hunyuanocr/README.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"OpenAI GPT2",permalink:"/eole/docs/recipes/gpt2/"},next:{title:"Llama2",permalink:"/eole/docs/recipes/llama2/"}},a={},l=[{value:"Set environment variables",id:"set-environment-variables",level:3},{value:"Convert the model",id:"convert-the-model",level:2},{value:"Run the test script",id:"run-the-test-script",level:2}];function u(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h1,{id:"hunyuanocr",children:"HunyuanOCR"}),"\n",(0,o.jsx)(t.h3,{id:"set-environment-variables",children:"Set environment variables"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"export EOLE_MODEL_DIR=<where_to_store_models>\nexport HF_TOKEN=<your_hf_token>\n"})}),"\n",(0,o.jsx)(t.h2,{id:"convert-the-model",children:"Convert the model"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"eole convert HF --model_dir tencent/HunyuanOCR --output $EOLE_MODEL_DIR/HunyuanOCR --token $HF_TOKEN\n"})}),"\n",(0,o.jsx)(t.p,{children:"Manually add at the bottom of the config.json the optional eos (HF config file is not up to date)"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'  "inference": {\n    "optional_eos": ["<\uff5chy_Assistant\uff5c>"],\n'})}),"\n",(0,o.jsx)(t.h2,{id:"run-the-test-script",children:"Run the test script"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"python3 test_inference.py\n"})}),"\n",(0,o.jsx)(t.p,{children:"This script shows the difference between an English and a Chinese prompt for the same task.\nChinese gives better results."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'Figure 4 | To test model performance under different compression ratios (requiring different numbers of vision tokens) and enhance the practicality of DeepSeek-OCR, we configure it with \nmultiple resolution modes.\n\n<table><caption>Table 1 | Multi resolution support of DeepEncoder. For both research and application purposes, we design DeepEncoder with diverse native resolution and dynamic resolution \nmodes.</caption><tr><td></td><td colspan="4">Native Resolution</td><td colspan="2">Dynamic \nResolution</td></tr><tr><td>Mode</td><td>Tiny</td><td>Small</td><td>Base</td><td>Large</td><td>Gundam</td><td>Gundam-M</td></tr><tr><td>Resolution</td><td>512</td><td>640</td><td>1024</td><t\nd>1280</td><td>640+1024</td><td>1024+1280</td></tr><tr><td>Tokens</td><td>64</td><td>100</td><td>256</td><td>400</td><td>n\xd7100+256</td><td>n\xd7256+400</td></tr><tr><td>Process</td><td>resize</\ntd><td>resize</td><td>padding</td><td>padding</td><td>resize + padding</td><td>resize + padding</td></tr></table>\n\n## 3.2.2. Multiple resolution support\n\nSuppose we have an image with 1000 optical characters and we want to test how many vision tokens are needed for decoding. This requires the model to support a variable number of vision \ntokens. That is to say the DeepEncoder needs to support multiple resolutions.\n\nWe meet the requirement aforementioned through dynamic interpolation of positional encodings, and design several resolution modes for simultaneous model training to achieve the capability of\na single DeepSeek-OCR model supporting multiple resolutions. As shown in Figure 4, DeepEncoder mainly supports two major input modes: native resolution and dynamic resolution. Each of them \ncontains multiple sub-modes.\n\nNative resolution supports four sub-modes: Tiny, Small, Base, and Large, with corresponding resolutions and token counts of 512\xd7512 (64), 640\xd7640 (100), 1024\xd71024 (256), and 1280\xd71280 (400) \nrespectively. Since Tiny and Small modes have relatively small resolutions, to avoid wasting vision tokens, images are processed by directly resizing the original shape. For Base and Large \nmodes, in order to preserve the original image aspect ratio, images are padded to the corresponding size. After padding, the number of valid vision tokens is less than the actual number of \nvision tokens, with the calculation formula being:\n\n$$ N_{valid}=\\lceil N_{actual}\\times[1-((max(w,h)-min(w,h))/(max(w,h)))] \\rceil $$\n\nwhere w and h represent the width and height of the original input image.\n'})}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:'<pFig>Figure 4 | To test model performance under different compression ratios (requiring different numbers of vision tokens) and enhance the practicality of DeepSeek-OCR, we configure it \nwith multiple resolution modes.</pFig><quad>(108,56),(875,240)</quad>\n\nthe 4096 tokens go through the compression module and the token becomes 4096/16=256, thus making the overall activation memory controllable.\n\n<table><caption>Table 1 | Multi resolution support of DeepEncoder. For both research and application purposes, we design DeepEncoder with diverse native resolution and dynamic resolution \nmodes.</caption><tr><td></td><td colspan="4">Native Resolution</td><td colspan="2">Dynamic \nResolution</td></tr><tr><td>Mode</td><td>Tiny</td><td>Small</td><td>Base</td><td>Large</td><td>Gundam</td><td>Gundam-M</td></tr><tr><td>Resolution</td><td>512</td><td>640</td><td>1024</td><t\nd>1280</td><td>640+1024</td><td>1024+1280</td></tr><tr><td>Tokens</td><td>64</td><td>100</td><td>256</td><td>400</td><td>n\xd7100+256</td><td>n\xd7256+400</td></tr><tr><td>Process</td><td>resize</\ntd><td>resize</td><td>padding</td><td>padding</td><td>resize + padding</td><td>resize + padding</td></tr></table>\n\n## 3.2.2. Multiple resolution support\n\nSuppose we have an image with 1000 optical characters and we want to test how many vision tokens are needed for decoding. This requires the model to support a variable number of vision \ntokens. That is to say the DeepEncoder needs to support multiple resolutions.\n\nWe meet the requirement aforementioned through dynamic interpolation of positional encodings, and design several resolution modes for simultaneous model training to achieve the capability of\na single DeepSeek-OCR model supporting multiple resolutions. As shown in Figure 4, DeepEncoder mainly supports two major input modes: native resolution and dynamic resolution. Each of them \ncontains multiple sub-modes.\n\nNative resolution supports four sub-modes: Tiny, Small, Base, and Large, with corresponding resolutions and token counts of 512\xd7512 (64), 640\xd7640 (100), 1024\xd71024 (256), and 1280\xd71280 (400) \nrespectively. Since Tiny and Small modes have relatively small resolutions, to avoid wasting vision tokens, images are processed by directly resizing the original shape. For Base and Large \nmodes, in order to preserve the original image aspect ratio, images are padded to the corresponding size. After padding, the number of valid vision tokens is less than the actual number of \nvision tokens, with the calculation formula being:\n\n$$ N_{valid}=\\lceil N_{actual}\\times[1-((max(w,h)-min(w,h))/(max(w,h)))] \\rceil $$\n\nwhere w and h represent the width and height of the original input image.\n'})})]})}function c(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>d,x:()=>s});var o=n(6540);const i={},r=o.createContext(i);function d(e){const t=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function s(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:d(e.components),o.createElement(r.Provider,{value:t},e.children)}}}]);