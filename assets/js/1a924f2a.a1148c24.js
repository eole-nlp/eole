"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[2037],{4853:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>a});var n=i(4848),o=i(8453);const s={},c="Compute dtype (precision) and storage dtype",r={id:"FAQ/precision",title:"Compute dtype (precision) and storage dtype",description:"Various compute precisions are supported. Below is a quick recap of the current cases.",source:"@site/docs/FAQ/precision.md",sourceDirName:"FAQ",slug:"/FAQ/precision",permalink:"/eole/docs/FAQ/precision",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/FAQ/precision.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi",permalink:"/eole/docs/FAQ/position_encoding"},next:{title:"How do I use Pretrained embeddings (e.g. GloVe)?",permalink:"/eole/docs/FAQ/pretrained_embeddings"}},d={},a=[{value:"How to configure",id:"how-to-configure",level:2},{value:"Available modes",id:"available-modes",level:2},{value:"Full precision",id:"full-precision",level:3},{value:"Half precision",id:"half-precision",level:3},{value:"BFloat16",id:"bfloat16",level:3},{value:"Int8",id:"int8",level:3}];function l(e){const t={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",strong:"strong",...(0,o.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"compute-dtype-precision-and-storage-dtype",children:"Compute dtype (precision) and storage dtype"}),"\n",(0,n.jsx)(t.p,{children:"Various compute precisions are supported. Below is a quick recap of the current cases."}),"\n",(0,n.jsx)(t.h2,{id:"how-to-configure",children:"How to configure"}),"\n",(0,n.jsxs)(t.p,{children:["It's important to note that compute precision does not necessarily reflect model parameters dtype.\nWith this considered, compute precision can be configured by setting the ",(0,n.jsx)(t.code,{children:"compute_dtype"})," field.\nFrom that, and other optimization settings (or specicic cases), the ",(0,n.jsx)(t.code,{children:"storage_dtype"})," computed field is deduced.\nThis is different from the specific quantization logic configured via ",(0,n.jsx)(t.code,{children:"quant_layers"})," and ",(0,n.jsx)(t.code,{children:"quant_type"}),". If such quantization is enabled, precision is still taken into account for non quantized components."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Note"}),": the ",(0,n.jsx)(t.code,{children:"compute_dtype"})," field can take both ",(0,n.jsx)(t.code,{children:"str"})," and ",(0,n.jsx)(t.code,{children:"torch.dtype"})," input types. An ",(0,n.jsx)(t.code,{children:"str"})," input is validated to the corresponding ",(0,n.jsx)(t.a,{href:"https://pytorch.org/docs/stable/tensors.html",children:(0,n.jsx)(t.code,{children:"torch.dtype"})})," via a custom mapping (see ",(0,n.jsx)(t.code,{children:"eole.config.common.RunningConfig.compute_dtype"}),")."]}),"\n",(0,n.jsx)(t.h2,{id:"available-modes",children:"Available modes"}),"\n",(0,n.jsx)(t.h3,{id:"full-precision",children:"Full precision"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.code,{children:"compute_dtype: {fp32, torch.float32}"}),"\nStandard float precision."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Note"}),": flash attention is not compatible with float32 precision."]}),"\n",(0,n.jsx)(t.h3,{id:"half-precision",children:"Half precision"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.code,{children:"compute_dtype: {fp16, torch.float16}"})}),"\n",(0,n.jsxs)(t.p,{children:["In most cases, the main model ",(0,n.jsx)(t.code,{children:"storage_dtype"})," will be ",(0,n.jsx)(t.code,{children:"torch.float32"}),", and some parameters will be automatically casted to ",(0,n.jsx)(t.code,{children:"torch.float16"})," with torch ",(0,n.jsx)(t.a,{href:"https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html",children:"Automatic Mixed Precision"}),"."]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Note"}),": this means that checkpoints will be stored in ",(0,n.jsx)(t.code,{children:"torch.float32"})," in the ",(0,n.jsx)(t.code,{children:"amp"})," case."]}),"\n",(0,n.jsx)(t.h3,{id:"bfloat16",children:"BFloat16"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.code,{children:"compute_dtype: {bf16, torch.bfloat16}"})}),"\n",(0,n.jsxs)(t.p,{children:["See ",(0,n.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Bfloat16_floating-point_format",children:"bfloat16 floating-point format"})," for specificities."]}),"\n",(0,n.jsx)(t.p,{children:"When using the flag use_amp=True, behavior will be the same as above, ie torch AMP (mixed precision)"}),"\n",(0,n.jsxs)(t.p,{children:["When using use_amp=False, we switch to torch-optimi which enables pure BF16 training using Kahan summation. see ",(0,n.jsx)(t.a,{href:"https://github.com/warner-benjamin/optimi",children:"Optimi"})]}),"\n",(0,n.jsx)(t.h3,{id:"int8",children:"Int8"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.code,{children:"compute_dtype: {int8, torch.int8}"})}),"\n",(0,n.jsxs)(t.p,{children:["This specific setting is only valid for ",(0,n.jsx)(t.strong,{children:"CPU prediction"}),", to enable ",(0,n.jsx)(t.a,{href:"https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html",children:"Dynamic Quantization"}),"."]}),"\n",(0,n.jsxs)(t.p,{children:["In that case, ",(0,n.jsx)(t.code,{children:"storage_dtype"})," will initially be ",(0,n.jsx)(t.code,{children:"torch.float32"}),", and the model will then be quantized to ",(0,n.jsx)(t.code,{children:"torch.qint8"})," with ",(0,n.jsx)(t.a,{href:"https://pytorch.org/docs/stable/generated/torch.ao.quantization.quantize_dynamic.html",children:(0,n.jsx)(t.code,{children:"torch.quantization.quantize_dynamic"})}),"."]})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}},8453:(e,t,i)=>{i.d(t,{R:()=>c,x:()=>r});var n=i(6540);const o={},s=n.createContext(o);function c(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:c(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);