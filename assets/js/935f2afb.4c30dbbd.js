"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[8581],{5610:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"apiSidebar":[{"type":"link","label":"Eole Core API","href":"/eole/docs/reference/index","docId":"reference/index","unlisted":false},{"type":"category","label":"Configuration","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Data","href":"/eole/docs/reference/Config/data","docId":"reference/Config/data","unlisted":false},{"type":"link","label":"Inference","href":"/eole/docs/reference/Config/inference","docId":"reference/Config/inference","unlisted":false},{"type":"link","label":"Models","href":"/eole/docs/reference/Config/models","docId":"reference/Config/models","unlisted":false},{"type":"link","label":"Main Entrypoints","href":"/eole/docs/reference/Config/run","docId":"reference/Config/run","unlisted":false},{"type":"link","label":"Training","href":"/eole/docs/reference/Config/training","docId":"reference/Config/training","unlisted":false},{"type":"link","label":"Transforms","href":"/eole/docs/reference/Config/transforms","docId":"reference/Config/transforms","unlisted":false}],"href":"/eole/docs/reference/Config/"},{"type":"category","label":"Core API","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Framework","href":"/eole/docs/reference/Core API/core","docId":"reference/Core API/core","unlisted":false},{"type":"link","label":"Modules","href":"/eole/docs/reference/Core API/modules","docId":"reference/Core API/modules","unlisted":false},{"type":"link","label":"Data Loaders","href":"/eole/docs/reference/Core API/dataloaders","docId":"reference/Core API/dataloaders","unlisted":false},{"type":"link","label":"Prediction","href":"/eole/docs/reference/Core API/inference","docId":"reference/Core API/inference","unlisted":false}]},{"type":"link","label":"Bibliography","href":"/eole/docs/reference/bibliography","docId":"reference/bibliography","unlisted":false}],"tutorialSidebar":[{"type":"link","label":"EOLE","href":"/eole/docs/","docId":"index","unlisted":false},{"type":"link","label":"Quickstart","href":"/eole/docs/quickstart","docId":"quickstart","unlisted":false},{"type":"category","label":"Key Concepts","items":[{"type":"link","label":"Configuration","href":"/eole/docs/concepts/config","docId":"concepts/config","unlisted":false},{"type":"link","label":"Command Line","href":"/eole/docs/concepts/command_line","docId":"concepts/command_line","unlisted":false},{"type":"link","label":"Dataset Weighting","href":"/eole/docs/concepts/weighting","docId":"concepts/weighting","unlisted":false},{"type":"link","label":"Data Transforms","href":"/eole/docs/concepts/transforms","docId":"concepts/transforms","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/eole/docs/category/key-concepts"},{"type":"category","label":"Frequently Asked Questions","items":[{"type":"link","label":"How can I create custom on-the-fly data transforms?","href":"/eole/docs/FAQ/custom_transforms","docId":"FAQ/custom_transforms","unlisted":false},{"type":"link","label":"Do you support multi-gpu?","href":"/eole/docs/FAQ/distributed","docId":"FAQ/distributed","unlisted":false},{"type":"link","label":"How can I ensemble Models at inference?","href":"/eole/docs/FAQ/ensemble_decoding","docId":"FAQ/ensemble_decoding","unlisted":false},{"type":"link","label":"How to use gradient checkpointing when dealing with a big model ?","href":"/eole/docs/FAQ/gradient_checkpointing","docId":"FAQ/gradient_checkpointing","unlisted":false},{"type":"link","label":"How to use LoRa and 8bit loading to finetune a big model ?","href":"/eole/docs/FAQ/lora","docId":"FAQ/lora","unlisted":false},{"type":"link","label":"How to switch from OpenNMT-py to EOLE?","href":"/eole/docs/FAQ/opennmt","docId":"FAQ/opennmt","unlisted":false},{"type":"link","label":"Performance tips","href":"/eole/docs/FAQ/performance","docId":"FAQ/performance","unlisted":false},{"type":"link","label":"Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi","href":"/eole/docs/FAQ/position_encoding","docId":"FAQ/position_encoding","unlisted":false},{"type":"link","label":"Compute dtype (precision) and storage dtype","href":"/eole/docs/FAQ/precision","docId":"FAQ/precision","unlisted":false},{"type":"link","label":"How do I use Pretrained embeddings (e.g. GloVe)?","href":"/eole/docs/FAQ/pretrained_embeddings","docId":"FAQ/pretrained_embeddings","unlisted":false},{"type":"link","label":"What special tokens are used?","href":"/eole/docs/FAQ/special_tokens","docId":"FAQ/special_tokens","unlisted":false},{"type":"link","label":"How can I apply on-the-fly tokenization and subword regularization when training?","href":"/eole/docs/FAQ/tokenization","docId":"FAQ/tokenization","unlisted":false},{"type":"link","label":"How can I update a checkpoint\'s vocabulary?","href":"/eole/docs/FAQ/update_vocab","docId":"FAQ/update_vocab","unlisted":false},{"type":"link","label":"Can I get word alignments while translating?","href":"/eole/docs/FAQ/word_alignments","docId":"FAQ/word_alignments","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/eole/docs/category/frequently-asked-questions"},{"type":"category","label":"Recipes","items":[{"type":"link","label":"Example of using Hugging Face streaming dataset","href":"/eole/docs/recipes/NewsPalm-synthetic/","docId":"recipes/NewsPalm-synthetic/README","unlisted":false},{"type":"link","label":"Recipes","href":"/eole/docs/recipes/","docId":"recipes/README","unlisted":false},{"type":"link","label":"CometKiwi","href":"/eole/docs/recipes/cometkiwi/","docId":"recipes/cometkiwi/README","unlisted":false},{"type":"link","label":"Serving Eurollm in a gradio interface with Eole","href":"/eole/docs/recipes/eurollm/","docId":"recipes/eurollm/README","unlisted":false},{"type":"link","label":"fineweb10B","href":"/eole/docs/recipes/fineweb10B/","docId":"recipes/fineweb10B/README","unlisted":false},{"type":"link","label":"OpenAI GPT2","href":"/eole/docs/recipes/gpt2/","docId":"recipes/gpt2/README","unlisted":false},{"type":"link","label":"Llama2","href":"/eole/docs/recipes/llama2/","docId":"recipes/llama2/README","unlisted":false},{"type":"link","label":"Llama3","href":"/eole/docs/recipes/llama3/","docId":"recipes/llama3/README","unlisted":false},{"type":"link","label":"Llama3","href":"/eole/docs/recipes/llama3.1/","docId":"recipes/llama3.1/README","unlisted":false},{"type":"link","label":"Mistral","href":"/eole/docs/recipes/mistral/","docId":"recipes/mistral/README","unlisted":false},{"type":"link","label":"Mixtral","href":"/eole/docs/recipes/mixtral/","docId":"recipes/mixtral/README","unlisted":false},{"type":"link","label":"Evaluation with MMLU dataset","href":"/eole/docs/recipes/mmlu/","docId":"recipes/mmlu/README","unlisted":false},{"type":"link","label":"Model Validator","href":"/eole/docs/recipes/model-validator/","docId":"recipes/model-validator/README","unlisted":false},{"type":"link","label":"NLLB","href":"/eole/docs/recipes/nllb/","docId":"recipes/nllb/README","unlisted":false},{"type":"link","label":"Pixtral","href":"/eole/docs/recipes/pixtral/","docId":"recipes/pixtral/README","unlisted":false},{"type":"link","label":"Serving models with Eole","href":"/eole/docs/recipes/server/","docId":"recipes/server/README","unlisted":false},{"type":"link","label":"Language Model Wiki-103","href":"/eole/docs/recipes/wiki_103/","docId":"recipes/wiki_103/README","unlisted":false},{"type":"link","label":"Translation WMT17 en-de","href":"/eole/docs/recipes/wmt17/","docId":"recipes/wmt17/README","unlisted":false},{"type":"link","label":"TowerInstruct (Mistral)","href":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/","docId":"recipes/wmt22_with_TowerInstruct-Mistral/README","unlisted":false},{"type":"link","label":"TowerInstruct-(Llama2)","href":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/","docId":"recipes/wmt22_with_TowerInstruct-llama2/README","unlisted":false},{"type":"link","label":"Llama3","href":"/eole/docs/recipes/wmt22_with_llama3.1/","docId":"recipes/wmt22_with_llama3.1/README","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/eole/docs/category/recipes"},{"type":"link","label":"Contributors","href":"/eole/docs/contributing","docId":"contributing","unlisted":false}]},"docs":{"concepts/command_line":{"id":"concepts/command_line","title":"Command Line","description":"Recap of command line utilities and how to call them.","sidebar":"tutorialSidebar"},"concepts/config":{"id":"concepts/config","title":"Configuration","description":"Crash course on Eole configuration.","sidebar":"tutorialSidebar"},"concepts/transforms":{"id":"concepts/transforms","title":"Data Transforms","description":"Recap of available on-the-fly data transforms.","sidebar":"tutorialSidebar"},"concepts/weighting":{"id":"concepts/weighting","title":"Dataset Weighting","description":"How to weight datasets to under/oversample on the fly.","sidebar":"tutorialSidebar"},"contributing":{"id":"contributing","title":"Contributors","description":"EOLE is a community developed project and we love developer contributions.","sidebar":"tutorialSidebar"},"FAQ/custom_transforms":{"id":"FAQ/custom_transforms","title":"How can I create custom on-the-fly data transforms?","description":"The code is easily extendable with custom transforms inheriting from the Transform base class.","sidebar":"tutorialSidebar"},"FAQ/distributed":{"id":"FAQ/distributed","title":"Do you support multi-gpu?","description":"First you need to make sure you export CUDAVISIBLEDEVICES=0,1,2,3.","sidebar":"tutorialSidebar"},"FAQ/ensemble_decoding":{"id":"FAQ/ensemble_decoding","title":"How can I ensemble Models at inference?","description":"You can specify several models in the onmttranslate command line: -model model1seed1 model2_seed2","sidebar":"tutorialSidebar"},"FAQ/gradient_checkpointing":{"id":"FAQ/gradient_checkpointing","title":"How to use gradient checkpointing when dealing with a big model ?","description":"* use_ckpting: [\\"ffn\\", \\"mha\\", \\"lora\\"]","sidebar":"tutorialSidebar"},"FAQ/lora":{"id":"FAQ/lora","title":"How to use LoRa and 8bit loading to finetune a big model ?","description":"Cf paper: LoRa","sidebar":"tutorialSidebar"},"FAQ/opennmt":{"id":"FAQ/opennmt","title":"How to switch from OpenNMT-py to EOLE?","description":"Configuration conversion","sidebar":"tutorialSidebar"},"FAQ/performance":{"id":"FAQ/performance","title":"Performance tips","description":"* use fp16","sidebar":"tutorialSidebar"},"FAQ/position_encoding":{"id":"FAQ/position_encoding","title":"Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi","description":"The basic feature is absolute position encoding stemming from the original Transformer Paper.","sidebar":"tutorialSidebar"},"FAQ/precision":{"id":"FAQ/precision","title":"Compute dtype (precision) and storage dtype","description":"Various compute precisions are supported. Below is a quick recap of the current cases.","sidebar":"tutorialSidebar"},"FAQ/pretrained_embeddings":{"id":"FAQ/pretrained_embeddings","title":"How do I use Pretrained embeddings (e.g. GloVe)?","description":"This is handled in the initial steps of the onmt_train execution.","sidebar":"tutorialSidebar"},"FAQ/special_tokens":{"id":"FAQ/special_tokens","title":"What special tokens are used?","description":"There are 4 main special tokens:","sidebar":"tutorialSidebar"},"FAQ/tokenization":{"id":"FAQ/tokenization","title":"How can I apply on-the-fly tokenization and subword regularization when training?","description":"This is part of the transforms paradigm, which allows to apply various processing to inputs before constituting batches to train models on (or predict). transforms basically is a list of functions that will be applied sequentially to the examples when read from file (or input list).","sidebar":"tutorialSidebar"},"FAQ/update_vocab":{"id":"FAQ/update_vocab","title":"How can I update a checkpoint\'s vocabulary?","description":"New vocabulary can be used to continue training from a checkpoint. Existing vocabulary embeddings will be mapped to the new vocabulary, and new vocabulary tokens will be initialized as usual.","sidebar":"tutorialSidebar"},"FAQ/word_alignments":{"id":"FAQ/word_alignments","title":"Can I get word alignments while translating?","description":"Raw alignments from averaging Transformer attention heads","sidebar":"tutorialSidebar"},"index":{"id":"index","title":"EOLE","description":"Documentation","sidebar":"tutorialSidebar"},"main":{"id":"main","title":"Overview","description":"This portal provides a detailed documentation of the OpenNMT-py toolkit. It describes how to use the PyTorch project and how it works."},"quickstart":{"id":"quickstart","title":"Quickstart","description":"How to train a model from scratch","sidebar":"tutorialSidebar"},"recipes/cometkiwi/README":{"id":"recipes/cometkiwi/README","title":"CometKiwi","description":"---","sidebar":"tutorialSidebar"},"recipes/eurollm/README":{"id":"recipes/eurollm/README","title":"Serving Eurollm in a gradio interface with Eole","description":"Retrieve and convert model","sidebar":"tutorialSidebar"},"recipes/fineweb10B/README":{"id":"recipes/fineweb10B/README","title":"fineweb10B","description":"This recipe is inspired from karpathy/llm.c#481","sidebar":"tutorialSidebar"},"recipes/gpt2/README":{"id":"recipes/gpt2/README","title":"OpenAI GPT2","description":"https://huggingface.co/openai-community/gpt2","sidebar":"tutorialSidebar"},"recipes/llama2/README":{"id":"recipes/llama2/README","title":"Llama2","description":"---","sidebar":"tutorialSidebar"},"recipes/llama3.1/README":{"id":"recipes/llama3.1/README","title":"Llama3","description":"---","sidebar":"tutorialSidebar"},"recipes/llama3/README":{"id":"recipes/llama3/README","title":"Llama3","description":"---","sidebar":"tutorialSidebar"},"recipes/mistral/README":{"id":"recipes/mistral/README","title":"Mistral","description":"---","sidebar":"tutorialSidebar"},"recipes/mixtral/README":{"id":"recipes/mixtral/README","title":"Mixtral","description":"---","sidebar":"tutorialSidebar"},"recipes/mmlu/README":{"id":"recipes/mmlu/README","title":"Evaluation with MMLU dataset","description":"How to run","sidebar":"tutorialSidebar"},"recipes/model-validator/README":{"id":"recipes/model-validator/README","title":"Model Validator","description":"1. Edit the models you want to run in the run.sh script:","sidebar":"tutorialSidebar"},"recipes/NewsPalm-synthetic/README":{"id":"recipes/NewsPalm-synthetic/README","title":"Example of using Hugging Face streaming dataset","description":"Based on//arxiv.org/pdf/2408.06537","sidebar":"tutorialSidebar"},"recipes/nllb/README":{"id":"recipes/nllb/README","title":"NLLB","description":"Conversion","sidebar":"tutorialSidebar"},"recipes/pixtral/README":{"id":"recipes/pixtral/README","title":"Pixtral","description":"Only the mistral-community/pixtral-12b version is supported right now. The official mistralai/Pixtral-12B-2409 config is rather incomplete.","sidebar":"tutorialSidebar"},"recipes/README":{"id":"recipes/README","title":"Recipes","description":"Available Recipes","sidebar":"tutorialSidebar"},"recipes/server/README":{"id":"recipes/server/README","title":"Serving models with Eole","description":"The provided example configuration allows to serve Llama3-8B-Instruct.","sidebar":"tutorialSidebar"},"recipes/wiki_103/README":{"id":"recipes/wiki_103/README","title":"Language Model Wiki-103","description":"Step 1: Download and clean the data, prepare subword model","sidebar":"tutorialSidebar"},"recipes/wmt17/README":{"id":"recipes/wmt17/README","title":"Translation WMT17 en-de","description":"---","sidebar":"tutorialSidebar"},"recipes/wmt22_with_llama3.1/README":{"id":"recipes/wmt22_with_llama3.1/README","title":"Llama3","description":"---","sidebar":"tutorialSidebar"},"recipes/wmt22_with_TowerInstruct-llama2/README":{"id":"recipes/wmt22_with_TowerInstruct-llama2/README","title":"TowerInstruct-(Llama2)","description":"---","sidebar":"tutorialSidebar"},"recipes/wmt22_with_TowerInstruct-Mistral/README":{"id":"recipes/wmt22_with_TowerInstruct-Mistral/README","title":"TowerInstruct (Mistral)","description":"---","sidebar":"tutorialSidebar"},"reference/bibliography":{"id":"reference/bibliography","title":"Bibliography","description":"Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014.","sidebar":"apiSidebar"},"reference/Config/config":{"id":"reference/Config/config","title":"Configuration","description":"TODO: add some example yaml configs here","sidebar":"apiSidebar"},"reference/Config/data":{"id":"reference/Config/data","title":"Data","description":"","sidebar":"apiSidebar"},"reference/Config/inference":{"id":"reference/Config/inference","title":"Inference","description":"","sidebar":"apiSidebar"},"reference/Config/models":{"id":"reference/Config/models","title":"Models","description":"Base Configs","sidebar":"apiSidebar"},"reference/Config/run":{"id":"reference/Config/run","title":"Main Entrypoints","description":"Common Base Config","sidebar":"apiSidebar"},"reference/Config/training":{"id":"reference/Config/training","title":"Training","description":"","sidebar":"apiSidebar"},"reference/Config/transforms":{"id":"reference/Config/transforms","title":"Transforms","description":"","sidebar":"apiSidebar"},"reference/Core API/core":{"id":"reference/Core API/core","title":"Framework","description":"Model","sidebar":"apiSidebar"},"reference/Core API/dataloaders":{"id":"reference/Core API/dataloaders","title":"Data Loaders","description":"Data Iterator","sidebar":"apiSidebar"},"reference/Core API/inference":{"id":"reference/Core API/inference","title":"Prediction","description":"Predictions","sidebar":"apiSidebar"},"reference/Core API/modules":{"id":"reference/Core API/modules","title":"Modules","description":"Embeddings","sidebar":"apiSidebar"},"reference/index":{"id":"reference/index","title":"Eole Core API","description":"Note : These sections are built via sphinx and converted into Markdown. Some layout or links might be shaky.","sidebar":"apiSidebar"}}}')}}]);