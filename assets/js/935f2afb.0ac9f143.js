"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[8581],{5610:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"apiSidebar":[{"type":"link","label":"Eole Core API","href":"/eole/docs/reference/index","docId":"reference/index","unlisted":false},{"type":"category","label":"Configuration","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Data","href":"/eole/docs/reference/Config/data","docId":"reference/Config/data","unlisted":false},{"type":"link","label":"Inference","href":"/eole/docs/reference/Config/inference","docId":"reference/Config/inference","unlisted":false},{"type":"link","label":"Models","href":"/eole/docs/reference/Config/models","docId":"reference/Config/models","unlisted":false},{"type":"link","label":"Main Entrypoints","href":"/eole/docs/reference/Config/run","docId":"reference/Config/run","unlisted":false},{"type":"link","label":"Training","href":"/eole/docs/reference/Config/training","docId":"reference/Config/training","unlisted":false}],"href":"/eole/docs/reference/Config/"},{"type":"category","label":"Core API","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Framework","href":"/eole/docs/reference/Core API/core","docId":"reference/Core API/core","unlisted":false},{"type":"link","label":"Modules","href":"/eole/docs/reference/Core API/modules","docId":"reference/Core API/modules","unlisted":false},{"type":"link","label":"Data Loaders","href":"/eole/docs/reference/Core API/dataloaders","docId":"reference/Core API/dataloaders","unlisted":false},{"type":"link","label":"Prediction","href":"/eole/docs/reference/Core API/inference","docId":"reference/Core API/inference","unlisted":false}]},{"type":"link","label":"Bibliography","href":"/eole/docs/reference/bibliography","docId":"reference/bibliography","unlisted":false}],"tutorialSidebar":[{"type":"link","label":"EOLE","href":"/eole/docs/","docId":"index","unlisted":false},{"type":"link","label":"Quickstart","href":"/eole/docs/quickstart","docId":"quickstart","unlisted":false},{"type":"category","label":"Key Concepts","items":[{"type":"link","label":"Configuration","href":"/eole/docs/concepts/config","docId":"concepts/config","unlisted":false},{"type":"link","label":"Command Line","href":"/eole/docs/concepts/command_line","docId":"concepts/command_line","unlisted":false},{"type":"link","label":"Dataset Weighting","href":"/eole/docs/concepts/weighting","docId":"concepts/weighting","unlisted":false},{"type":"link","label":"Data Transforms","href":"/eole/docs/concepts/transforms","docId":"concepts/transforms","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/eole/docs/category/key-concepts"},{"type":"category","label":"Frequently Asked Questions","items":[{"type":"link","label":"How can I create custom on-the-fly data transforms?","href":"/eole/docs/FAQ/custom_transforms","docId":"FAQ/custom_transforms","unlisted":false},{"type":"link","label":"Do you support multi-gpu?","href":"/eole/docs/FAQ/distributed","docId":"FAQ/distributed","unlisted":false},{"type":"link","label":"How can I ensemble Models at inference?","href":"/eole/docs/FAQ/ensemble_decoding","docId":"FAQ/ensemble_decoding","unlisted":false},{"type":"link","label":"How to use gradient checkpointing when dealing with a big model ?","href":"/eole/docs/FAQ/gradient_checkpointing","docId":"FAQ/gradient_checkpointing","unlisted":false},{"type":"link","label":"How to use LoRa and 8bit loading to finetune a big model ?","href":"/eole/docs/FAQ/lora","docId":"FAQ/lora","unlisted":false},{"type":"link","label":"How to switch from OpenNMT-py to EOLE?","href":"/eole/docs/FAQ/opennmt","docId":"FAQ/opennmt","unlisted":false},{"type":"link","label":"Performance tips","href":"/eole/docs/FAQ/performance","docId":"FAQ/performance","unlisted":false},{"type":"link","label":"Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi","href":"/eole/docs/FAQ/position_encoding","docId":"FAQ/position_encoding","unlisted":false},{"type":"link","label":"Compute dtype (precision) and storage dtype","href":"/eole/docs/FAQ/precision","docId":"FAQ/precision","unlisted":false},{"type":"link","label":"How do I use Pretrained embeddings (e.g. GloVe)?","href":"/eole/docs/FAQ/pretrained_embeddings","docId":"FAQ/pretrained_embeddings","unlisted":false},{"type":"link","label":"What special tokens are used?","href":"/eole/docs/FAQ/special_tokens","docId":"FAQ/special_tokens","unlisted":false},{"type":"link","label":"How can I apply on-the-fly tokenization and subword regularization when training?","href":"/eole/docs/FAQ/tokenization","docId":"FAQ/tokenization","unlisted":false},{"type":"link","label":"How can I update a checkpoint\'s vocabulary?","href":"/eole/docs/FAQ/update_vocab","docId":"FAQ/update_vocab","unlisted":false},{"type":"link","label":"Can I get word alignments while translating?","href":"/eole/docs/FAQ/word_alignments","docId":"FAQ/word_alignments","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/eole/docs/category/frequently-asked-questions"},{"type":"category","label":"Recipes","items":[{"type":"link","label":"Recipes","href":"/eole/docs/recipes/","docId":"recipes/README","unlisted":false},{"type":"link","label":"CometKiwi","href":"/eole/docs/recipes/cometkiwi/","docId":"recipes/cometkiwi/README","unlisted":false},{"type":"link","label":"OpenAI GPT2","href":"/eole/docs/recipes/gpt2/","docId":"recipes/gpt2/README","unlisted":false},{"type":"link","label":"Llama2","href":"/eole/docs/recipes/llama2/","docId":"recipes/llama2/README","unlisted":false},{"type":"link","label":"Llama3","href":"/eole/docs/recipes/llama3/","docId":"recipes/llama3/README","unlisted":false},{"type":"link","label":"Mistral","href":"/eole/docs/recipes/mistral/","docId":"recipes/mistral/README","unlisted":false},{"type":"link","label":"Mixtral","href":"/eole/docs/recipes/mixtral/","docId":"recipes/mixtral/README","unlisted":false},{"type":"link","label":"Evaluation with MMLU dataset","href":"/eole/docs/recipes/mmlu/","docId":"recipes/mmlu/README","unlisted":false},{"type":"link","label":"Language Model Wiki-103","href":"/eole/docs/recipes/wiki_103/","docId":"recipes/wiki_103/README","unlisted":false},{"type":"link","label":"Translation WMT17 en-de","href":"/eole/docs/recipes/wmt17/","docId":"recipes/wmt17/README","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/eole/docs/category/recipes"},{"type":"link","label":"Contributors","href":"/eole/docs/contributing","docId":"contributing","unlisted":false}]},"docs":{"concepts/command_line":{"id":"concepts/command_line","title":"Command Line","description":"Recap of command line utilities and how to call them.","sidebar":"tutorialSidebar"},"concepts/config":{"id":"concepts/config","title":"Configuration","description":"Crash course on Eole configuration.","sidebar":"tutorialSidebar"},"concepts/transforms":{"id":"concepts/transforms","title":"Data Transforms","description":"Recap of available on-the-fly data transforms.","sidebar":"tutorialSidebar"},"concepts/weighting":{"id":"concepts/weighting","title":"Dataset Weighting","description":"How to weight datasets to under/oversample on the fly.","sidebar":"tutorialSidebar"},"contributing":{"id":"contributing","title":"Contributors","description":"EOLE is a community developed project and we love developer contributions.","sidebar":"tutorialSidebar"},"FAQ/custom_transforms":{"id":"FAQ/custom_transforms","title":"How can I create custom on-the-fly data transforms?","description":"The code is easily extendable with custom transforms inheriting from the Transform base class.","sidebar":"tutorialSidebar"},"FAQ/distributed":{"id":"FAQ/distributed","title":"Do you support multi-gpu?","description":"First you need to make sure you export CUDAVISIBLEDEVICES=0,1,2,3.","sidebar":"tutorialSidebar"},"FAQ/ensemble_decoding":{"id":"FAQ/ensemble_decoding","title":"How can I ensemble Models at inference?","description":"You can specify several models in the onmttranslate command line: -model model1seed1 model2_seed2","sidebar":"tutorialSidebar"},"FAQ/gradient_checkpointing":{"id":"FAQ/gradient_checkpointing","title":"How to use gradient checkpointing when dealing with a big model ?","description":"* use_ckpting: [\\"ffn\\", \\"mha\\", \\"lora\\"]","sidebar":"tutorialSidebar"},"FAQ/lora":{"id":"FAQ/lora","title":"How to use LoRa and 8bit loading to finetune a big model ?","description":"Cf paper: LoRa","sidebar":"tutorialSidebar"},"FAQ/opennmt":{"id":"FAQ/opennmt","title":"How to switch from OpenNMT-py to EOLE?","description":"Configuration conversion","sidebar":"tutorialSidebar"},"FAQ/performance":{"id":"FAQ/performance","title":"Performance tips","description":"* use fp16","sidebar":"tutorialSidebar"},"FAQ/position_encoding":{"id":"FAQ/position_encoding","title":"Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi","description":"The basic feature is absolute position encoding stemming from the original Transformer Paper.","sidebar":"tutorialSidebar"},"FAQ/precision":{"id":"FAQ/precision","title":"Compute dtype (precision) and storage dtype","description":"Various compute precisions are supported. Below is a quick recap of the current cases.","sidebar":"tutorialSidebar"},"FAQ/pretrained_embeddings":{"id":"FAQ/pretrained_embeddings","title":"How do I use Pretrained embeddings (e.g. GloVe)?","description":"This is handled in the initial steps of the onmt_train execution.","sidebar":"tutorialSidebar"},"FAQ/special_tokens":{"id":"FAQ/special_tokens","title":"What special tokens are used?","description":"In the v2, special tokens were different for SEQ2SEQ and LM:","sidebar":"tutorialSidebar"},"FAQ/tokenization":{"id":"FAQ/tokenization","title":"How can I apply on-the-fly tokenization and subword regularization when training?","description":"This is part of the transforms paradigm, which allows to apply various processing to inputs before constituting batches to train models on (or predict). transforms basically is a list of functions that will be applied sequentially to the examples when read from file (or input list).","sidebar":"tutorialSidebar"},"FAQ/update_vocab":{"id":"FAQ/update_vocab","title":"How can I update a checkpoint\'s vocabulary?","description":"New vocabulary can be used to continue training from a checkpoint. Existing vocabulary embeddings will be mapped to the new vocabulary, and new vocabulary tokens will be initialized as usual.","sidebar":"tutorialSidebar"},"FAQ/word_alignments":{"id":"FAQ/word_alignments","title":"Can I get word alignments while translating?","description":"Raw alignments from averaging Transformer attention heads","sidebar":"tutorialSidebar"},"index":{"id":"index","title":"EOLE","description":"Documentation","sidebar":"tutorialSidebar"},"main":{"id":"main","title":"Overview","description":"This portal provides a detailed documentation of the OpenNMT-py toolkit. It describes how to use the PyTorch project and how it works."},"quickstart":{"id":"quickstart","title":"Quickstart","description":"How to train a model from scratch","sidebar":"tutorialSidebar"},"recipes/cometkiwi/README":{"id":"recipes/cometkiwi/README","title":"CometKiwi","description":"---","sidebar":"tutorialSidebar"},"recipes/gpt2/README":{"id":"recipes/gpt2/README","title":"OpenAI GPT2","description":"https://huggingface.co/openai-community/gpt2","sidebar":"tutorialSidebar"},"recipes/llama2/README":{"id":"recipes/llama2/README","title":"Llama2","description":"---","sidebar":"tutorialSidebar"},"recipes/llama3/README":{"id":"recipes/llama3/README","title":"Llama3","description":"---","sidebar":"tutorialSidebar"},"recipes/mistral/README":{"id":"recipes/mistral/README","title":"Mistral","description":"---","sidebar":"tutorialSidebar"},"recipes/mixtral/README":{"id":"recipes/mixtral/README","title":"Mixtral","description":"---","sidebar":"tutorialSidebar"},"recipes/mmlu/README":{"id":"recipes/mmlu/README","title":"Evaluation with MMLU dataset","description":"How to run","sidebar":"tutorialSidebar"},"recipes/README":{"id":"recipes/README","title":"Recipes","description":"Available Recipes","sidebar":"tutorialSidebar"},"recipes/wiki_103/README":{"id":"recipes/wiki_103/README","title":"Language Model Wiki-103","description":"Step 1: Download and clean the data, prepare subword model","sidebar":"tutorialSidebar"},"recipes/wmt17/README":{"id":"recipes/wmt17/README","title":"Translation WMT17 en-de","description":"---","sidebar":"tutorialSidebar"},"reference/bibliography":{"id":"reference/bibliography","title":"Bibliography","description":"Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014.","sidebar":"apiSidebar"},"reference/Config/config":{"id":"reference/Config/config","title":"Configuration","description":"TODO: add some example yaml configs here","sidebar":"apiSidebar"},"reference/Config/data":{"id":"reference/Config/data","title":"Data","description":"pydantic model eole.config.data.BaseVocabConfig[source]","sidebar":"apiSidebar"},"reference/Config/inference":{"id":"reference/Config/inference","title":"Inference","description":"pydantic model eole.config.inference.DecodingConfig[source]","sidebar":"apiSidebar"},"reference/Config/models":{"id":"reference/Config/models","title":"Models","description":"Base Configs","sidebar":"apiSidebar"},"reference/Config/run":{"id":"reference/Config/run","title":"Main Entrypoints","description":"Common Base Config","sidebar":"apiSidebar"},"reference/Config/training":{"id":"reference/Config/training","title":"Training","description":"pydantic model eole.config.training.OptimizerConfig[source]","sidebar":"apiSidebar"},"reference/Core API/core":{"id":"reference/Core API/core","title":"Framework","description":"Model","sidebar":"apiSidebar"},"reference/Core API/dataloaders":{"id":"reference/Core API/dataloaders","title":"Data Loaders","description":"Data Iterator","sidebar":"apiSidebar"},"reference/Core API/inference":{"id":"reference/Core API/inference","title":"Prediction","description":"Predictions","sidebar":"apiSidebar"},"reference/Core API/modules":{"id":"reference/Core API/modules","title":"Modules","description":"Embeddings","sidebar":"apiSidebar"},"reference/index":{"id":"reference/index","title":"Eole Core API","description":"Note : These sections are built via sphinx and converted into Markdown. Some layout or links might be shaky.","sidebar":"apiSidebar"}}}')}}]);