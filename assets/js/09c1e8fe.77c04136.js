"use strict";(self.webpackChunkdocusaurus_tsx=self.webpackChunkdocusaurus_tsx||[]).push([[9210],{5309:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>f,frontMatter:()=>o,metadata:()=>d,toc:()=>a});var i=t(4848),r=t(8453);const o={},l="Models",d={id:"reference/Config/models",title:"Models",description:"Base Configs",source:"@site/docs/reference/Config/models.md",sourceDirName:"reference/Config",slug:"/reference/Config/models",permalink:"/eole/docs/reference/Config/models",draft:!1,unlisted:!1,editUrl:"https://github.com/eole-nlp/eole/tree/main/docs/docs/reference/Config/models.md",tags:[],version:"current",frontMatter:{},sidebar:"apiSidebar",previous:{title:"Inference",permalink:"/eole/docs/reference/Config/inference"},next:{title:"Main Entrypoints",permalink:"/eole/docs/reference/Config/run"}},s={},a=[{value:"Base Configs",id:"base-configs",level:2},{value:"<em>pydantic model</em> eole.config.models.BaseModelConfig[source]",id:"pydantic-model-eoleconfigmodelsbasemodelconfigsource",level:3},{value:"<em>field</em> add_estimator <em>: bool</em> <em>= False</em>",id:"field-add_estimator--bool--false",level:4},{value:"<em>field</em> architecture <em>: str | None</em> <em>= None</em>",id:"field-architecture--str--none--none",level:4},{value:"<em>field</em> decoder <em>: TransformerDecoderConfig | TransformerLMDecoderConfig | RnnDecoderConfig | CnnDecoderConfig | None</em> <em>= None</em>",id:"field-decoder--transformerdecoderconfig--transformerlmdecoderconfig--rnndecoderconfig--cnndecoderconfig--none--none",level:4},{value:"<em>field</em> embeddings <em>: EmbeddingsConfig</em> <em>[Optional]</em>",id:"field-embeddings--embeddingsconfig-optional",level:4},{value:"<em>field</em> encoder <em>: TransformerEncoderConfig | RnnEncoderConfig | CnnEncoderConfig | MeanEncoderConfig | None</em> <em>= None</em>",id:"field-encoder--transformerencoderconfig--rnnencoderconfig--cnnencoderconfig--meanencoderconfig--none--none",level:4},{value:"<em>field</em> generator_function <em>: Literal[&#39;softmax&#39;, &#39;sparsemax&#39;]</em> <em>= &#39;softmax&#39;</em>",id:"field-generator_function--literalsoftmax-sparsemax--softmax",level:4},{value:"<em>field</em> hidden_size <em>: int</em> <em>= -1</em>",id:"field-hidden_size--int---1",level:4},{value:"<em>field</em> input_feed <em>: int</em> <em>= 1</em>",id:"field-input_feed--int--1",level:4},{value:"<em>field</em> layers <em>: int</em> <em>= -1</em>",id:"field-layers--int---1",level:4},{value:"<em>field</em> left_pad <em>: bool</em> <em>= False</em>",id:"field-left_pad--bool--false",level:4},{value:"<em>field</em> share_decoder_embeddings <em>: bool</em> <em>= False</em>",id:"field-share_decoder_embeddings--bool--false",level:4},{value:"<em>field</em> share_embeddings <em>: bool</em> <em>= False</em>",id:"field-share_embeddings--bool--false",level:4},{value:"<em>field</em> transformer_ff <em>: int</em> <em>= -1</em>",id:"field-transformer_ff--int---1",level:4},{value:"<em>field</em> word_vec_size <em>: int</em> <em>= -1</em>",id:"field-word_vec_size--int---1",level:4},{value:"<em>validator</em> default_architecture  <em>\xbb</em>  <em>all fields</em>[source]",id:"validator-default_architecture----all-fieldssource",level:4},{value:"<em>validator</em> str_to_dict  <em>\xbb</em>  <em>embeddings</em> <em>,</em> <em>encoder</em> <em>,</em> <em>decoder</em>[source]",id:"validator-str_to_dict----embeddings--encoder--decodersource",level:4},{value:"update_model_opts()[source]",id:"update_model_optssource",level:4},{value:"<em>property</em> model_type <em>: ModelType</em>[source]",id:"property-model_type--modeltypesource",level:4},{value:"<em>pydantic model</em> eole.config.models.EmbeddingsConfig[source]",id:"pydantic-model-eoleconfigmodelsembeddingsconfigsource",level:3},{value:"<em>field</em> freeze_word_vecs_dec <em>: bool</em> <em>= False</em>",id:"field-freeze_word_vecs_dec--bool--false",level:4},{value:"<em>field</em> freeze_word_vecs_enc <em>: bool</em> <em>= False</em>",id:"field-freeze_word_vecs_enc--bool--false",level:4},{value:"<em>field</em> n_positions <em>: int | None</em> <em>= None</em>",id:"field-n_positions--int--none--none",level:4},{value:"<em>field</em> position_encoding <em>: bool</em> <em>= False</em>",id:"field-position_encoding--bool--false",level:4},{value:"<em>field</em> position_encoding_type <em>: PositionEncodingType | None</em> <em>= PositionEncodingType.SinusoidalInterleaved</em>",id:"field-position_encoding_type--positionencodingtype--none--positionencodingtypesinusoidalinterleaved",level:4},{value:"<em>field</em> position_shift <em>: int | None</em> <em>= 0</em>",id:"field-position_shift--int--none--0",level:4},{value:"<em>field</em> src_word_vec_size <em>: int</em> <em>= 512</em>",id:"field-src_word_vec_size--int--512",level:4},{value:"<em>field</em> tgt_word_vec_size <em>: int</em> <em>= 512</em>",id:"field-tgt_word_vec_size--int--512",level:4},{value:"<em>field</em> word_vec_size <em>: int</em> <em>= -1</em>",id:"field-word_vec_size--int---1-1",level:4},{value:"<em>validator</em> validate_embeddings  <em>\xbb</em>  <em>all fields</em>[source]",id:"validator-validate_embeddings----all-fieldssource",level:4},{value:"<em>pydantic model</em> eole.config.models.EncoderConfig[source]",id:"pydantic-model-eoleconfigmodelsencoderconfigsource",level:3},{value:"<em>field</em> encoder_type <em>: str | None</em> <em>= &#39;rnn&#39;</em>",id:"field-encoder_type--str--none--rnn",level:4},{value:"<em>field</em> hidden_size <em>: int</em> <em>= 512</em>",id:"field-hidden_size--int--512",level:4},{value:"<em>field</em> layers <em>: int</em> <em>= 2</em>",id:"field-layers--int--2",level:4},{value:"<em>field</em> src_word_vec_size <em>: int</em> <em>= 512</em>",id:"field-src_word_vec_size--int--512-1",level:4},{value:"<em>pydantic model</em> eole.config.models.DecoderConfig[source]",id:"pydantic-model-eoleconfigmodelsdecoderconfigsource",level:3},{value:"<em>field</em> coverage_attn <em>: bool</em> <em>= False</em>",id:"field-coverage_attn--bool--false",level:4},{value:"<em>field</em> decoder_type <em>: str | None</em> <em>= &#39;rnn&#39;</em>",id:"field-decoder_type--str--none--rnn",level:4},{value:"<em>field</em> global_attention <em>: Literal[&#39;dot&#39;, &#39;general&#39;, &#39;mlp&#39;, None]</em> <em>= &#39;general&#39;</em>",id:"field-global_attention--literaldot-general-mlp-none--general",level:4},{value:"<em>field</em> global_attention_function <em>: Literal[&#39;softmax&#39;, &#39;sparsemax&#39;]</em> <em>= &#39;softmax&#39;</em>",id:"field-global_attention_function--literalsoftmax-sparsemax--softmax",level:4},{value:"<em>field</em> hidden_size <em>: int</em> <em>= 512</em>",id:"field-hidden_size--int--512-1",level:4},{value:"<em>field</em> lambda_coverage <em>: float</em> <em>= 0.0</em>",id:"field-lambda_coverage--float--00",level:4},{value:"<em>field</em> layers <em>: int</em> <em>= 2</em>",id:"field-layers--int--2-1",level:4},{value:"<em>field</em> tgt_word_vec_size <em>: int</em> <em>= 512</em>",id:"field-tgt_word_vec_size--int--512-1",level:4},{value:"<em>pydantic model</em> eole.config.models.CustomModelConfig[source]",id:"pydantic-model-eoleconfigmodelscustommodelconfigsource",level:3},{value:"<em>field</em> architecture <em>: Literal[&#39;custom&#39;]</em> <em>= &#39;custom&#39;</em>",id:"field-architecture--literalcustom--custom",level:4},{value:"Transformer",id:"transformer",level:2},{value:"<em>pydantic model</em> eole.config.models.TransformerConfig[source]",id:"pydantic-model-eoleconfigmodelstransformerconfigsource",level:3},{value:"<em>field</em> add_ffnbias <em>: bool</em> <em>= False</em>",id:"field-add_ffnbias--bool--false",level:4},{value:"<em>field</em> add_qkvbias <em>: bool</em> <em>= False</em>",id:"field-add_qkvbias--bool--false",level:4},{value:"<em>field</em> heads <em>: int</em> <em>= 8</em>",id:"field-heads--int--8",level:4},{value:"<em>field</em> heads_kv <em>: int | None</em> <em>= None</em>",id:"field-heads_kv--int--none--none",level:4},{value:"<em>field</em> layer_norm <em>: Literal[&#39;standard&#39;, &#39;rms&#39;]</em> <em>= &#39;standard&#39;</em>",id:"field-layer_norm--literalstandard-rms--standard",level:4},{value:"<em>field</em> mlp_activation_fn <em>: ActivationFunction</em> <em>= ActivationFunction.relu</em>",id:"field-mlp_activation_fn--activationfunction--activationfunctionrelu",level:4},{value:"<em>field</em> n_positions <em>: int | None</em> <em>= None</em>",id:"field-n_positions--int--none--none-1",level:4},{value:"<em>field</em> norm_eps <em>: float</em> <em>= 1e-06</em>",id:"field-norm_eps--float--1e-06",level:4},{value:"<em>field</em> num_experts <em>: int</em> <em>= 0</em>",id:"field-num_experts--int--0",level:4},{value:"<em>field</em> num_experts_per_tok <em>: int</em> <em>= 2</em>",id:"field-num_experts_per_tok--int--2",level:4},{value:"<em>field</em> parallel_residual <em>: bool</em> <em>= False</em>",id:"field-parallel_residual--bool--false",level:4},{value:"<em>field</em> position_encoding_type <em>: PositionEncodingType | None</em> <em>= PositionEncodingType.SinusoidalInterleaved</em>",id:"field-position_encoding_type--positionencodingtype--none--positionencodingtypesinusoidalinterleaved-1",level:4},{value:"<em>field</em> relative_positions_buckets <em>: int</em> <em>= 0</em>",id:"field-relative_positions_buckets--int--0",level:4},{value:"<em>field</em> rotary_dim <em>: int</em> <em>= 0</em>",id:"field-rotary_dim--int--0",level:4},{value:"<em>field</em> rotary_interleave <em>: bool</em> <em>= True</em>",id:"field-rotary_interleave--bool--true",level:4},{value:"<em>field</em> rotary_theta <em>: int</em> <em>= 10000</em>",id:"field-rotary_theta--int--10000",level:4},{value:"<em>field</em> shared_layer_norm <em>: bool</em> <em>= False</em>",id:"field-shared_layer_norm--bool--false",level:4},{value:"<em>field</em> sliding_window <em>: int</em> <em>= 0</em>",id:"field-sliding_window--int--0",level:4},{value:"<em>field</em> transformer_ff <em>: int</em> <em>= 2048</em>",id:"field-transformer_ff--int--2048",level:4},{value:"<em>pydantic model</em> eole.config.models.TransformerEncoderConfig[source]",id:"pydantic-model-eoleconfigmodelstransformerencoderconfigsource",level:3},{value:"<em>field</em> encoder_type <em>: Literal[&#39;transformer&#39;]</em> <em>= &#39;transformer&#39;</em>",id:"field-encoder_type--literaltransformer--transformer",level:4},{value:"<em>pydantic model</em> eole.config.models.TransformerDecoderConfig[source]",id:"pydantic-model-eoleconfigmodelstransformerdecoderconfigsource",level:3},{value:"<em>field</em> aan_useffn <em>: bool</em> <em>= False</em>",id:"field-aan_useffn--bool--false",level:4},{value:"<em>field</em> alignment_heads <em>: int</em> <em>= 0</em>",id:"field-alignment_heads--int--0",level:4},{value:"<em>field</em> alignment_layer <em>: int</em> <em>= -2</em>",id:"field-alignment_layer--int---2",level:4},{value:"<em>field</em> decoder_type <em>: Literal[&#39;transformer&#39;]</em> <em>= &#39;transformer&#39;</em>",id:"field-decoder_type--literaltransformer--transformer",level:4},{value:"<em>field</em> full_context_alignment <em>: bool</em> <em>= False</em>",id:"field-full_context_alignment--bool--false",level:4},{value:"<em>field</em> lambda_align <em>: float</em> <em>= 0.0</em>",id:"field-lambda_align--float--00",level:4},{value:"<em>pydantic model</em> eole.config.models.TransformerLMDecoderConfig[source]",id:"pydantic-model-eoleconfigmodelstransformerlmdecoderconfigsource",level:3},{value:"<em>field</em> decoder_type <em>: Literal[&#39;transformer_lm&#39;]</em> <em>= &#39;transformer_lm&#39;</em>",id:"field-decoder_type--literaltransformer_lm--transformer_lm",level:4},{value:"<em>pydantic model</em> eole.config.models.TransformerModelConfig[source]",id:"pydantic-model-eoleconfigmodelstransformermodelconfigsource",level:3},{value:"<em>field</em> architecture <em>: Literal[&#39;transformer&#39;]</em> <em>= &#39;transformer&#39;</em>",id:"field-architecture--literaltransformer--transformer",level:4},{value:"<em>validator</em> default_architecture  <em>\xbb</em>  <em>all fields</em>[source]",id:"validator-default_architecture----all-fieldssource-1",level:4},{value:"<em>validator</em> encoder_decoder_type  <em>\xbb</em>  <em>all fields</em>[source]",id:"validator-encoder_decoder_type----all-fieldssource",level:4},{value:"<em>pydantic model</em> eole.config.models.TransformerLMModelConfig[source]",id:"pydantic-model-eoleconfigmodelstransformerlmmodelconfigsource",level:3},{value:"<em>field</em> architecture <em>: Literal[&#39;transformer_lm&#39;]</em> <em>= &#39;transformer_lm&#39;</em>",id:"field-architecture--literaltransformer_lm--transformer_lm",level:4},{value:"<em>field</em> encoder <em>: None</em> <em>= None</em>",id:"field-encoder--none--none",level:4},{value:"<em>validator</em> default_architecture  <em>\xbb</em>  <em>all fields</em>[source]",id:"validator-default_architecture----all-fieldssource-2",level:4},{value:"<em>validator</em> encoder_decoder_type  <em>\xbb</em>  <em>all fields</em>[source]",id:"validator-encoder_decoder_type----all-fieldssource-1",level:4},{value:"RNN",id:"rnn",level:2},{value:"<em>pydantic model</em> eole.config.models.RnnConfig[source]",id:"pydantic-model-eoleconfigmodelsrnnconfigsource",level:3},{value:"<em>field</em> bridge <em>: bool</em> <em>= False</em>",id:"field-bridge--bool--false",level:4},{value:"<em>field</em> rnn_type <em>: Literal[&#39;LSTM&#39;, &#39;GRU&#39;]</em> <em>= &#39;LSTM&#39;</em>",id:"field-rnn_type--literallstm-gru--lstm",level:4},{value:"<em>pydantic model</em> eole.config.models.RnnEncoderConfig[source]",id:"pydantic-model-eoleconfigmodelsrnnencoderconfigsource",level:3},{value:"<em>field</em> encoder_type <em>: Literal[&#39;rnn&#39;, &#39;brnn&#39;]</em> <em>= &#39;rnn&#39;</em>",id:"field-encoder_type--literalrnn-brnn--rnn",level:4},{value:"<em>pydantic model</em> eole.config.models.RnnDecoderConfig[source]",id:"pydantic-model-eoleconfigmodelsrnndecoderconfigsource",level:3},{value:"<em>field</em> bidirectional_encoder <em>: bool | None</em> <em>= False</em>",id:"field-bidirectional_encoder--bool--none--false",level:4},{value:"<em>field</em> context_gate <em>: Literal[&#39;source&#39;, &#39;target&#39;, &#39;both&#39;, None]</em> <em>= None</em>",id:"field-context_gate--literalsource-target-both-none--none",level:4},{value:"<em>field</em> decoder_type <em>: Literal[&#39;rnn&#39;]</em> <em>= &#39;rnn&#39;</em>",id:"field-decoder_type--literalrnn--rnn",level:4},{value:"<em>pydantic model</em> eole.config.models.RnnModelConfig[source]",id:"pydantic-model-eoleconfigmodelsrnnmodelconfigsource",level:3},{value:"<em>field</em> architecture <em>: Literal[&#39;rnn&#39;]</em> <em>= &#39;rnn&#39;</em>",id:"field-architecture--literalrnn--rnn",level:4},{value:"<em>validator</em> default_architecture  <em>\xbb</em>  <em>all fields</em>[source]",id:"validator-default_architecture----all-fieldssource-3",level:4},{value:"<em>validator</em> encoder_decoder_type  <em>\xbb</em>  <em>all fields</em>[source]",id:"validator-encoder_decoder_type----all-fieldssource-2",level:4},{value:"CNN",id:"cnn",level:2},{value:"<em>pydantic model</em> eole.config.models.CnnConfig[source]",id:"pydantic-model-eoleconfigmodelscnnconfigsource",level:3},{value:"<em>field</em> cnn_kernel_width <em>: int</em> <em>= 3</em>",id:"field-cnn_kernel_width--int--3",level:4},{value:"<em>pydantic model</em> eole.config.models.CnnEncoderConfig[source]",id:"pydantic-model-eoleconfigmodelscnnencoderconfigsource",level:3},{value:"<em>field</em> encoder_type <em>: Literal[&#39;cnn&#39;]</em> <em>= &#39;cnn&#39;</em>",id:"field-encoder_type--literalcnn--cnn",level:4},{value:"<em>pydantic model</em> eole.config.models.CnnDecoderConfig[source]",id:"pydantic-model-eoleconfigmodelscnndecoderconfigsource",level:3},{value:"<em>field</em> decoder_type <em>: Literal[&#39;cnn&#39;]</em> <em>= &#39;cnn&#39;</em>",id:"field-decoder_type--literalcnn--cnn",level:4},{value:"<em>pydantic model</em> eole.config.models.CnnModelConfig[source]",id:"pydantic-model-eoleconfigmodelscnnmodelconfigsource",level:3},{value:"<em>field</em> architecture <em>: Literal[&#39;cnn&#39;]</em> <em>= &#39;cnn&#39;</em>",id:"field-architecture--literalcnn--cnn",level:4},{value:"<em>validator</em> default_architecture  <em>\xbb</em>  <em>all fields</em>[source]",id:"validator-default_architecture----all-fieldssource-4",level:4},{value:"<em>validator</em> encoder_decoder_type  <em>\xbb</em>  <em>all fields</em>[source]",id:"validator-encoder_decoder_type----all-fieldssource-3",level:4}];function c(e){const n={a:"a",code:"code",details:"details",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",summary:"summary",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"models",children:"Models"}),"\n",(0,i.jsx)(n.h2,{id:"base-configs",children:"Base Configs"}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsbasemodelconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.BaseModelConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L323-L516",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.code,{children:"Config"})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "BaseModelConfig",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/EmbeddingsConfig"\n            }\n         ],\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "brnn": "#/$defs/RnnEncoderConfig",\n                     "cnn": "#/$defs/CnnEncoderConfig",\n                     "mean": "#/$defs/MeanEncoderConfig",\n                     "rnn": "#/$defs/RnnEncoderConfig",\n                     "transformer": "#/$defs/TransformerEncoderConfig"\n                  },\n                  "propertyName": "encoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/MeanEncoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig",\n                     "transformer_lm": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": -1,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "architecture": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "title": "Architecture"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "gated-gelu",\n            "gated-silu"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "CnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnEncoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "MeanEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "mean",\n               "default": "mean",\n               "enum": [\n                  "mean"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            }\n         },\n         "title": "MeanEncoderConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "enum": [\n                  "rnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "default": "rnn",\n               "enum": [\n                  "rnn",\n                  "brnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            }\n         },\n         "title": "RnnEncoderConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            }\n         },\n         "title": "TransformerEncoderConfig",\n         "type": "object"\n      },\n      "TransformerLMDecoderConfig": {\n         "additionalProperties": false,\n         "description": "Right now just wraps TransformerDecoderConfig for simplicity.\\nMight merge in a single class later once TransformerLM path is clarified.",\n         "properties": {\n            "decoder_type": {\n               "const": "transformer_lm",\n               "default": "transformer_lm",\n               "enum": [\n                  "transformer_lm"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerLMDecoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.add_estimator",children:(0,i.jsx)(n.code,{children:"add_estimator (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.architecture",children:(0,i.jsx)(n.code,{children:"architecture (str | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.decoder",children:(0,i.jsx)(n.code,{children:"decoder (eole.config.models.TransformerDecoderConfig | eole.config.models.TransformerLMDecoderConfig | eole.config.models.RnnDecoderConfig | eole.config.models.CnnDecoderConfig | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.embeddings",children:(0,i.jsx)(n.code,{children:"embeddings (eole.config.models.EmbeddingsConfig)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.encoder",children:(0,i.jsx)(n.code,{children:"encoder (eole.config.models.TransformerEncoderConfig | eole.config.models.RnnEncoderConfig | eole.config.models.CnnEncoderConfig | eole.config.models.MeanEncoderConfig | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.generator_function",children:(0,i.jsx)(n.code,{children:"generator_function (Literal['softmax', 'sparsemax'])"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.hidden_size",children:(0,i.jsx)(n.code,{children:"hidden_size (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.input_feed",children:(0,i.jsx)(n.code,{children:"input_feed (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.layers",children:(0,i.jsx)(n.code,{children:"layers (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.left_pad",children:(0,i.jsx)(n.code,{children:"left_pad (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.share_decoder_embeddings",children:(0,i.jsx)(n.code,{children:"share_decoder_embeddings (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.share_embeddings",children:(0,i.jsx)(n.code,{children:"share_embeddings (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.transformer_ff",children:(0,i.jsx)(n.code,{children:"transformer_ff (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.word_vec_size",children:(0,i.jsx)(n.code,{children:"word_vec_size (int)"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validators:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"_override_values"})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"_validate_model_config"})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,i.jsx)(n.code,{children:"str_to_dict"})})," \xbb ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.decoder",children:(0,i.jsx)(n.code,{children:"decoder"})})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,i.jsx)(n.code,{children:"str_to_dict"})})," \xbb ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.embeddings",children:(0,i.jsx)(n.code,{children:"embeddings"})})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,i.jsx)(n.code,{children:"str_to_dict"})})," \xbb ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.encoder",children:(0,i.jsx)(n.code,{children:"encoder"})})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-add_estimator--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," add_estimator ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Add estimator layer"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-architecture--str--none--none",children:[(0,i.jsx)(n.em,{children:"field"})," architecture ",(0,i.jsx)(n.em,{children:": str | None"})," ",(0,i.jsx)(n.em,{children:"= None"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-decoder--transformerdecoderconfig--transformerlmdecoderconfig--rnndecoderconfig--cnndecoderconfig--none--none",children:[(0,i.jsx)(n.em,{children:"field"})," decoder ",(0,i.jsxs)(n.em,{children:[": ",(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig",children:"TransformerDecoderConfig"})," | ",(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMDecoderConfig",children:"TransformerLMDecoderConfig"})," | ",(0,i.jsx)(n.a,{href:"#eole.config.models.RnnDecoderConfig",children:"RnnDecoderConfig"})," | ",(0,i.jsx)(n.a,{href:"#eole.config.models.CnnDecoderConfig",children:"CnnDecoderConfig"})," | None"]})," ",(0,i.jsx)(n.em,{children:"= None"})]}),"\n",(0,i.jsx)(n.p,{children:"Major parameters of a decoder."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,i.jsx)(n.code,{children:"str_to_dict"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-embeddings--embeddingsconfig-optional",children:[(0,i.jsx)(n.em,{children:"field"})," embeddings ",(0,i.jsxs)(n.em,{children:[": ",(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig",children:"EmbeddingsConfig"})]})," ",(0,i.jsx)(n.em,{children:"[Optional]"})]}),"\n",(0,i.jsx)(n.p,{children:"Contains most of the args useful to build the Embeddings module."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,i.jsx)(n.code,{children:"str_to_dict"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-encoder--transformerencoderconfig--rnnencoderconfig--cnnencoderconfig--meanencoderconfig--none--none",children:[(0,i.jsx)(n.em,{children:"field"})," encoder ",(0,i.jsxs)(n.em,{children:[": ",(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerEncoderConfig",children:"TransformerEncoderConfig"})," | ",(0,i.jsx)(n.a,{href:"#eole.config.models.RnnEncoderConfig",children:"RnnEncoderConfig"})," | ",(0,i.jsx)(n.a,{href:"#eole.config.models.CnnEncoderConfig",children:"CnnEncoderConfig"})," | MeanEncoderConfig | None"]})," ",(0,i.jsx)(n.em,{children:"= None"})]}),"\n",(0,i.jsx)(n.p,{children:"Major parameters of an encoder."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,i.jsx)(n.code,{children:"str_to_dict"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-generator_function--literalsoftmax-sparsemax--softmax",children:[(0,i.jsx)(n.em,{children:"field"})," generator_function ",(0,i.jsx)(n.em,{children:": Literal['softmax', 'sparsemax']"})," ",(0,i.jsx)(n.em,{children:"= 'softmax'"})]}),"\n",(0,i.jsx)(n.p,{children:"Which function to use for generating probabilities over the target vocabulary."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-hidden_size--int---1",children:[(0,i.jsx)(n.em,{children:"field"})," hidden_size ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= -1"})]}),"\n",(0,i.jsx)(n.p,{children:"Size of hidden states. Overwrites [encoder/decoder].hidden_size if set."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-input_feed--int--1",children:[(0,i.jsx)(n.em,{children:"field"})," input_feed ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 1"})]}),"\n",(0,i.jsx)(n.p,{children:"Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-layers--int---1",children:[(0,i.jsx)(n.em,{children:"field"})," layers ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= -1"})]}),"\n",(0,i.jsx)(n.p,{children:"Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers)."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-left_pad--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," left_pad ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Enable left-padding, useful for some LLMs."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-share_decoder_embeddings--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," share_decoder_embeddings ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Use a share weight matrix for the input and output word embeddings in the decoder."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-share_embeddings--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," share_embeddings ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-transformer_ff--int---1",children:[(0,i.jsx)(n.em,{children:"field"})," transformer_ff ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= -1"})]}),"\n",(0,i.jsx)(n.p,{children:"Size of hidden transformer feed-forward."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-word_vec_size--int---1",children:[(0,i.jsx)(n.em,{children:"field"})," word_vec_size ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= -1"})]}),"\n",(0,i.jsx)(n.p,{children:"Word embedding size for src and tgt."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-default_architecture----all-fieldssource",children:[(0,i.jsx)(n.em,{children:"validator"})," default_architecture  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.em,{children:"all fields"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L428-L435",children:"[source]"})]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-str_to_dict----embeddings--encoder--decodersource",children:[(0,i.jsx)(n.em,{children:"validator"})," str_to_dict  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.embeddings",children:(0,i.jsx)(n.em,{children:"embeddings"})})," ",(0,i.jsx)(n.em,{children:","})," ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.encoder",children:(0,i.jsx)(n.em,{children:"encoder"})})," ",(0,i.jsx)(n.em,{children:","})," ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.decoder",children:(0,i.jsx)(n.em,{children:"decoder"})}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L408-L417",children:"[source]"})]}),"\n",(0,i.jsxs)(n.h4,{id:"update_model_optssource",children:["update_model_opts()",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L437-L475",children:"[source]"})]}),"\n",(0,i.jsxs)(n.h4,{id:"property-model_type--modeltypesource",children:[(0,i.jsx)(n.em,{children:"property"})," model_type ",(0,i.jsx)(n.em,{children:": ModelType"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L399-L406",children:"[source]"})]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsembeddingsconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.EmbeddingsConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L12-L62",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.code,{children:"Config"})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "EmbeddingsConfig",\n   "type": "object",\n   "properties": {\n      "src_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for src.",\n         "title": "Src Word Vec Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "freeze_word_vecs_enc": {\n         "default": false,\n         "description": "Freeze word embeddings on the encoder side.",\n         "title": "Freeze Word Vecs Enc",\n         "type": "boolean"\n      },\n      "freeze_word_vecs_dec": {\n         "default": false,\n         "description": "Freeze word embeddings on the encoder side.",\n         "title": "Freeze Word Vecs Dec",\n         "type": "boolean"\n      },\n      "position_encoding": {\n         "default": false,\n         "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n         "title": "Position Encoding",\n         "type": "boolean"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      },\n      "position_shift": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": 0,\n         "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n         "title": "Position Shift"\n      }\n   },\n   "$defs": {\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.freeze_word_vecs_dec",children:(0,i.jsx)(n.code,{children:"freeze_word_vecs_dec (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.freeze_word_vecs_enc",children:(0,i.jsx)(n.code,{children:"freeze_word_vecs_enc (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.n_positions",children:(0,i.jsx)(n.code,{children:"n_positions (int | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.position_encoding",children:(0,i.jsx)(n.code,{children:"position_encoding (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.position_encoding_type",children:(0,i.jsx)(n.code,{children:"position_encoding_type (eole.constants.PositionEncodingType | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.position_shift",children:(0,i.jsx)(n.code,{children:"position_shift (int | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.src_word_vec_size",children:(0,i.jsx)(n.code,{children:"src_word_vec_size (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.tgt_word_vec_size",children:(0,i.jsx)(n.code,{children:"tgt_word_vec_size (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.word_vec_size",children:(0,i.jsx)(n.code,{children:"word_vec_size (int)"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validators:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,i.jsx)(n.code,{children:"validate_embeddings"})})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-freeze_word_vecs_dec--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," freeze_word_vecs_dec ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Freeze word embeddings on the encoder side."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,i.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-freeze_word_vecs_enc--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," freeze_word_vecs_enc ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Freeze word embeddings on the encoder side."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,i.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-n_positions--int--none--none",children:[(0,i.jsx)(n.em,{children:"field"})," n_positions ",(0,i.jsx)(n.em,{children:": int | None"})," ",(0,i.jsx)(n.em,{children:"= None"})]}),"\n",(0,i.jsx)(n.p,{children:"Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,i.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-position_encoding--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," position_encoding ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,i.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-position_encoding_type--positionencodingtype--none--positionencodingtypesinusoidalinterleaved",children:[(0,i.jsx)(n.em,{children:"field"})," position_encoding_type ",(0,i.jsx)(n.em,{children:": PositionEncodingType | None"})," ",(0,i.jsx)(n.em,{children:"= PositionEncodingType.SinusoidalInterleaved"})]}),"\n",(0,i.jsx)(n.p,{children:"Type of positional encoding."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,i.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-position_shift--int--none--0",children:[(0,i.jsx)(n.em,{children:"field"})," position_shift ",(0,i.jsx)(n.em,{children:": int | None"})," ",(0,i.jsx)(n.em,{children:"= 0"})]}),"\n",(0,i.jsx)(n.p,{children:"Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,i.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-src_word_vec_size--int--512",children:[(0,i.jsx)(n.em,{children:"field"})," src_word_vec_size ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 512"})]}),"\n",(0,i.jsx)(n.p,{children:"Word embedding size for src."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,i.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-tgt_word_vec_size--int--512",children:[(0,i.jsx)(n.em,{children:"field"})," tgt_word_vec_size ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 512"})]}),"\n",(0,i.jsx)(n.p,{children:"Word embedding size for tgt."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,i.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-word_vec_size--int---1-1",children:[(0,i.jsx)(n.em,{children:"field"})," word_vec_size ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= -1"})]}),"\n",(0,i.jsx)(n.p,{children:"Word embedding size for src and tgt."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EmbeddingsConfig.validate_embeddings",children:(0,i.jsx)(n.code,{children:"validate_embeddings"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-validate_embeddings----all-fieldssource",children:[(0,i.jsx)(n.em,{children:"validator"})," validate_embeddings  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.em,{children:"all fields"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L52-L62",children:"[source]"})]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsencoderconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.EncoderConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L65-L78",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.code,{children:"Config"})]}),"\n",(0,i.jsx)(n.p,{children:"Abstract class for all encoders"}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "EncoderConfig",\n   "description": "Abstract class for all encoders",\n   "type": "object",\n   "properties": {\n      "encoder_type": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "rnn",\n         "description": "Type of encoder layer(s) to use.",\n         "title": "Encoder Type"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the encoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of encoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "src_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for src.",\n         "title": "Src Word Vec Size",\n         "type": "integer"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EncoderConfig.encoder_type",children:(0,i.jsx)(n.code,{children:"encoder_type (str | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EncoderConfig.hidden_size",children:(0,i.jsx)(n.code,{children:"hidden_size (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EncoderConfig.layers",children:(0,i.jsx)(n.code,{children:"layers (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.EncoderConfig.src_word_vec_size",children:(0,i.jsx)(n.code,{children:"src_word_vec_size (int)"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-encoder_type--str--none--rnn",children:[(0,i.jsx)(n.em,{children:"field"})," encoder_type ",(0,i.jsx)(n.em,{children:": str | None"})," ",(0,i.jsx)(n.em,{children:"= 'rnn'"})]}),"\n",(0,i.jsx)(n.p,{children:"Type of encoder layer(s) to use."}),"\n",(0,i.jsxs)(n.h4,{id:"field-hidden_size--int--512",children:[(0,i.jsx)(n.em,{children:"field"})," hidden_size ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 512"})]}),"\n",(0,i.jsx)(n.p,{children:"Size of encoder hidden states."}),"\n",(0,i.jsxs)(n.h4,{id:"field-layers--int--2",children:[(0,i.jsx)(n.em,{children:"field"})," layers ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 2"})]}),"\n",(0,i.jsx)(n.p,{children:"Number of layers in the encoder."}),"\n",(0,i.jsxs)(n.h4,{id:"field-src_word_vec_size--int--512-1",children:[(0,i.jsx)(n.em,{children:"field"})," src_word_vec_size ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 512"})]}),"\n",(0,i.jsx)(n.p,{children:"Word embedding size for src."}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsdecoderconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.DecoderConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L81-L111",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.code,{children:"Config"})]}),"\n",(0,i.jsx)(n.p,{children:"Abstract class for all decoders"}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "DecoderConfig",\n   "description": "Abstract class for all decoders",\n   "type": "object",\n   "properties": {\n      "decoder_type": {\n         "anyOf": [\n            {\n               "type": "string"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "rnn",\n         "description": "Type of decoder layer(s) to use.",\n         "title": "Decoder Type"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the decoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of decoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "coverage_attn": {\n         "default": false,\n         "description": "Train a coverage attention layer.",\n         "title": "Coverage Attn",\n         "type": "boolean"\n      },\n      "lambda_coverage": {\n         "default": 0.0,\n         "description": "Lambda value for coverage loss of See et al (2017)",\n         "title": "Lambda Coverage",\n         "type": "number"\n      },\n      "global_attention": {\n         "default": "general",\n         "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n         "enum": [\n            "dot",\n            "general",\n            "mlp",\n            null\n         ],\n         "title": "Global Attention"\n      },\n      "global_attention_function": {\n         "default": "softmax",\n         "description": "Global attention function to use.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Global Attention Function",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.coverage_attn",children:(0,i.jsx)(n.code,{children:"coverage_attn (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.decoder_type",children:(0,i.jsx)(n.code,{children:"decoder_type (str | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.global_attention",children:(0,i.jsx)(n.code,{children:"global_attention (Literal['dot', 'general', 'mlp', None])"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.global_attention_function",children:(0,i.jsx)(n.code,{children:"global_attention_function (Literal['softmax', 'sparsemax'])"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.hidden_size",children:(0,i.jsx)(n.code,{children:"hidden_size (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.lambda_coverage",children:(0,i.jsx)(n.code,{children:"lambda_coverage (float)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.layers",children:(0,i.jsx)(n.code,{children:"layers (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig.tgt_word_vec_size",children:(0,i.jsx)(n.code,{children:"tgt_word_vec_size (int)"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validators:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"_validate_decoder_config"})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-coverage_attn--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," coverage_attn ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Train a coverage attention layer."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-decoder_type--str--none--rnn",children:[(0,i.jsx)(n.em,{children:"field"})," decoder_type ",(0,i.jsx)(n.em,{children:": str | None"})," ",(0,i.jsx)(n.em,{children:"= 'rnn'"})]}),"\n",(0,i.jsx)(n.p,{children:"Type of decoder layer(s) to use."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-global_attention--literaldot-general-mlp-none--general",children:[(0,i.jsx)(n.em,{children:"field"})," global_attention ",(0,i.jsx)(n.em,{children:": Literal['dot', 'general', 'mlp', None]"})," ",(0,i.jsx)(n.em,{children:"= 'general'"})]}),"\n",(0,i.jsx)(n.p,{children:"The attention type to use. (Luong=general, Bahdanau=MLP)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-global_attention_function--literalsoftmax-sparsemax--softmax",children:[(0,i.jsx)(n.em,{children:"field"})," global_attention_function ",(0,i.jsx)(n.em,{children:": Literal['softmax', 'sparsemax']"})," ",(0,i.jsx)(n.em,{children:"= 'softmax'"})]}),"\n",(0,i.jsx)(n.p,{children:"Global attention function to use."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-hidden_size--int--512-1",children:[(0,i.jsx)(n.em,{children:"field"})," hidden_size ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 512"})]}),"\n",(0,i.jsx)(n.p,{children:"Size of decoder hidden states."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-lambda_coverage--float--00",children:[(0,i.jsx)(n.em,{children:"field"})," lambda_coverage ",(0,i.jsx)(n.em,{children:": float"})," ",(0,i.jsx)(n.em,{children:"= 0.0"})]}),"\n",(0,i.jsx)(n.p,{children:"Lambda value for coverage loss of See et al (2017)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-layers--int--2-1",children:[(0,i.jsx)(n.em,{children:"field"})," layers ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 2"})]}),"\n",(0,i.jsx)(n.p,{children:"Number of layers in the decoder."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-tgt_word_vec_size--int--512-1",children:[(0,i.jsx)(n.em,{children:"field"})," tgt_word_vec_size ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 512"})]}),"\n",(0,i.jsx)(n.p,{children:"Word embedding size for tgt."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelscustommodelconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.CustomModelConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L519-L524",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig",children:(0,i.jsx)(n.code,{children:"BaseModelConfig"})})]}),"\n",(0,i.jsx)(n.p,{children:"Wrap anything that does not fit a set common architecture."}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "CustomModelConfig",\n   "description": "Wrap anything that does not fit a set common architecture.",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/EmbeddingsConfig"\n            }\n         ],\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "brnn": "#/$defs/RnnEncoderConfig",\n                     "cnn": "#/$defs/CnnEncoderConfig",\n                     "mean": "#/$defs/MeanEncoderConfig",\n                     "rnn": "#/$defs/RnnEncoderConfig",\n                     "transformer": "#/$defs/TransformerEncoderConfig"\n                  },\n                  "propertyName": "encoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/MeanEncoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig",\n                     "transformer_lm": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": -1,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "architecture": {\n         "const": "custom",\n         "default": "custom",\n         "enum": [\n            "custom"\n         ],\n         "title": "Architecture",\n         "type": "string"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "gated-gelu",\n            "gated-silu"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "CnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnEncoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "MeanEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "mean",\n               "default": "mean",\n               "enum": [\n                  "mean"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            }\n         },\n         "title": "MeanEncoderConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "enum": [\n                  "rnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "default": "rnn",\n               "enum": [\n                  "rnn",\n                  "brnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            }\n         },\n         "title": "RnnEncoderConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            }\n         },\n         "title": "TransformerEncoderConfig",\n         "type": "object"\n      },\n      "TransformerLMDecoderConfig": {\n         "additionalProperties": false,\n         "description": "Right now just wraps TransformerDecoderConfig for simplicity.\\nMight merge in a single class later once TransformerLM path is clarified.",\n         "properties": {\n            "decoder_type": {\n               "const": "transformer_lm",\n               "default": "transformer_lm",\n               "enum": [\n                  "transformer_lm"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerLMDecoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.CustomModelConfig.architecture",children:(0,i.jsx)(n.code,{children:"architecture (Literal['custom'])"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Validators:"})}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-architecture--literalcustom--custom",children:[(0,i.jsx)(n.em,{children:"field"})," architecture ",(0,i.jsx)(n.em,{children:": Literal['custom']"})," ",(0,i.jsx)(n.em,{children:"= 'custom'"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"transformer",children:"Transformer"}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformerconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L159-L246",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.code,{children:"Config"})]}),"\n",(0,i.jsx)(n.p,{children:"This base TransformerConfig class regroups parameters than can\nboth be set at model level or either encoder/decoder level.\nBaseModelConfig._override_values validator overrides\nencoder/decoder values with model values if relevant."}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerConfig",\n   "description": "This base TransformerConfig class regroups parameters than can\\nboth be set at model level or either encoder/decoder level.\\nBaseModelConfig._override_values validator overrides\\nencoder/decoder values with model values if relevant.",\n   "type": "object",\n   "properties": {\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "rotary_interleave": {\n         "default": true,\n         "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n         "title": "Rotary Interleave",\n         "type": "boolean"\n      },\n      "rotary_theta": {\n         "default": 10000,\n         "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n         "title": "Rotary Theta",\n         "type": "integer"\n      },\n      "rotary_dim": {\n         "default": 0,\n         "description": "Rotary dim when model requires it to be different to head dim.",\n         "title": "Rotary Dim",\n         "type": "integer"\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-06,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "gated-gelu",\n            "gated-silu"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.add_ffnbias",children:(0,i.jsx)(n.code,{children:"add_ffnbias (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.add_qkvbias",children:(0,i.jsx)(n.code,{children:"add_qkvbias (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.heads",children:(0,i.jsx)(n.code,{children:"heads (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.heads_kv",children:(0,i.jsx)(n.code,{children:"heads_kv (int | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.layer_norm",children:(0,i.jsx)(n.code,{children:"layer_norm (Literal['standard', 'rms'])"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.mlp_activation_fn",children:(0,i.jsx)(n.code,{children:"mlp_activation_fn (eole.constants.ActivationFunction)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.n_positions",children:(0,i.jsx)(n.code,{children:"n_positions (int | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.norm_eps",children:(0,i.jsx)(n.code,{children:"norm_eps (float)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.num_experts",children:(0,i.jsx)(n.code,{children:"num_experts (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.num_experts_per_tok",children:(0,i.jsx)(n.code,{children:"num_experts_per_tok (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.parallel_residual",children:(0,i.jsx)(n.code,{children:"parallel_residual (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.position_encoding_type",children:(0,i.jsx)(n.code,{children:"position_encoding_type (eole.constants.PositionEncodingType | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.relative_positions_buckets",children:(0,i.jsx)(n.code,{children:"relative_positions_buckets (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.rotary_dim",children:(0,i.jsx)(n.code,{children:"rotary_dim (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.rotary_interleave",children:(0,i.jsx)(n.code,{children:"rotary_interleave (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.rotary_theta",children:(0,i.jsx)(n.code,{children:"rotary_theta (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.shared_layer_norm",children:(0,i.jsx)(n.code,{children:"shared_layer_norm (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.sliding_window",children:(0,i.jsx)(n.code,{children:"sliding_window (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig.transformer_ff",children:(0,i.jsx)(n.code,{children:"transformer_ff (int)"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-add_ffnbias--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," add_ffnbias ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Add bias to nn.Linear of MLP FFN."}),"\n",(0,i.jsxs)(n.h4,{id:"field-add_qkvbias--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," add_qkvbias ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too."}),"\n",(0,i.jsxs)(n.h4,{id:"field-heads--int--8",children:[(0,i.jsx)(n.em,{children:"field"})," heads ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 8"})]}),"\n",(0,i.jsx)(n.p,{children:"Number of heads for transformer self-attention."}),"\n",(0,i.jsxs)(n.h4,{id:"field-heads_kv--int--none--none",children:[(0,i.jsx)(n.em,{children:"field"})," heads_kv ",(0,i.jsx)(n.em,{children:": int | None"})," ",(0,i.jsx)(n.em,{children:"= None"})]}),"\n",(0,i.jsx)(n.p,{children:"Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)"}),"\n",(0,i.jsxs)(n.h4,{id:"field-layer_norm--literalstandard-rms--standard",children:[(0,i.jsx)(n.em,{children:"field"})," layer_norm ",(0,i.jsx)(n.em,{children:": Literal['standard', 'rms']"})," ",(0,i.jsx)(n.em,{children:"= 'standard'"})]}),"\n",(0,i.jsx)(n.p,{children:"Type of layer normalization in transformer architecture."}),"\n",(0,i.jsxs)(n.h4,{id:"field-mlp_activation_fn--activationfunction--activationfunctionrelu",children:[(0,i.jsx)(n.em,{children:"field"})," mlp_activation_fn ",(0,i.jsx)(n.em,{children:": ActivationFunction"})," ",(0,i.jsx)(n.em,{children:"= ActivationFunction.relu"})]}),"\n",(0,i.jsx)(n.p,{children:"The activation function to use in MLP layer."}),"\n",(0,i.jsxs)(n.h4,{id:"field-n_positions--int--none--none-1",children:[(0,i.jsx)(n.em,{children:"field"})," n_positions ",(0,i.jsx)(n.em,{children:": int | None"})," ",(0,i.jsx)(n.em,{children:"= None"})]}),"\n",(0,i.jsx)(n.p,{children:"Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative"}),"\n",(0,i.jsxs)(n.h4,{id:"field-norm_eps--float--1e-06",children:[(0,i.jsx)(n.em,{children:"field"})," norm_eps ",(0,i.jsx)(n.em,{children:": float"})," ",(0,i.jsx)(n.em,{children:"= 1e-06"})]}),"\n",(0,i.jsx)(n.p,{children:"Layer norm epsilon."}),"\n",(0,i.jsxs)(n.h4,{id:"field-num_experts--int--0",children:[(0,i.jsx)(n.em,{children:"field"})," num_experts ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 0"})]}),"\n",(0,i.jsx)(n.p,{children:"Number of experts for MoE models."}),"\n",(0,i.jsxs)(n.h4,{id:"field-num_experts_per_tok--int--2",children:[(0,i.jsx)(n.em,{children:"field"})," num_experts_per_tok ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 2"})]}),"\n",(0,i.jsx)(n.p,{children:"Number of experts per token."}),"\n",(0,i.jsxs)(n.h4,{id:"field-parallel_residual--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," parallel_residual ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture."}),"\n",(0,i.jsxs)(n.h4,{id:"field-position_encoding_type--positionencodingtype--none--positionencodingtypesinusoidalinterleaved-1",children:[(0,i.jsx)(n.em,{children:"field"})," position_encoding_type ",(0,i.jsx)(n.em,{children:": PositionEncodingType | None"})," ",(0,i.jsx)(n.em,{children:"= PositionEncodingType.SinusoidalInterleaved"})]}),"\n",(0,i.jsx)(n.p,{children:"Type of positional encoding."}),"\n",(0,i.jsxs)(n.h4,{id:"field-relative_positions_buckets--int--0",children:[(0,i.jsx)(n.em,{children:"field"})," relative_positions_buckets ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 0"})]}),"\n",(0,i.jsxs)(n.p,{children:["Enable relative position bias (",(0,i.jsx)(n.a,{href:"https://github.com/google-research/text-to-text-transfer-transformer",children:"https://github.com/google-research/text-to-text-transfer-transformer"}),")."]}),"\n",(0,i.jsxs)(n.h4,{id:"field-rotary_dim--int--0",children:[(0,i.jsx)(n.em,{children:"field"})," rotary_dim ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 0"})]}),"\n",(0,i.jsx)(n.p,{children:"Rotary dim when model requires it to be different to head dim."}),"\n",(0,i.jsxs)(n.h4,{id:"field-rotary_interleave--bool--true",children:[(0,i.jsx)(n.em,{children:"field"})," rotary_interleave ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= True"})]}),"\n",(0,i.jsx)(n.p,{children:"Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)"}),"\n",(0,i.jsxs)(n.h4,{id:"field-rotary_theta--int--10000",children:[(0,i.jsx)(n.em,{children:"field"})," rotary_theta ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 10000"})]}),"\n",(0,i.jsx)(n.p,{children:"Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral"}),"\n",(0,i.jsxs)(n.h4,{id:"field-shared_layer_norm--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," shared_layer_norm ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models."}),"\n",(0,i.jsxs)(n.h4,{id:"field-sliding_window--int--0",children:[(0,i.jsx)(n.em,{children:"field"})," sliding_window ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 0"})]}),"\n",(0,i.jsx)(n.p,{children:"Sliding window for transformer self-attention."}),"\n",(0,i.jsxs)(n.h4,{id:"field-transformer_ff--int--2048",children:[(0,i.jsx)(n.em,{children:"field"})," transformer_ff ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 2048"})]}),"\n",(0,i.jsx)(n.p,{children:"Size of hidden transformer feed-forward."}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformerencoderconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerEncoderConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L250-L253",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig",children:(0,i.jsx)(n.code,{children:"TransformerConfig"})}),", ",(0,i.jsx)(n.a,{href:"#eole.config.models.EncoderConfig",children:(0,i.jsx)(n.code,{children:"EncoderConfig"})})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerEncoderConfig",\n   "type": "object",\n   "properties": {\n      "encoder_type": {\n         "const": "transformer",\n         "default": "transformer",\n         "enum": [\n            "transformer"\n         ],\n         "title": "Encoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the encoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of encoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "src_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for src.",\n         "title": "Src Word Vec Size",\n         "type": "integer"\n      },\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "rotary_interleave": {\n         "default": true,\n         "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n         "title": "Rotary Interleave",\n         "type": "boolean"\n      },\n      "rotary_theta": {\n         "default": 10000,\n         "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n         "title": "Rotary Theta",\n         "type": "integer"\n      },\n      "rotary_dim": {\n         "default": 0,\n         "description": "Rotary dim when model requires it to be different to head dim.",\n         "title": "Rotary Dim",\n         "type": "integer"\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-06,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "gated-gelu",\n            "gated-silu"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerEncoderConfig.encoder_type",children:(0,i.jsx)(n.code,{children:"encoder_type (Literal['transformer'])"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-encoder_type--literaltransformer--transformer",children:[(0,i.jsx)(n.em,{children:"field"})," encoder_type ",(0,i.jsx)(n.em,{children:": Literal['transformer']"})," ",(0,i.jsx)(n.em,{children:"= 'transformer'"})]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformerdecoderconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerDecoderConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L256-L307",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig",children:(0,i.jsx)(n.code,{children:"TransformerConfig"})}),", ",(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig",children:(0,i.jsx)(n.code,{children:"DecoderConfig"})})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerDecoderConfig",\n   "type": "object",\n   "properties": {\n      "decoder_type": {\n         "const": "transformer",\n         "default": "transformer",\n         "enum": [\n            "transformer"\n         ],\n         "title": "Decoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the decoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of decoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "coverage_attn": {\n         "default": false,\n         "description": "Train a coverage attention layer.",\n         "title": "Coverage Attn",\n         "type": "boolean"\n      },\n      "lambda_coverage": {\n         "default": 0.0,\n         "description": "Lambda value for coverage loss of See et al (2017)",\n         "title": "Lambda Coverage",\n         "type": "number"\n      },\n      "global_attention": {\n         "default": "general",\n         "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n         "enum": [\n            "dot",\n            "general",\n            "mlp",\n            null\n         ],\n         "title": "Global Attention"\n      },\n      "global_attention_function": {\n         "default": "softmax",\n         "description": "Global attention function to use.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Global Attention Function",\n         "type": "string"\n      },\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "rotary_interleave": {\n         "default": true,\n         "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n         "title": "Rotary Interleave",\n         "type": "boolean"\n      },\n      "rotary_theta": {\n         "default": 10000,\n         "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n         "title": "Rotary Theta",\n         "type": "integer"\n      },\n      "rotary_dim": {\n         "default": 0,\n         "description": "Rotary dim when model requires it to be different to head dim.",\n         "title": "Rotary Dim",\n         "type": "integer"\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-06,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      },\n      "aan_useffn": {\n         "default": false,\n         "description": "Turn on the FFN layer in the AAN decoder.",\n         "title": "Aan Useffn",\n         "type": "boolean"\n      },\n      "alignment_layer": {\n         "default": -2,\n         "description": "Layer number which has to be supervised.",\n         "title": "Alignment Layer",\n         "type": "integer"\n      },\n      "alignment_heads": {\n         "default": 0,\n         "description": "Number of cross attention heads per layer to supervise with.",\n         "title": "Alignment Heads",\n         "type": "integer"\n      },\n      "full_context_alignment": {\n         "default": false,\n         "description": "Whether alignment is conditioned on full target context.",\n         "title": "Full Context Alignment",\n         "type": "boolean"\n      },\n      "lambda_align": {\n         "default": 0.0,\n         "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n         "title": "Lambda Align",\n         "type": "number"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "gated-gelu",\n            "gated-silu"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.aan_useffn",children:(0,i.jsx)(n.code,{children:"aan_useffn (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.alignment_heads",children:(0,i.jsx)(n.code,{children:"alignment_heads (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.alignment_layer",children:(0,i.jsx)(n.code,{children:"alignment_layer (int)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.decoder_type",children:(0,i.jsx)(n.code,{children:"decoder_type (Literal['transformer'])"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.full_context_alignment",children:(0,i.jsx)(n.code,{children:"full_context_alignment (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig.lambda_align",children:(0,i.jsx)(n.code,{children:"lambda_align (float)"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validators:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"_validate_transformer_decoder_config"})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-aan_useffn--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," aan_useffn ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Turn on the FFN layer in the AAN decoder."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-alignment_heads--int--0",children:[(0,i.jsx)(n.em,{children:"field"})," alignment_heads ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 0"})]}),"\n",(0,i.jsx)(n.p,{children:"Number of cross attention heads per layer to supervise with."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-alignment_layer--int---2",children:[(0,i.jsx)(n.em,{children:"field"})," alignment_layer ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= -2"})]}),"\n",(0,i.jsx)(n.p,{children:"Layer number which has to be supervised."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-decoder_type--literaltransformer--transformer",children:[(0,i.jsx)(n.em,{children:"field"})," decoder_type ",(0,i.jsx)(n.em,{children:": Literal['transformer']"})," ",(0,i.jsx)(n.em,{children:"= 'transformer'"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-full_context_alignment--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," full_context_alignment ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Whether alignment is conditioned on full target context."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-lambda_align--float--00",children:[(0,i.jsx)(n.em,{children:"field"})," lambda_align ",(0,i.jsx)(n.em,{children:": float"})," ",(0,i.jsx)(n.em,{children:"= 0.0"})]}),"\n",(0,i.jsxs)(n.p,{children:["Lambda value for alignement loss of Garg et al, 2019 (",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/1909.02074",children:"https://arxiv.org/abs/1909.02074"}),")"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformerlmdecoderconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerLMDecoderConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L310-L316",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerDecoderConfig",children:(0,i.jsx)(n.code,{children:"TransformerDecoderConfig"})})]}),"\n",(0,i.jsx)(n.p,{children:"Right now just wraps TransformerDecoderConfig for simplicity.\nMight merge in a single class later once TransformerLM path is clarified."}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerLMDecoderConfig",\n   "description": "Right now just wraps TransformerDecoderConfig for simplicity.\\nMight merge in a single class later once TransformerLM path is clarified.",\n   "type": "object",\n   "properties": {\n      "decoder_type": {\n         "const": "transformer_lm",\n         "default": "transformer_lm",\n         "enum": [\n            "transformer_lm"\n         ],\n         "title": "Decoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the decoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of decoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "coverage_attn": {\n         "default": false,\n         "description": "Train a coverage attention layer.",\n         "title": "Coverage Attn",\n         "type": "boolean"\n      },\n      "lambda_coverage": {\n         "default": 0.0,\n         "description": "Lambda value for coverage loss of See et al (2017)",\n         "title": "Lambda Coverage",\n         "type": "number"\n      },\n      "global_attention": {\n         "default": "general",\n         "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n         "enum": [\n            "dot",\n            "general",\n            "mlp",\n            null\n         ],\n         "title": "Global Attention"\n      },\n      "global_attention_function": {\n         "default": "softmax",\n         "description": "Global attention function to use.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Global Attention Function",\n         "type": "string"\n      },\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "rotary_interleave": {\n         "default": true,\n         "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n         "title": "Rotary Interleave",\n         "type": "boolean"\n      },\n      "rotary_theta": {\n         "default": 10000,\n         "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n         "title": "Rotary Theta",\n         "type": "integer"\n      },\n      "rotary_dim": {\n         "default": 0,\n         "description": "Rotary dim when model requires it to be different to head dim.",\n         "title": "Rotary Dim",\n         "type": "integer"\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-06,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      },\n      "aan_useffn": {\n         "default": false,\n         "description": "Turn on the FFN layer in the AAN decoder.",\n         "title": "Aan Useffn",\n         "type": "boolean"\n      },\n      "alignment_layer": {\n         "default": -2,\n         "description": "Layer number which has to be supervised.",\n         "title": "Alignment Layer",\n         "type": "integer"\n      },\n      "alignment_heads": {\n         "default": 0,\n         "description": "Number of cross attention heads per layer to supervise with.",\n         "title": "Alignment Heads",\n         "type": "integer"\n      },\n      "full_context_alignment": {\n         "default": false,\n         "description": "Whether alignment is conditioned on full target context.",\n         "title": "Full Context Alignment",\n         "type": "boolean"\n      },\n      "lambda_align": {\n         "default": 0.0,\n         "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n         "title": "Lambda Align",\n         "type": "number"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "gated-gelu",\n            "gated-silu"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMDecoderConfig.decoder_type",children:(0,i.jsx)(n.code,{children:"decoder_type (Literal['transformer_lm'])"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Validators:"})}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-decoder_type--literaltransformer_lm--transformer_lm",children:[(0,i.jsx)(n.em,{children:"field"})," decoder_type ",(0,i.jsx)(n.em,{children:": Literal['transformer_lm']"})," ",(0,i.jsx)(n.em,{children:"= 'transformer_lm'"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_transformer_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformermodelconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerModelConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L586-L620",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig",children:(0,i.jsx)(n.code,{children:"TransformerConfig"})}),", ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig",children:(0,i.jsx)(n.code,{children:"BaseModelConfig"})})]}),"\n",(0,i.jsx)(n.p,{children:"Facilitate setting some transformer specific params at model level."}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerModelConfig",\n   "description": "Facilitate setting some transformer specific params at model level.",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/EmbeddingsConfig"\n            }\n         ],\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "brnn": "#/$defs/RnnEncoderConfig",\n                     "cnn": "#/$defs/CnnEncoderConfig",\n                     "mean": "#/$defs/MeanEncoderConfig",\n                     "rnn": "#/$defs/RnnEncoderConfig",\n                     "transformer": "#/$defs/TransformerEncoderConfig"\n                  },\n                  "propertyName": "encoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/MeanEncoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig",\n                     "transformer_lm": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "architecture": {\n         "const": "transformer",\n         "default": "transformer",\n         "enum": [\n            "transformer"\n         ],\n         "title": "Architecture",\n         "type": "string"\n      },\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "rotary_interleave": {\n         "default": true,\n         "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n         "title": "Rotary Interleave",\n         "type": "boolean"\n      },\n      "rotary_theta": {\n         "default": 10000,\n         "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n         "title": "Rotary Theta",\n         "type": "integer"\n      },\n      "rotary_dim": {\n         "default": 0,\n         "description": "Rotary dim when model requires it to be different to head dim.",\n         "title": "Rotary Dim",\n         "type": "integer"\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-06,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "gated-gelu",\n            "gated-silu"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "CnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnEncoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "MeanEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "mean",\n               "default": "mean",\n               "enum": [\n                  "mean"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            }\n         },\n         "title": "MeanEncoderConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "enum": [\n                  "rnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "default": "rnn",\n               "enum": [\n                  "rnn",\n                  "brnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            }\n         },\n         "title": "RnnEncoderConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            }\n         },\n         "title": "TransformerEncoderConfig",\n         "type": "object"\n      },\n      "TransformerLMDecoderConfig": {\n         "additionalProperties": false,\n         "description": "Right now just wraps TransformerDecoderConfig for simplicity.\\nMight merge in a single class later once TransformerLM path is clarified.",\n         "properties": {\n            "decoder_type": {\n               "const": "transformer_lm",\n               "default": "transformer_lm",\n               "enum": [\n                  "transformer_lm"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerLMDecoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerModelConfig.architecture",children:(0,i.jsx)(n.code,{children:"architecture (Literal['transformer'])"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validators:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"_validate_transformer"})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerModelConfig.encoder_decoder_type",children:(0,i.jsx)(n.code,{children:"encoder_decoder_type"})})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-architecture--literaltransformer--transformer",children:[(0,i.jsx)(n.em,{children:"field"})," architecture ",(0,i.jsx)(n.em,{children:": Literal['transformer']"})," ",(0,i.jsx)(n.em,{children:"= 'transformer'"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_transformer"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerModelConfig.encoder_decoder_type",children:(0,i.jsx)(n.code,{children:"encoder_decoder_type"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-default_architecture----all-fieldssource-1",children:[(0,i.jsx)(n.em,{children:"validator"})," default_architecture  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.em,{children:"all fields"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L609-L616",children:"[source]"})]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-encoder_decoder_type----all-fieldssource",children:[(0,i.jsx)(n.em,{children:"validator"})," encoder_decoder_type  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.em,{children:"all fields"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L593-L607",children:"[source]"})]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelstransformerlmmodelconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.TransformerLMModelConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L623-L657",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerConfig",children:(0,i.jsx)(n.code,{children:"TransformerConfig"})}),", ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig",children:(0,i.jsx)(n.code,{children:"BaseModelConfig"})})]}),"\n",(0,i.jsx)(n.p,{children:"Facilitate setting some transformer specific params at model level."}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "TransformerLMModelConfig",\n   "description": "Facilitate setting some transformer specific params at model level.",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/EmbeddingsConfig"\n            }\n         ],\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder",\n         "type": "null"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig",\n                     "transformer_lm": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": 2048,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "architecture": {\n         "const": "transformer_lm",\n         "default": "transformer_lm",\n         "enum": [\n            "transformer_lm"\n         ],\n         "title": "Architecture",\n         "type": "string"\n      },\n      "sliding_window": {\n         "default": 0,\n         "description": "Sliding window for transformer self-attention.",\n         "title": "Sliding Window",\n         "type": "integer"\n      },\n      "heads": {\n         "default": 8,\n         "description": "Number of heads for transformer self-attention.",\n         "title": "Heads",\n         "type": "integer"\n      },\n      "relative_positions_buckets": {\n         "default": 0,\n         "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n         "title": "Relative Positions Buckets",\n         "type": "integer"\n      },\n      "mlp_activation_fn": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/ActivationFunction"\n            }\n         ],\n         "default": "relu",\n         "description": "The activation function to use in MLP layer."\n      },\n      "rotary_interleave": {\n         "default": true,\n         "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n         "title": "Rotary Interleave",\n         "type": "boolean"\n      },\n      "rotary_theta": {\n         "default": 10000,\n         "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n         "title": "Rotary Theta",\n         "type": "integer"\n      },\n      "rotary_dim": {\n         "default": 0,\n         "description": "Rotary dim when model requires it to be different to head dim.",\n         "title": "Rotary Dim",\n         "type": "integer"\n      },\n      "layer_norm": {\n         "default": "standard",\n         "description": "Type of layer normalization in transformer architecture.",\n         "enum": [\n            "standard",\n            "rms"\n         ],\n         "title": "Layer Norm",\n         "type": "string"\n      },\n      "norm_eps": {\n         "default": 1e-06,\n         "description": "Layer norm epsilon.",\n         "title": "Norm Eps",\n         "type": "number"\n      },\n      "shared_layer_norm": {\n         "default": false,\n         "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n         "title": "Shared Layer Norm",\n         "type": "boolean"\n      },\n      "add_qkvbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n         "title": "Add Qkvbias",\n         "type": "boolean"\n      },\n      "heads_kv": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n         "title": "Heads Kv"\n      },\n      "add_ffnbias": {\n         "default": false,\n         "description": "Add bias to nn.Linear of MLP FFN.",\n         "title": "Add Ffnbias",\n         "type": "boolean"\n      },\n      "parallel_residual": {\n         "default": false,\n         "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n         "title": "Parallel Residual",\n         "type": "boolean"\n      },\n      "num_experts": {\n         "default": 0,\n         "description": "Number of experts for MoE models.",\n         "title": "Num Experts",\n         "type": "integer"\n      },\n      "num_experts_per_tok": {\n         "default": 2,\n         "description": "Number of experts per token.",\n         "title": "Num Experts Per Tok",\n         "type": "integer"\n      },\n      "position_encoding_type": {\n         "anyOf": [\n            {\n               "$ref": "#/$defs/PositionEncodingType"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": "SinusoidalInterleaved",\n         "description": "Type of positional encoding."\n      },\n      "n_positions": {\n         "anyOf": [\n            {\n               "type": "integer"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n         "title": "N Positions"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "gated-gelu",\n            "gated-silu"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "enum": [\n                  "rnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerLMDecoderConfig": {\n         "additionalProperties": false,\n         "description": "Right now just wraps TransformerDecoderConfig for simplicity.\\nMight merge in a single class later once TransformerLM path is clarified.",\n         "properties": {\n            "decoder_type": {\n               "const": "transformer_lm",\n               "default": "transformer_lm",\n               "enum": [\n                  "transformer_lm"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerLMDecoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.architecture",children:(0,i.jsx)(n.code,{children:"architecture (Literal['transformer_lm'])"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.encoder",children:(0,i.jsx)(n.code,{children:"encoder (None)"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validators:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"_validate_transformer"})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.encoder_decoder_type",children:(0,i.jsx)(n.code,{children:"encoder_decoder_type"})})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,i.jsx)(n.code,{children:"str_to_dict"})})," \xbb ",(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.encoder",children:(0,i.jsx)(n.code,{children:"encoder"})})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-architecture--literaltransformer_lm--transformer_lm",children:[(0,i.jsx)(n.em,{children:"field"})," architecture ",(0,i.jsx)(n.em,{children:": Literal['transformer_lm']"})," ",(0,i.jsx)(n.em,{children:"= 'transformer_lm'"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_transformer"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.encoder_decoder_type",children:(0,i.jsx)(n.code,{children:"encoder_decoder_type"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-encoder--none--none",children:[(0,i.jsx)(n.em,{children:"field"})," encoder ",(0,i.jsx)(n.em,{children:": None"})," ",(0,i.jsx)(n.em,{children:"= None"})]}),"\n",(0,i.jsx)(n.p,{children:"Major parameters of an encoder."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_transformer"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.TransformerLMModelConfig.encoder_decoder_type",children:(0,i.jsx)(n.code,{children:"encoder_decoder_type"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig.str_to_dict",children:(0,i.jsx)(n.code,{children:"str_to_dict"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-default_architecture----all-fieldssource-2",children:[(0,i.jsx)(n.em,{children:"validator"})," default_architecture  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.em,{children:"all fields"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L645-L652",children:"[source]"})]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-encoder_decoder_type----all-fieldssource-1",children:[(0,i.jsx)(n.em,{children:"validator"})," encoder_decoder_type  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.em,{children:"all fields"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L633-L643",children:"[source]"})]}),"\n",(0,i.jsx)(n.h2,{id:"rnn",children:"RNN"}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsrnnconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.RnnConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L114-L124",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.code,{children:"Config"})]}),"\n",(0,i.jsx)(n.p,{children:"Just to facilitate testing discriminator stuff."}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "RnnConfig",\n   "description": "Just to facilitate testing discriminator stuff.",\n   "type": "object",\n   "properties": {\n      "bridge": {\n         "default": false,\n         "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n         "title": "Bridge",\n         "type": "boolean"\n      },\n      "rnn_type": {\n         "default": "LSTM",\n         "description": "The gate type to use in the RNNs.",\n         "enum": [\n            "LSTM",\n            "GRU"\n         ],\n         "title": "Rnn Type",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.RnnConfig.bridge",children:(0,i.jsx)(n.code,{children:"bridge (bool)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.RnnConfig.rnn_type",children:(0,i.jsx)(n.code,{children:"rnn_type (Literal['LSTM', 'GRU'])"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-bridge--bool--false",children:[(0,i.jsx)(n.em,{children:"field"})," bridge ",(0,i.jsx)(n.em,{children:": bool"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsx)(n.p,{children:"Have an additional layer between the last encoder state and the first decoder state (RNN specific)."}),"\n",(0,i.jsxs)(n.h4,{id:"field-rnn_type--literallstm-gru--lstm",children:[(0,i.jsx)(n.em,{children:"field"})," rnn_type ",(0,i.jsx)(n.em,{children:": Literal['LSTM', 'GRU']"})," ",(0,i.jsx)(n.em,{children:"= 'LSTM'"})]}),"\n",(0,i.jsx)(n.p,{children:"The gate type to use in the RNNs."}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsrnnencoderconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.RnnEncoderConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L127-L128",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.RnnConfig",children:(0,i.jsx)(n.code,{children:"RnnConfig"})}),", ",(0,i.jsx)(n.a,{href:"#eole.config.models.EncoderConfig",children:(0,i.jsx)(n.code,{children:"EncoderConfig"})})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "RnnEncoderConfig",\n   "type": "object",\n   "properties": {\n      "encoder_type": {\n         "default": "rnn",\n         "enum": [\n            "rnn",\n            "brnn"\n         ],\n         "title": "Encoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the encoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of encoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "src_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for src.",\n         "title": "Src Word Vec Size",\n         "type": "integer"\n      },\n      "bridge": {\n         "default": false,\n         "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n         "title": "Bridge",\n         "type": "boolean"\n      },\n      "rnn_type": {\n         "default": "LSTM",\n         "description": "The gate type to use in the RNNs.",\n         "enum": [\n            "LSTM",\n            "GRU"\n         ],\n         "title": "Rnn Type",\n         "type": "string"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.RnnEncoderConfig.encoder_type",children:(0,i.jsx)(n.code,{children:"encoder_type (Literal['rnn', 'brnn'])"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-encoder_type--literalrnn-brnn--rnn",children:[(0,i.jsx)(n.em,{children:"field"})," encoder_type ",(0,i.jsx)(n.em,{children:": Literal['rnn', 'brnn']"})," ",(0,i.jsx)(n.em,{children:"= 'rnn'"})]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsrnndecoderconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.RnnDecoderConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L131-L136",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.RnnConfig",children:(0,i.jsx)(n.code,{children:"RnnConfig"})}),", ",(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig",children:(0,i.jsx)(n.code,{children:"DecoderConfig"})})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "RnnDecoderConfig",\n   "type": "object",\n   "properties": {\n      "decoder_type": {\n         "const": "rnn",\n         "default": "rnn",\n         "enum": [\n            "rnn"\n         ],\n         "title": "Decoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the decoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of decoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "coverage_attn": {\n         "default": false,\n         "description": "Train a coverage attention layer.",\n         "title": "Coverage Attn",\n         "type": "boolean"\n      },\n      "lambda_coverage": {\n         "default": 0.0,\n         "description": "Lambda value for coverage loss of See et al (2017)",\n         "title": "Lambda Coverage",\n         "type": "number"\n      },\n      "global_attention": {\n         "default": "general",\n         "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n         "enum": [\n            "dot",\n            "general",\n            "mlp",\n            null\n         ],\n         "title": "Global Attention"\n      },\n      "global_attention_function": {\n         "default": "softmax",\n         "description": "Global attention function to use.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Global Attention Function",\n         "type": "string"\n      },\n      "bridge": {\n         "default": false,\n         "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n         "title": "Bridge",\n         "type": "boolean"\n      },\n      "rnn_type": {\n         "default": "LSTM",\n         "description": "The gate type to use in the RNNs.",\n         "enum": [\n            "LSTM",\n            "GRU"\n         ],\n         "title": "Rnn Type",\n         "type": "string"\n      },\n      "context_gate": {\n         "default": null,\n         "description": "Type of context gate to use.",\n         "enum": [\n            "source",\n            "target",\n            "both",\n            null\n         ],\n         "title": "Context Gate"\n      },\n      "bidirectional_encoder": {\n         "anyOf": [\n            {\n               "type": "boolean"\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": false,\n         "title": "Bidirectional Encoder"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.RnnDecoderConfig.bidirectional_encoder",children:(0,i.jsx)(n.code,{children:"bidirectional_encoder (bool | None)"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.RnnDecoderConfig.context_gate",children:(0,i.jsx)(n.code,{children:"context_gate (Literal['source', 'target', 'both', None])"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.RnnDecoderConfig.decoder_type",children:(0,i.jsx)(n.code,{children:"decoder_type (Literal['rnn'])"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Validators:"})}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-bidirectional_encoder--bool--none--false",children:[(0,i.jsx)(n.em,{children:"field"})," bidirectional_encoder ",(0,i.jsx)(n.em,{children:": bool | None"})," ",(0,i.jsx)(n.em,{children:"= False"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-context_gate--literalsource-target-both-none--none",children:[(0,i.jsx)(n.em,{children:"field"})," context_gate ",(0,i.jsx)(n.em,{children:": Literal['source', 'target', 'both', None]"})," ",(0,i.jsx)(n.em,{children:"= None"})]}),"\n",(0,i.jsx)(n.p,{children:"Type of context gate to use."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-decoder_type--literalrnn--rnn",children:[(0,i.jsx)(n.em,{children:"field"})," decoder_type ",(0,i.jsx)(n.em,{children:": Literal['rnn']"})," ",(0,i.jsx)(n.em,{children:"= 'rnn'"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelsrnnmodelconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.RnnModelConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L527-L553",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.RnnConfig",children:(0,i.jsx)(n.code,{children:"RnnConfig"})}),", ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig",children:(0,i.jsx)(n.code,{children:"BaseModelConfig"})})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "RnnModelConfig",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/EmbeddingsConfig"\n            }\n         ],\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "brnn": "#/$defs/RnnEncoderConfig",\n                     "cnn": "#/$defs/CnnEncoderConfig",\n                     "mean": "#/$defs/MeanEncoderConfig",\n                     "rnn": "#/$defs/RnnEncoderConfig",\n                     "transformer": "#/$defs/TransformerEncoderConfig"\n                  },\n                  "propertyName": "encoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/MeanEncoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig",\n                     "transformer_lm": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": -1,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "architecture": {\n         "const": "rnn",\n         "default": "rnn",\n         "enum": [\n            "rnn"\n         ],\n         "title": "Architecture",\n         "type": "string"\n      },\n      "bridge": {\n         "default": false,\n         "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n         "title": "Bridge",\n         "type": "boolean"\n      },\n      "rnn_type": {\n         "default": "LSTM",\n         "description": "The gate type to use in the RNNs.",\n         "enum": [\n            "LSTM",\n            "GRU"\n         ],\n         "title": "Rnn Type",\n         "type": "string"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "gated-gelu",\n            "gated-silu"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "CnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnEncoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "MeanEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "mean",\n               "default": "mean",\n               "enum": [\n                  "mean"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            }\n         },\n         "title": "MeanEncoderConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "enum": [\n                  "rnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "default": "rnn",\n               "enum": [\n                  "rnn",\n                  "brnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            }\n         },\n         "title": "RnnEncoderConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            }\n         },\n         "title": "TransformerEncoderConfig",\n         "type": "object"\n      },\n      "TransformerLMDecoderConfig": {\n         "additionalProperties": false,\n         "description": "Right now just wraps TransformerDecoderConfig for simplicity.\\nMight merge in a single class later once TransformerLM path is clarified.",\n         "properties": {\n            "decoder_type": {\n               "const": "transformer_lm",\n               "default": "transformer_lm",\n               "enum": [\n                  "transformer_lm"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerLMDecoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.RnnModelConfig.architecture",children:(0,i.jsx)(n.code,{children:"architecture (Literal['rnn'])"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validators:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.RnnModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.RnnModelConfig.encoder_decoder_type",children:(0,i.jsx)(n.code,{children:"encoder_decoder_type"})})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-architecture--literalrnn--rnn",children:[(0,i.jsx)(n.em,{children:"field"})," architecture ",(0,i.jsx)(n.em,{children:": Literal['rnn']"})," ",(0,i.jsx)(n.em,{children:"= 'rnn'"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.RnnModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.RnnModelConfig.encoder_decoder_type",children:(0,i.jsx)(n.code,{children:"encoder_decoder_type"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-default_architecture----all-fieldssource-3",children:[(0,i.jsx)(n.em,{children:"validator"})," default_architecture  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.em,{children:"all fields"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L546-L553",children:"[source]"})]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-encoder_decoder_type----all-fieldssource-2",children:[(0,i.jsx)(n.em,{children:"validator"})," encoder_decoder_type  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.em,{children:"all fields"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L530-L544",children:"[source]"})]}),"\n",(0,i.jsx)(n.h2,{id:"cnn",children:"CNN"}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelscnnconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.CnnConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L139-L144",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.code,{children:"Config"})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "CnnConfig",\n   "type": "object",\n   "properties": {\n      "cnn_kernel_width": {\n         "default": 3,\n         "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n         "title": "Cnn Kernel Width",\n         "type": "integer"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.CnnConfig.cnn_kernel_width",children:(0,i.jsx)(n.code,{children:"cnn_kernel_width (int)"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-cnn_kernel_width--int--3",children:[(0,i.jsx)(n.em,{children:"field"})," cnn_kernel_width ",(0,i.jsx)(n.em,{children:": int"})," ",(0,i.jsx)(n.em,{children:"= 3"})]}),"\n",(0,i.jsx)(n.p,{children:"Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers."}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelscnnencoderconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.CnnEncoderConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L147-L148",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.CnnConfig",children:(0,i.jsx)(n.code,{children:"CnnConfig"})}),", ",(0,i.jsx)(n.a,{href:"#eole.config.models.EncoderConfig",children:(0,i.jsx)(n.code,{children:"EncoderConfig"})})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "CnnEncoderConfig",\n   "type": "object",\n   "properties": {\n      "encoder_type": {\n         "const": "cnn",\n         "default": "cnn",\n         "enum": [\n            "cnn"\n         ],\n         "title": "Encoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the encoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of encoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "src_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for src.",\n         "title": "Src Word Vec Size",\n         "type": "integer"\n      },\n      "cnn_kernel_width": {\n         "default": 3,\n         "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n         "title": "Cnn Kernel Width",\n         "type": "integer"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.CnnEncoderConfig.encoder_type",children:(0,i.jsx)(n.code,{children:"encoder_type (Literal['cnn'])"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-encoder_type--literalcnn--cnn",children:[(0,i.jsx)(n.em,{children:"field"})," encoder_type ",(0,i.jsx)(n.em,{children:": Literal['cnn']"})," ",(0,i.jsx)(n.em,{children:"= 'cnn'"})]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelscnndecoderconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.CnnDecoderConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L151-L152",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.CnnConfig",children:(0,i.jsx)(n.code,{children:"CnnConfig"})}),", ",(0,i.jsx)(n.a,{href:"#eole.config.models.DecoderConfig",children:(0,i.jsx)(n.code,{children:"DecoderConfig"})})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "CnnDecoderConfig",\n   "type": "object",\n   "properties": {\n      "decoder_type": {\n         "const": "cnn",\n         "default": "cnn",\n         "enum": [\n            "cnn"\n         ],\n         "title": "Decoder Type",\n         "type": "string"\n      },\n      "layers": {\n         "default": 2,\n         "description": "Number of layers in the decoder.",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "hidden_size": {\n         "default": 512,\n         "description": "Size of decoder hidden states.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "tgt_word_vec_size": {\n         "default": 512,\n         "description": "Word embedding size for tgt.",\n         "title": "Tgt Word Vec Size",\n         "type": "integer"\n      },\n      "coverage_attn": {\n         "default": false,\n         "description": "Train a coverage attention layer.",\n         "title": "Coverage Attn",\n         "type": "boolean"\n      },\n      "lambda_coverage": {\n         "default": 0.0,\n         "description": "Lambda value for coverage loss of See et al (2017)",\n         "title": "Lambda Coverage",\n         "type": "number"\n      },\n      "global_attention": {\n         "default": "general",\n         "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n         "enum": [\n            "dot",\n            "general",\n            "mlp",\n            null\n         ],\n         "title": "Global Attention"\n      },\n      "global_attention_function": {\n         "default": "softmax",\n         "description": "Global attention function to use.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Global Attention Function",\n         "type": "string"\n      },\n      "cnn_kernel_width": {\n         "default": 3,\n         "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n         "title": "Cnn Kernel Width",\n         "type": "integer"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.CnnDecoderConfig.decoder_type",children:(0,i.jsx)(n.code,{children:"decoder_type (Literal['cnn'])"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Validators:"})}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-decoder_type--literalcnn--cnn",children:[(0,i.jsx)(n.em,{children:"field"})," decoder_type ",(0,i.jsx)(n.em,{children:": Literal['cnn']"})," ",(0,i.jsx)(n.em,{children:"= 'cnn'"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_decoder_config"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"pydantic-model-eoleconfigmodelscnnmodelconfigsource",children:[(0,i.jsx)(n.em,{children:"pydantic model"})," eole.config.models.CnnModelConfig",(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L556-L583",children:"[source]"})]}),"\n",(0,i.jsxs)(n.p,{children:["Bases: ",(0,i.jsx)(n.a,{href:"#eole.config.models.CnnConfig",children:(0,i.jsx)(n.code,{children:"CnnConfig"})}),", ",(0,i.jsx)(n.a,{href:"#eole.config.models.BaseModelConfig",children:(0,i.jsx)(n.code,{children:"BaseModelConfig"})})]}),"\n",(0,i.jsx)(n.p,{}),(0,i.jsxs)(n.details,{className:"autodoc_pydantic_collapsable_json",children:["\n",(0,i.jsx)(n.summary,{children:"Show JSON schema"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n   "title": "CnnModelConfig",\n   "type": "object",\n   "properties": {\n      "embeddings": {\n         "allOf": [\n            {\n               "$ref": "#/$defs/EmbeddingsConfig"\n            }\n         ],\n         "description": "Contains most of the args useful to build the Embeddings module."\n      },\n      "encoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "brnn": "#/$defs/RnnEncoderConfig",\n                     "cnn": "#/$defs/CnnEncoderConfig",\n                     "mean": "#/$defs/MeanEncoderConfig",\n                     "rnn": "#/$defs/RnnEncoderConfig",\n                     "transformer": "#/$defs/TransformerEncoderConfig"\n                  },\n                  "propertyName": "encoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnEncoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/MeanEncoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of an encoder.",\n         "title": "Encoder"\n      },\n      "decoder": {\n         "anyOf": [\n            {\n               "discriminator": {\n                  "mapping": {\n                     "cnn": "#/$defs/CnnDecoderConfig",\n                     "rnn": "#/$defs/RnnDecoderConfig",\n                     "transformer": "#/$defs/TransformerDecoderConfig",\n                     "transformer_lm": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  "propertyName": "decoder_type"\n               },\n               "oneOf": [\n                  {\n                     "$ref": "#/$defs/TransformerDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/TransformerLMDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/RnnDecoderConfig"\n                  },\n                  {\n                     "$ref": "#/$defs/CnnDecoderConfig"\n                  }\n               ]\n            },\n            {\n               "type": "null"\n            }\n         ],\n         "default": null,\n         "description": "Major parameters of a decoder.",\n         "title": "Decoder"\n      },\n      "hidden_size": {\n         "default": -1,\n         "description": "Size of hidden states. Overwrites [encoder/decoder].hidden_size if set.",\n         "title": "Hidden Size",\n         "type": "integer"\n      },\n      "word_vec_size": {\n         "default": -1,\n         "description": "Word embedding size for src and tgt.",\n         "title": "Word Vec Size",\n         "type": "integer"\n      },\n      "layers": {\n         "default": -1,\n         "description": "Number of layers in both encoder and decoder (will overwrite enc_layers/dec_layers).",\n         "title": "Layers",\n         "type": "integer"\n      },\n      "transformer_ff": {\n         "default": -1,\n         "description": "Size of hidden transformer feed-forward.",\n         "title": "Transformer Ff",\n         "type": "integer"\n      },\n      "share_decoder_embeddings": {\n         "default": false,\n         "description": "Use a share weight matrix for the input and output word embeddings in the decoder.",\n         "title": "Share Decoder Embeddings",\n         "type": "boolean"\n      },\n      "share_embeddings": {\n         "default": false,\n         "description": "Share the word embeddings between encoder and decoder. Need to use shared vocabulary for this option.",\n         "title": "Share Embeddings",\n         "type": "boolean"\n      },\n      "input_feed": {\n         "default": 1,\n         "description": "Feed the context vector at each time step as additional input (via concatenation with the word embeddings) to the decoder.",\n         "title": "Input Feed",\n         "type": "integer"\n      },\n      "generator_function": {\n         "default": "softmax",\n         "description": "Which function to use for generating probabilities over the target vocabulary.",\n         "enum": [\n            "softmax",\n            "sparsemax"\n         ],\n         "title": "Generator Function",\n         "type": "string"\n      },\n      "add_estimator": {\n         "default": false,\n         "description": "Add estimator layer",\n         "title": "Add Estimator",\n         "type": "boolean"\n      },\n      "left_pad": {\n         "default": false,\n         "description": "Enable left-padding, useful for some LLMs.",\n         "title": "Left Pad",\n         "type": "boolean"\n      },\n      "architecture": {\n         "const": "cnn",\n         "default": "cnn",\n         "enum": [\n            "cnn"\n         ],\n         "title": "Architecture",\n         "type": "string"\n      },\n      "cnn_kernel_width": {\n         "default": 3,\n         "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n         "title": "Cnn Kernel Width",\n         "type": "integer"\n      }\n   },\n   "$defs": {\n      "ActivationFunction": {\n         "enum": [\n            "relu",\n            "gelu",\n            "silu",\n            "gated-gelu",\n            "gated-silu"\n         ],\n         "title": "ActivationFunction",\n         "type": "string"\n      },\n      "CnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnDecoderConfig",\n         "type": "object"\n      },\n      "CnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "cnn",\n               "default": "cnn",\n               "enum": [\n                  "cnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "cnn_kernel_width": {\n               "default": 3,\n               "description": "Size of windows in the cnn, the kernel_size is (cnn_kernel_width, 1) in convolution layers.",\n               "title": "Cnn Kernel Width",\n               "type": "integer"\n            }\n         },\n         "title": "CnnEncoderConfig",\n         "type": "object"\n      },\n      "EmbeddingsConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "word_vec_size": {\n               "default": -1,\n               "description": "Word embedding size for src and tgt.",\n               "title": "Word Vec Size",\n               "type": "integer"\n            },\n            "freeze_word_vecs_enc": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Enc",\n               "type": "boolean"\n            },\n            "freeze_word_vecs_dec": {\n               "default": false,\n               "description": "Freeze word embeddings on the encoder side.",\n               "title": "Freeze Word Vecs Dec",\n               "type": "boolean"\n            },\n            "position_encoding": {\n               "default": false,\n               "description": "Absolute position encoding, see position_encoding_type. Necessary for non-RNN style models.",\n               "title": "Position Encoding",\n               "type": "boolean"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "position_shift": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": 0,\n               "description": "Positions IDS shift before making position embed dirty patch to cover for xlm-roberta-xl",\n               "title": "Position Shift"\n            }\n         },\n         "title": "EmbeddingsConfig",\n         "type": "object"\n      },\n      "MeanEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "mean",\n               "default": "mean",\n               "enum": [\n                  "mean"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            }\n         },\n         "title": "MeanEncoderConfig",\n         "type": "object"\n      },\n      "PositionEncodingType": {\n         "enum": [\n            "SinusoidalInterleaved",\n            "SinusoidalConcat",\n            "Learned",\n            "Relative",\n            "Rotary",\n            "Alibi"\n         ],\n         "title": "PositionEncodingType",\n         "type": "string"\n      },\n      "RnnDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "rnn",\n               "default": "rnn",\n               "enum": [\n                  "rnn"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            },\n            "context_gate": {\n               "default": null,\n               "description": "Type of context gate to use.",\n               "enum": [\n                  "source",\n                  "target",\n                  "both",\n                  null\n               ],\n               "title": "Context Gate"\n            },\n            "bidirectional_encoder": {\n               "anyOf": [\n                  {\n                     "type": "boolean"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": false,\n               "title": "Bidirectional Encoder"\n            }\n         },\n         "title": "RnnDecoderConfig",\n         "type": "object"\n      },\n      "RnnEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "default": "rnn",\n               "enum": [\n                  "rnn",\n                  "brnn"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "bridge": {\n               "default": false,\n               "description": "Have an additional layer between the last encoder state and the first decoder state (RNN specific).",\n               "title": "Bridge",\n               "type": "boolean"\n            },\n            "rnn_type": {\n               "default": "LSTM",\n               "description": "The gate type to use in the RNNs.",\n               "enum": [\n                  "LSTM",\n                  "GRU"\n               ],\n               "title": "Rnn Type",\n               "type": "string"\n            }\n         },\n         "title": "RnnEncoderConfig",\n         "type": "object"\n      },\n      "TransformerDecoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "decoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerDecoderConfig",\n         "type": "object"\n      },\n      "TransformerEncoderConfig": {\n         "additionalProperties": false,\n         "properties": {\n            "encoder_type": {\n               "const": "transformer",\n               "default": "transformer",\n               "enum": [\n                  "transformer"\n               ],\n               "title": "Encoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the encoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of encoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "src_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for src.",\n               "title": "Src Word Vec Size",\n               "type": "integer"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            }\n         },\n         "title": "TransformerEncoderConfig",\n         "type": "object"\n      },\n      "TransformerLMDecoderConfig": {\n         "additionalProperties": false,\n         "description": "Right now just wraps TransformerDecoderConfig for simplicity.\\nMight merge in a single class later once TransformerLM path is clarified.",\n         "properties": {\n            "decoder_type": {\n               "const": "transformer_lm",\n               "default": "transformer_lm",\n               "enum": [\n                  "transformer_lm"\n               ],\n               "title": "Decoder Type",\n               "type": "string"\n            },\n            "layers": {\n               "default": 2,\n               "description": "Number of layers in the decoder.",\n               "title": "Layers",\n               "type": "integer"\n            },\n            "hidden_size": {\n               "default": 512,\n               "description": "Size of decoder hidden states.",\n               "title": "Hidden Size",\n               "type": "integer"\n            },\n            "tgt_word_vec_size": {\n               "default": 512,\n               "description": "Word embedding size for tgt.",\n               "title": "Tgt Word Vec Size",\n               "type": "integer"\n            },\n            "coverage_attn": {\n               "default": false,\n               "description": "Train a coverage attention layer.",\n               "title": "Coverage Attn",\n               "type": "boolean"\n            },\n            "lambda_coverage": {\n               "default": 0.0,\n               "description": "Lambda value for coverage loss of See et al (2017)",\n               "title": "Lambda Coverage",\n               "type": "number"\n            },\n            "global_attention": {\n               "default": "general",\n               "description": "The attention type to use. (Luong=general, Bahdanau=MLP)",\n               "enum": [\n                  "dot",\n                  "general",\n                  "mlp",\n                  null\n               ],\n               "title": "Global Attention"\n            },\n            "global_attention_function": {\n               "default": "softmax",\n               "description": "Global attention function to use.",\n               "enum": [\n                  "softmax",\n                  "sparsemax"\n               ],\n               "title": "Global Attention Function",\n               "type": "string"\n            },\n            "sliding_window": {\n               "default": 0,\n               "description": "Sliding window for transformer self-attention.",\n               "title": "Sliding Window",\n               "type": "integer"\n            },\n            "heads": {\n               "default": 8,\n               "description": "Number of heads for transformer self-attention.",\n               "title": "Heads",\n               "type": "integer"\n            },\n            "transformer_ff": {\n               "default": 2048,\n               "description": "Size of hidden transformer feed-forward.",\n               "title": "Transformer Ff",\n               "type": "integer"\n            },\n            "relative_positions_buckets": {\n               "default": 0,\n               "description": "Enable relative position bias (https://github.com/google-research/text-to-text-transfer-transformer).",\n               "title": "Relative Positions Buckets",\n               "type": "integer"\n            },\n            "mlp_activation_fn": {\n               "allOf": [\n                  {\n                     "$ref": "#/$defs/ActivationFunction"\n                  }\n               ],\n               "default": "relu",\n               "description": "The activation function to use in MLP layer."\n            },\n            "rotary_interleave": {\n               "default": true,\n               "description": "Interleave the head dimensions when rotary embeddings are applied. Otherwise the head dimensions are sliced in half. (True=default Llama from Meta (original), False= used by all HuggingFace models)",\n               "title": "Rotary Interleave",\n               "type": "boolean"\n            },\n            "rotary_theta": {\n               "default": 10000,\n               "description": "Rotary theta base length, 1e4 for Llama2.Mistral, 1e6 for Mixtral",\n               "title": "Rotary Theta",\n               "type": "integer"\n            },\n            "rotary_dim": {\n               "default": 0,\n               "description": "Rotary dim when model requires it to be different to head dim.",\n               "title": "Rotary Dim",\n               "type": "integer"\n            },\n            "layer_norm": {\n               "default": "standard",\n               "description": "Type of layer normalization in transformer architecture.",\n               "enum": [\n                  "standard",\n                  "rms"\n               ],\n               "title": "Layer Norm",\n               "type": "string"\n            },\n            "norm_eps": {\n               "default": 1e-06,\n               "description": "Layer norm epsilon.",\n               "title": "Norm Eps",\n               "type": "number"\n            },\n            "shared_layer_norm": {\n               "default": false,\n               "description": "Use a shared layer_norm in parallel residual attention. Note: must be True for Falcon 7B, False for Falcon 40B, same for GPT-J and GPT-NeoX models.",\n               "title": "Shared Layer Norm",\n               "type": "boolean"\n            },\n            "add_qkvbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of Query/Key/Value in MHA. Note: this will add bias to output projection layer too.",\n               "title": "Add Qkvbias",\n               "type": "boolean"\n            },\n            "heads_kv": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Number of heads for KV. heads_kv=heads if None, else number of heads for KV(e.g. Falcon 40B)",\n               "title": "Heads Kv"\n            },\n            "add_ffnbias": {\n               "default": false,\n               "description": "Add bias to nn.Linear of MLP FFN.",\n               "title": "Add Ffnbias",\n               "type": "boolean"\n            },\n            "parallel_residual": {\n               "default": false,\n               "description": "Use parallel residual in decoder layer. Note: this is used by GPT-J / Falcon Architecture.",\n               "title": "Parallel Residual",\n               "type": "boolean"\n            },\n            "num_experts": {\n               "default": 0,\n               "description": "Number of experts for MoE models.",\n               "title": "Num Experts",\n               "type": "integer"\n            },\n            "num_experts_per_tok": {\n               "default": 2,\n               "description": "Number of experts per token.",\n               "title": "Num Experts Per Tok",\n               "type": "integer"\n            },\n            "position_encoding_type": {\n               "anyOf": [\n                  {\n                     "$ref": "#/$defs/PositionEncodingType"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": "SinusoidalInterleaved",\n               "description": "Type of positional encoding."\n            },\n            "n_positions": {\n               "anyOf": [\n                  {\n                     "type": "integer"\n                  },\n                  {\n                     "type": "null"\n                  }\n               ],\n               "default": null,\n               "description": "Two casesCase 1: Absolute number of positions to learn position embeddings on (position_encoding_type: Learned)Case 2: Max Relative PositionsIn the case of position_encoding_type: Relative",\n               "title": "N Positions"\n            },\n            "aan_useffn": {\n               "default": false,\n               "description": "Turn on the FFN layer in the AAN decoder.",\n               "title": "Aan Useffn",\n               "type": "boolean"\n            },\n            "alignment_layer": {\n               "default": -2,\n               "description": "Layer number which has to be supervised.",\n               "title": "Alignment Layer",\n               "type": "integer"\n            },\n            "alignment_heads": {\n               "default": 0,\n               "description": "Number of cross attention heads per layer to supervise with.",\n               "title": "Alignment Heads",\n               "type": "integer"\n            },\n            "full_context_alignment": {\n               "default": false,\n               "description": "Whether alignment is conditioned on full target context.",\n               "title": "Full Context Alignment",\n               "type": "boolean"\n            },\n            "lambda_align": {\n               "default": 0.0,\n               "description": "Lambda value for alignement loss of Garg et al, 2019 (https://arxiv.org/abs/1909.02074)",\n               "title": "Lambda Align",\n               "type": "number"\n            }\n         },\n         "title": "TransformerLMDecoderConfig",\n         "type": "object"\n      }\n   },\n   "additionalProperties": false\n}\n'})}),"\n"]}),(0,i.jsx)(n.p,{}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Config:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_assignment"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"validate_default"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"use_enum_values"}),": ",(0,i.jsx)(n.em,{children:"bool = True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"extra"}),": ",(0,i.jsx)(n.em,{children:"str = forbid"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"protected_namespaces"}),": ",(0,i.jsx)(n.em,{children:"tuple = ()"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fields:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.CnnModelConfig.architecture",children:(0,i.jsx)(n.code,{children:"architecture (Literal['cnn'])"})})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validators:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.CnnModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"#eole.config.models.CnnModelConfig.encoder_decoder_type",children:(0,i.jsx)(n.code,{children:"encoder_decoder_type"})})," \xbb ",(0,i.jsx)(n.code,{children:"all fields"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"field-architecture--literalcnn--cnn",children:[(0,i.jsx)(n.em,{children:"field"})," architecture ",(0,i.jsx)(n.em,{children:": Literal['cnn']"})," ",(0,i.jsx)(n.em,{children:"= 'cnn'"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validated by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_override_values"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.code,{children:"_validate_model_config"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.CnnModelConfig.default_architecture",children:(0,i.jsx)(n.code,{children:"default_architecture"})})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"#eole.config.models.CnnModelConfig.encoder_decoder_type",children:(0,i.jsx)(n.code,{children:"encoder_decoder_type"})})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-default_architecture----all-fieldssource-4",children:[(0,i.jsx)(n.em,{children:"validator"})," default_architecture  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.em,{children:"all fields"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L576-L583",children:"[source]"})]}),"\n",(0,i.jsxs)(n.h4,{id:"validator-encoder_decoder_type----all-fieldssource-3",children:[(0,i.jsx)(n.em,{children:"validator"})," encoder_decoder_type  ",(0,i.jsx)(n.em,{children:"\xbb"}),"  ",(0,i.jsx)(n.em,{children:"all fields"}),(0,i.jsx)(n.a,{href:"https://github.com/eole-nlp/eole/blob/master/eole/config/models.py#L559-L574",children:"[source]"})]})]})}function f(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>d});var i=t(6540);const r={},o=i.createContext(r);function l(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);