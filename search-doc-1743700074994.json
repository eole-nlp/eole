{"searchDocs":[{"title":"Command Line","type":0,"sectionRef":"#","url":"/eole/docs/concepts/command_line","content":"","keywords":"","version":"Next"},{"title":"Categories of Tools​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#categories-of-tools","content":" ","version":"Next","tagName":"h2"},{"title":"Main Entrypoints​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#main-entrypoints","content":" build_vocabtraintranslate  ","version":"Next","tagName":"h3"},{"title":"Model Conversion Tools​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#model-conversion-tools","content":" convert Flavors: xgen, redpajama, llama_legacy, falcon, mpt, HF, T5  ","version":"Next","tagName":"h3"},{"title":"Model Management Tools​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#model-management-tools","content":" model Subcommands: lora, release, extract_embeddings, average  ","version":"Next","tagName":"h3"},{"title":"Miscellaneous Tools (Mostly Legacy)​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#miscellaneous-tools-mostly-legacy","content":" tools Subcommands: LM_scoring, oracle_comet, run_mmlu, spm_to_vocab, mbr_bleu, embeddings_to_torch, oracle_bleu  ","version":"Next","tagName":"h3"},{"title":"Usage​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#usage","content":" The main entrypoints are typically used with a yaml configuration file. Most parameters can also be overridden via corresponding command line flags if needed.  ","version":"Next","tagName":"h2"},{"title":"Examples​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#examples","content":" eole build_vocab -c your_config.yaml eole train -c your_config.yaml eole predict -c your_config.yaml   ","version":"Next","tagName":"h3"},{"title":"Additional Tools​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#additional-tools","content":" All other tools have specific arguments that can be inspected via the command helpstring -h.  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Command Line","url":"/eole/docs/concepts/command_line#example","content":" eole tool_name -h  ","version":"Next","tagName":"h3"},{"title":"EOLE","type":0,"sectionRef":"#","url":"/eole/docs/","content":"","keywords":"","version":"Next"},{"title":"Latest developments​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#latest-developments","content":" Mistral-3.1-24B-instruct support (text and image input)Pure-BF16 Training thanks to Kahan Summation implemented hereWeb-based (Google translator-like) interface featuring the latest EuroLLM-8B-Instruct LLM: read more hereEstimator layer which enables to rescore multiple beams in the same model. Read article here and hereSupport Hugging Face Tokenizers for better compatiblityNew recipes for TowerInstruct-llama2 and TowerInstruct-MistralSupport latest models for Llama3.x, Gemma2, PixtralReplicate CometKiwi(XL/XXL) Encoder+Estimator models  ","version":"Next","tagName":"h2"},{"title":"Work completed​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#work-completed","content":" We have made significant progress in several areas:  Configuration Management: Streamlined through pydantic models.Command Line Entry Points: Improved using structured subparsers for better organization.Reproducible Recipes: Provided for widely used models and tasks, ensuring consistency and reliability.Core API Simplification: Refined around the new configuration objects for ease of use.Revamped Fast API based server: see above example with EuroLLM-9B-Instruct  ","version":"Next","tagName":"h2"},{"title":"Future Directions​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#future-directions","content":" There are still several exciting avenues to explore:  Further Simplification and Refactoring: Continue enhancing the codebase for clarity and efficiency.Documentation: Enhance and expand the documentation for better user guidance.Test Coverage: Improve testing to ensure code reliability and performance.Logging Enhancements: Implement more sophisticated logging mechanisms.Broader Model Support: Extend support to include a wider range of open models, potentially multi-modal.    ","version":"Next","tagName":"h3"},{"title":"Key Features​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#key-features","content":" Versatile Training and Inference: Train from scratch, finetune, and infer models of various architectures including Transformer Encoder/Decoder/EncoderDecoder and RNN EncoderDecoder.Dynamic Data Transforms: Apply on-the-fly transformations in the dataloading logic for both training and inference.Comprehensive LLM Support: Includes converters for Llama, Mistral, Phi, Gemma ...Advanced Quantization: Support for 8-bit and 4-bit quantization, along with LoRA adapters, with or without checkpointing, as well as mixed precision (FP16).Efficient Finetuning: Finetune 7B and 13B models on a single RTX 24GB GPU using 4-bit quantization.Flexible Inference: Perform inference in 4-bit or 8-bit using the same layer quantization methods as in finetuning.Tensor Parallelism: Enable tensor parallelism for both training and inference when models exceed the memory capacity of a single GPU.    ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#setup","content":" ","version":"Next","tagName":"h2"},{"title":"Using Docker​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#using-docker","content":" To facilitate setup and reproducibility, we provide Docker images via the GitHub Container Registry: EOLE Docker Images.  You can customize the workflow and build your own images based on specific needs using build.sh and Dockerfile in the docker directory of the repository.  To pull the Docker image:  docker pull ghcr.io/eole-nlp/eole:0.2.0-torch2.6.0-ubuntu22.04-cuda12.6   Example one-liner to run a container and open a bash shell within it:  docker run --rm -it --runtime=nvidia ghcr.io/eole-nlp/eole:0.2.0-torch2.6.0-ubuntu22.04-cuda12.6   Note: Ensure you have the Nvidia Container Toolkit (formerly nvidia-docker) installed to take advantage of CUDA/GPU features.  Depending on your needs, you can add various flags:  -p 5000:5000: Forward an exposed port from your container to your host.-v /some/local/directory:/some/container/directory: Mount a local directory to a container directory.--entrypoint some_command: Run a specific command as the container entry point (instead of the default bash shell).  ","version":"Next","tagName":"h3"},{"title":"Installing Locally​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#installing-locally","content":" Requirements​  Python &gt;= 3.10PyTorch &gt;= 2.5 &lt; 2.8  Installation from Source​  To install from source:  git clone https://github.com/eole-nlp/eole cd eole pip install -e .   Installation from PyPI​  Installation from PyPI will be available soon.  Notes​  If you encounter a MemoryError during installation, try using pip with the --no-cache-dir option.  (Optional) Some advanced features (e.g., pretrained models or specific transforms) require extra packages. Install them with:  pip install -r requirements.opt.txt   ","version":"Next","tagName":"h3"},{"title":"Manual Installation of Some Dependencies​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#manual-installation-of-some-dependencies","content":" Flash Attention​  To use Flash Attention, install it manually:  pip install flash-attn --no-build-isolation   AWQ​  For inference or quantizing an AWQ model, AutoAWQ is required. Install it with:  pip install autoawq   For more details, refer to AutoAWQ.  ","version":"Next","tagName":"h3"},{"title":"Notes on Mixed-precision or Low precision Training​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#notes-on-mixed-precision-or-low-precision-training","content":" Until Feb 25, we used torch optimizers with or without AMP (mixed precision) or &quot;fusedadam&quot; which was an old implementation of Apex/Nvidia using FP16 with dynamic loss scaling and without FP32 master weights. As of 0.2 &quot;fusedadam&quot; is deprecated and we implemented pure-BF16 training.  As a result, config flags are now:  For FP16-amp or BF16-amp training (using pytorch optimizers and amp implementation)  compute_dtype: fp16 or bf16 use_amp: true optim: adam or adamw   Special note: even though it may not be logical, we still use the torch GradScaler in BF16-AMP. Even if the BF16 range is similar to FP32, scaling prevents from underflowing. We tested BF16-AMP without the GradScaler and it does not give good results.  For pure-bf16 training (using torch-optimi and kahan summation)  compute_dtype: bf16 use_amp: true optim: adam or adamw   Pure-BF16 training is faster than AMP and the memory footprint is reduced (master weights are kept in BF16 vs FP32). However Kahan Summation is not magical, results are good but not as good as AMP. Use this feature mainly when memory footprint is an issue with LLMs.    ","version":"Next","tagName":"h2"},{"title":"Contributing​","type":1,"pageTitle":"EOLE","url":"/eole/docs/#contributing","content":" We love contributions! Please look at issues marked with the contributions welcome tag.  Before raising an issue, make sure you read the requirements and the Full Documentation. You can also check if a Recipe fits your use case.  Unless there is a bug, please use the Discussions tab to ask questions or propose new topics/features. ","version":"Next","tagName":"h2"},{"title":"Dataset Weighting","type":0,"sectionRef":"#","url":"/eole/docs/concepts/weighting","content":"","keywords":"","version":"Next"},{"title":"Example​","type":1,"pageTitle":"Dataset Weighting","url":"/eole/docs/concepts/weighting#example","content":" In the following example, we will sequentially sample 7 examples from corpus_1, and 3 examples from corpus_2, and so on:  # &lt;your_config&gt;.yaml ... # Corpus opts: data: corpus_1: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt weight: 7 corpus_2: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt weight: 3 valid: path_src: toy-ende/src-val.txt path_tgt: toy-ende/tgt-val.txt ...  ","version":"Next","tagName":"h3"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/eole/docs/concepts/config","content":"Configuration One of the core principles around Eole is the structured configuration logic via Pydantic models. This allows for centralized validation of numerous parameters, as well as proper nesting of various objects and scopes. It can be a lot at first, but that's a necessary evil for proper structure and modularity. Here is an example configuration to train a GPT-style language model: # General data/vocab/run related settings seed: 42 save_data: test_save_data src_vocab_size: 60000 tgt_vocab_size: 60000 share_vocab: true src_vocab: my_vocab.txt report_every: 100 # report stats every 100 steps # datasets data: # training sets can be numerous, and named anything corpus_1: path_src: my_training_set.txt # single validation set, always named &quot;valid&quot; valid: path_src: my_validation_set.txt # default transforms, in application order transforms: [onmt_tokenize, filtertoolong] # transforms configuration transforms_configs: onmt_tokenize: src_subword_type: bpe src_subword_model: my_subwords_model.bpe src_onmttok_kwargs: {&quot;mode&quot;: &quot;aggressive&quot;, &quot;joiner_annotate&quot;: True, &quot;preserve_placeholders&quot;: True, &quot;case_markup&quot;: True, &quot;soft_case_regions&quot;: True, &quot;preserve_segmented_tokens&quot;: True} filtertoolong: src_seq_length: 512 tgt_seq_length: 512 # model architecture configuration model: architecture: &quot;transformer_lm&quot; layers: 6 heads: 8 hidden_size: 512 transformer_ff: 2048 embeddings: word_vec_size: 512 position_encoding: true # training routine configuration training: # Train on a single GPU world_size: 1 gpu_ranks: [0] # Batching batch_size: 2048 batch_type: tokens # Optimizer model_dtype: &quot;fp32&quot; optim: &quot;adam&quot; learning_rate: 2 warmup_steps: 8000 decay_method: &quot;noam&quot; adam_beta2: 0.998 # Hyperparams dropout_steps: [0] dropout: [0.1] attention_dropout: [0.1] max_grad_norm: 0 label_smoothing: 0.1 param_init: 0 param_init_glorot: true normalization: &quot;tokens&quot; # Where to save the checkpoints (creates a directory) model_path: my_model # Steps intervals save_checkpoint_steps: 10 train_steps: 50 valid_steps: 500 ","keywords":"","version":"Next"},{"title":"Data Transforms","type":0,"sectionRef":"#","url":"/eole/docs/concepts/transforms","content":"","keywords":"","version":"Next"},{"title":"Transform Types​","type":1,"pageTitle":"Data Transforms","url":"/eole/docs/concepts/transforms#transform-types","content":" The concept of TransformType was introduced to facilitate transparent configuration management. The underlying issue at stake is that all transforms are not meant to be used in the same concept. For instance, the filtertoolong transform is meant as a &quot;safeguard&quot; to limit the size of training batches. Enabling this transform when predicting can introduce some unwanted behaviours and poor results. For now, the possible transform types are:  Default // &quot;any&quot;: usable in any context (default unless specified otherwise in the transform class definition);Train // &quot;train&quot;: usable only in training context;Predict // &quot;predict&quot;: usable only in prediction context.  This concept might be extended later for various needs, such as different data types, etc.  ","version":"Next","tagName":"h3"},{"title":"General purpose​","type":1,"pageTitle":"Data Transforms","url":"/eole/docs/concepts/transforms#general-purpose","content":" Filter examples by length​  Transform name: filtertoolong  Class: eole.transforms.misc.FilterTooLongTransform  The following options can be added to the configuration :  src_seq_length: maximum source sequence length;tgt_seq_length: maximum target sequence length.  Add custom prefix to examples​  Transform name: prefix  Class: eole.transforms.misc.PrefixTransform  For each dataset that the prefix transform is applied to, you can set the additional src_prefix and tgt_prefix parameters in its data configuration:  data: corpus_1: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt transforms: [prefix] weight: 1 src_prefix: __some_src_prefix__ tgt_prefix: __some_tgt_prefix__   At inference if you want to use the target prefix feature to prefix your target segment with a unique prefix (as opposed to a target prefix coming from a line-by-line file) you need to set your yaml file as follow (example given with a target language as in the NLLB-200 case):  tgt_prefix: &quot;spa_Latn&quot; tgt_file_prefix: true   Add custom suffix to examples​  Transform name: suffix  Class: eole.transforms.misc.SuffixTransform  For each dataset that the suffix transform is applied to, you can set the additional src_suffix and tgt_suffix parameters in its data configuration:  data: corpus_1: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt transforms: [suffix] weight: 1 src_suffix: __some_src_suffix__ tgt_suffix: __some_tgt_suffix__   Convert examples to uppercase​  Transform name: uppercase  Class: eole.transforms.uppercase.UpperCaseTransform  Converts source and target (if present) examples to uppercase so the model can learn better to translate sentences in all caps. This transform normalizes the examples so the uppercased strings are stripped from any diacritics and accents. Usually this is desirable for most languages, although there are few exceptions.  The following option can be added to the main configuration (same ratio for all dataset with this transform):  upper_corpus_ratio: ratio of the corpus that will be transformed to uppercase (default: 0.01);  Normalize punctuation​  Transform name: normalize  Class: eole.transforms.normalize.NormalizeTransform  Normalizes source and target (if present) examples using the same rules as Moses punctuation normalizer.  The following options can be added to the configuration of each dataset:  src_lang: en, de, cz/cs, fr (default='')tgt_lang: en, de, cz/cs, fr (default='')penn: Penn substitution (default=True)norm_quote_commas: Normalize quotations and commas (default=True)norm_numbers: Normalize numbers (default=True)pre_replace_unicode_punct: Replace unicode punct (default=False)post_remove_control_chars: Remove control chars (default=False)  Clean dataset​  Transform name: clean  Class: eole.transforms.clean.CleanTransform  Cleans source and target (if present) examples using a set of rules.  The following options can be added to the configuration of each dataset:  src_eq_tgt: Remove example when source=target (default=True)same_char: Remove example if the same char is repeated 4 times (default=True)same_word: Remove example if the same word is repeated 3 times (default=True)script_ok: Remove example which contains chars that do not belong to these scripts (default=['Latin', 'Common'])script_nok: Remove example which contains chars that belong to these scripts (default=[])src_tgt_ratio: Remove example for which src/tgt ration is &lt;1/ratio or &gt;ratio (default=2)avg_tok_min: Remove example for which the average token length is &lt; X (default=3)avg_tok_max: Remove example for which the average token length is &gt; X (default=20)lang_id: Remove example for which detected language is not in [] (default=['en', 'fr'])  Context / Doc aware transform​  Transform name: docify  Class: eole.transforms.docify.DocifyTransform  Concatenates several segments into one, separated with a delimiter.  Pre-requisite:  Dataset must be &quot;Docs&quot; separated by an empty line which will make clear a story ends at this empty line.  The following options can be added to the main configuration (same options for all dataset with this transform):  doc_length: max token to be concatenated (default=200)max_context: number of delimiter (default=1 , ie 2 segments concatenated)  When working with several workers, this require some precaution in order to make sure &quot;doc&quot; are read linearly.  max_context + 1 needs to be a multiple of stride = Number of gpu x num_workers  Example: max_context=1 and 1 GPU, then num_workers must be 2 or 4.  Augment source segments with fuzzy matches for Neural Fuzzy Repair​  Transform name: fuzzymatch  Class: eole.transforms.fuzzymatch.FuzzyMatchTransform  Augments source segments with fuzzy matches for Neural Fuzzy Repair, as described in Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation. Currently, the transform augments source segments with only a single fuzzy match. The Translation Memory (TM) format should be a flat text file, with each line containing the source and the target segment separated by a delimiter. As fuzzy matching during training is computational intensive, we offer some advice to achieve good performance and minimize overhead:  Depending on your system's specs, you may have to experiment with the options bucket_size, bucket_size_init, and bucket_size_increment;You should increase the num_workers and prefetch_factor so your GPU does not have to wait for the batches to be augmented with fuzzy matches;Try to use a sensible Translation Memory size. 200k-250k translation units should be enough for yielding a sufficient number of matches;Although the transform performs some basic filtering both in the TM and in the corpus for very short or very long segments, some examples may still be long enough, so you should increase a bit the src_seq_length;Currently, when using n_sample, examples are always processed one by one and not in batches.  The following options can be added to the main configuration (valid for all datasets using this transform):  tm_path: The path to the Translation Memory text file;fuzzy_corpus_ratio: Ratio of corpus to augment with fuzzy matches (default: 0.1);fuzzy_threshold: The fuzzy matching threshold (default: 70);tm_delimiter: The delimiter used in the flat text TM (default: &quot;\\t&quot;);fuzzy_token: The fuzzy token to be added with the matches (default: &quot;｟fuzzy｠&quot;);fuzzymatch_min_length: Min length for TM entries and examples to match (default: 4);fuzzymatch_max_length: Max length for TM entries and examples to match (default: 70).  Augment source and target segments with inline tags​  Transform name: inlinetags  Class: eole.transforms.inlinetags.InlineTagsTransform  Augments source and target segments with inline tags (placeholders). The transform adds 2 kind of tags, paired tags (an opening and a closing tag) and isolated (standalone) tags, and requires a tab-delimited dictionary text file with source and target terms and phrases. A dictionary with 20-30k entries is recommended. User-defined tags must include the number placeholder #, e.g. &quot;｟user_start_tag_#｠&quot;.  The following options can be added to the main configuration (valid for all datasets using this transform):  tags_dictionary_path: The path to the dictionary text file;tags_corpus_ratio: Ratio of corpus to augment with inline tags (default: 0.1);max_tags: Maximum number of tags that can be added to a single sentence. (default: 12);paired_stag: The format of an opening paired inline tag. Must include the character # (default: &quot;｟ph_#_beg｠&quot;);paired_etag: The format of a closing paired inline tag. Must include the character # (default: &quot;｟ph_#_end｠&quot;);isolated_tag: The format of an isolated inline tag. Must include the character # (default: &quot;｟ph_#_std｠&quot;);src_delimiter: Any special token used for augmented src sentences (default: &quot;｟fuzzy｠&quot;);  Make the model learn to use terminology​  Transform name: terminology  Class: eole.transforms.terminology.TerminologyTransform  Augments source segments with terms so the model can learn to use user-provided terms at inference. It requires a dictionary with source and target terms, delimited with a tab. The transform uses Spacy's lemmatization facilities in order to a) solve the word inflection problem when searching for terms in any form, and b) make the model inflect correctly most target terms at inference. The lemmatization is applied at the dictionary entries and also at the source and target examples, and the term searches during training are performed on the lemmatized examples. The format of a processed segment augmented with terms is as follows:This is an ｟src_term_start｠ augmented ｟tgt_term_start｠ target_lemma_for_augmented ｟tgt_term_end｠ example.The following options can be added to the main configuration (valid for all datasets using this transform):  termbase_path: The path to the dictionary text file;src_spacy_language_model: Name of the spacy language model for the source corpus;tgt_spacy_language_model: Name of the spacy language model for the target corpus;term_corpus_ratio: Ratio of corpus to augment with terms # (default: 0.3);term_example_ratio: Max terms allowed in an example # (default: 0.2);src_term_stoken: The source term start token # (default: &quot;｟src_term_start｠&quot;);tgt_term_stoken: The target term start token # (default: &quot;｟tgt_term_start｠&quot;);tgt_term_etoken: The target term end token # (default: &quot;｟tgt_term_end｠&quot;);term_source_delimiter: Any special token used for augmented src sentences. The default is the fuzzy token used in the FuzzyMatch transform # (default: &quot;｟fuzzy｠&quot;);  ","version":"Next","tagName":"h3"},{"title":"Tokenization​","type":1,"pageTitle":"Data Transforms","url":"/eole/docs/concepts/transforms#tokenization","content":" Common options for the tokenization transforms are the following:  src_subword_model: path of source side (or both if shared) subword model;tgt_subword_model: path of target side subword model;src_subword_nbest: number of candidates for subword regularization (sentencepiece), source side;tgt_subword_nbest: number of candidates for subword regularization (sentencepiece), target_side;src_subword_alpha: smoothing parameter for sentencepiece regularization / dropout probability for BPE, source side;tgt_subword_alpha: smoothing parameter for sentencepiece regularization / dropout probability for BPE, target side.  OpenNMT Tokenizer​  Transform name: onmt_tokenize  Class: eole.transforms.tokenize.OnmtTokenizerTransform  Additional options are available:  src_subword_type: type of subword model for source side (from [&quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot;]);tgt_subword_type: type of subword model for target side (from [&quot;none&quot;, &quot;sentencepiece&quot;, &quot;bpe&quot;]);src_onmttok_kwargs: additional kwargs for pyonmttok Tokenizer class, source side;tgt_onmttok_kwargs: additional kwargs for pyonmttok Tokenizer class, target side.  SentencePiece​  Transform name: sentencepiece  Class: eole.transforms.tokenize.SentencePieceTransform  The src_subword_model and tgt_subword_model should be valid sentencepiece models.  BPE subword-nmt​  Transform name: bpe  Class: eole.transforms.tokenize.BPETransform  The src_subword_model and tgt_subword_model should be valid BPE models.  ","version":"Next","tagName":"h3"},{"title":"BART-style noise​","type":1,"pageTitle":"Data Transforms","url":"/eole/docs/concepts/transforms#bart-style-noise","content":" BART-style noise is composed of several parts, as described in BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.  These different types of noise can be controlled with the following options:  permute_sent_ratio: proportion of sentences to permute (default boundaries are &quot;.&quot;, &quot;?&quot; and &quot;!&quot;);rotate_ratio: proportion of inputs to permute;insert_ratio: proportion of additional random tokens to insert;random_ratio: proportion of tokens to replace with random;mask_ratio: proportion of words/subwords to mask;mask_length: length of masking window (from [&quot;subword&quot;, &quot;word&quot;, &quot;span-poisson&quot;]);poisson_lambda: $\\lambda$ value for Poisson distribution to sample span length (in the case of mask_length set to span-poisson);replace_length: when masking N tokens, replace with 0, 1, &quot; &quot;or N tokens. (set to -1 for N).  ","version":"Next","tagName":"h3"},{"title":"SwitchOut and sampling​","type":1,"pageTitle":"Data Transforms","url":"/eole/docs/concepts/transforms#switchout-and-sampling","content":" SwitchOut​  Transform name: switchout  Class: eole.transforms.sampling.SwitchOutTransform  Options:  switchout_temperature: sampling temperature for SwitchOut.  Drop some tokens​  Transform name: tokendrop  Class: eole.transforms.sampling.TokenDropTransform  Options:  tokendrop_temperature: sampling temperature for token deletion.  Mask some tokens​  Transform name: tokenmask  Class: eole.transforms.sampling.TokenMaskTransform  Options:  tokenmask_temperature: sampling temperature for token masking. ","version":"Next","tagName":"h3"},{"title":"Do you support multi-gpu?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/distributed","content":"Do you support multi-gpu? First you need to make sure you export CUDA_VISIBLE_DEVICES=0,1,2,3. If you want to use GPU id 1 and 3 of your OS, you will need to export CUDA_VISIBLE_DEVICES=1,3 Both -world_size and -gpu_ranks need to be set. E.g. -world_size 4 -gpu_ranks 0 1 2 3 will use 4 GPU on this node only. Warning - Deprecated Multi-node distributed training has not been properly re-implemented since OpenNMT-py 2.0. If you want to use 2 nodes with 2 GPU each, you need to set -master_ip and -master_port, and -world_size 4 -gpu_ranks 0 1: on the first node-world_size 4 -gpu_ranks 2 3: on the second node-accum_count 2: This will accumulate over 2 batches before updating parameters. If you use a regular network card (1 Gbps) then we suggest to use a higher -accum_count to minimize the inter-node communication. Note: In the legacy version, when training on several GPUs, you couldn't have them in 'Exclusive' compute mode (nvidia-smi -c 3). The multi-gpu setup relied on a Producer/Consumer setup. This setup means there will be 2&lt;n_gpu&gt; + 1 processes spawned, with 2 processes per GPU, one for model training and one (Consumer) that hosts a Queue of batches that will be processed next. The additional process is the Producer, creating batches and sending them to the Consumers. This setup is beneficial for both wall time and memory, since it loads data shards 'in advance', and does not require to load it for each GPU process. The new codebase allows GPUs to be in exclusive mode, because batches are moved to the device later in the process. Hence, there is no 'producer' process on each GPU.","keywords":"","version":"Next"},{"title":"How can I create custom on-the-fly data transforms?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/custom_transforms","content":"How can I create custom on-the-fly data transforms? The code is easily extendable with custom transforms inheriting from the Transform base class. You can for instance have a look at the FilterTooLongTransform and the corresponding FilterTooLongConfig classes as a template: class FilterTooLongConfig(TransformConfig): src_seq_length: int | None = Field( default=192, description=&quot;Maximum source sequence length.&quot; ) tgt_seq_length: int | None = Field( default=192, description=&quot;Maximum target sequence length.&quot; ) ... @register_transform(name=&quot;filtertoolong&quot;) class FilterTooLongTransform(Transform): &quot;&quot;&quot;Filter out sentence that are too long.&quot;&quot;&quot; config_model = FilterTooLongConfig def __init__(self, config): super().__init__(config) def _parse_config(self): self.src_seq_length = self.config.src_seq_length self.tgt_seq_length = self.config.tgt_seq_length def apply(self, example, is_train=False, stats=None, **kwargs): &quot;&quot;&quot;Return None if too long else return as is.&quot;&quot;&quot; if len(example[&quot;src&quot;]) &gt; self.src_seq_length or ( example[&quot;tgt&quot;] is not None and len(example[&quot;tgt&quot;]) &gt; self.tgt_seq_length - 2 ): if stats is not None: stats.update(FilterTooLongStats()) return None else: return example def _repr_args(self): &quot;&quot;&quot;Return str represent key arguments for class.&quot;&quot;&quot; return &quot;{}={}, {}={}&quot;.format( &quot;src_seq_length&quot;, self.src_seq_length, &quot;tgt_seq_length&quot;, self.tgt_seq_length ) Methods: _parse_opts allows to parse options from the config_model;apply is where the transform happens;_repr_args is for clean logging purposes. As you can see, there is the @register_transform wrapper before the class definition. This will allow for the class to be automatically detected (if put in the proper transforms folder) and usable in your training configurations through its name argument. You could also collect statistics for your custom transform by creating a class inheriting ObservableStats: class FilterTooLongStats(ObservableStats): &quot;&quot;&quot;Runing statistics for FilterTooLongTransform.&quot;&quot;&quot; __slots__ = [&quot;filtered&quot;] def __init__(self): self.filtered = 1 def update(self, other: &quot;FilterTooLongStats&quot;): self.filtered += other.filtered NOTE: Add elements to keep track in the __init__ and also __slot__ to make it lightweight;Supply update logic in update method;(Optional) override __str__ to change default log message format;Instantiate and passing the statistic object in the apply method of the corresponding transform class;statistics will be gathered per corpus per worker, but only first worker will report for its shard by default. The example argument of apply is a dict of the form: { &quot;src&quot;: &lt;source string&gt;, &quot;tgt&quot;: &lt;target string&gt;, &quot;align&quot;: &lt;alignment pharaoh string&gt; # optional } This is defined in eole.inputters.text_corpus.ParallelCorpus.load. This class is not easily extendable for now but it can be considered for future developments. For instance, we could create some CustomParallelCorpus class that would handle other kind of inputs.","keywords":"","version":"Next"},{"title":"Contributors","type":0,"sectionRef":"#","url":"/eole/docs/contributing","content":"","keywords":"","version":"Next"},{"title":"Guidelines​","type":1,"pageTitle":"Contributors","url":"/eole/docs/contributing#guidelines","content":" Before sending a PR, please do this checklist first:  Please run eole/tests/pull_request_check.sh and fix any errors. When adding new functionality, also add tests to this script. Included checks: black and flake8 check for coding style;unittest;continuous integration tests listed in .github/workflows/push.yml. When adding/modifying class constructor, please make the arguments as same naming style as its superclass in PyTorch.If your change is based on a paper, please include a clear comment and reference in the code (more on that below).  ","version":"Next","tagName":"h2"},{"title":"Docstrings​","type":1,"pageTitle":"Contributors","url":"/eole/docs/contributing#docstrings","content":" Above all, try to follow the Google docstring format (Napoleon example,Google styleguide). This makes it easy to include your contributions in the Sphinx documentation. And, do feel free to autodoc your contributions in the API .rst files in the docs/source folder! If you do, check that your additions look right.  cd docs # install some dependencies if necessary: pip install -r requirements.txt make html firefox build/html/main.html # or your browser of choice   Some particular advice:  Try to follow Python 3 typing module conventions when documenting types. Exception: use &quot;or&quot; instead of unions for more readabilityFor external types, use the full &quot;import name&quot;. Common abbreviations (e.g. np) are acceptable. For torch.Tensor types, the torch. is optional.Please don't use tics like (`str`) or rst directives like (:obj:`str`). Napoleon handles types very well without additional help, so avoid the clutter. Google docstrings don't support multiple returns. For multiple returns, the following works well with Sphinx and is still very readable. def foo(a, b): &quot;&quot;&quot;This is my docstring. Args: a (object): Something. b (class): Another thing. Returns: (object, class): * a: Something or rather with a long description that spills over. * b: And another thing. &quot;&quot;&quot; return a, b When citing a paper, avoid directly linking in the docstring! Add a Bibtex entry to docs/source/refs.bib. E.g., to cite &quot;Attention Is All You Need&quot;, visit arXiv, choose thebibtext link, search docs/source/refs.bibusing CTRL-F for DBLP:journals/corr/VaswaniSPUJGKP17, and if you do not find it then copy-paste the citation into refs.bib. Then, in your docstring, use :cite:`DBLP:journals/corr/VaswaniSPUJGKP17` . However, a link is better than nothing. Please document tensor shapes. Prefer the format``(a, b, c)``. This style is easy to read, allows using x for multplication, and is common (PyTorch uses a few variations on the parentheses format, AllenNLP uses exactly this format, Fairseq uses the parentheses format with single ticks). Again, a different style is better than no shape documentation. Please avoid unnecessary space characters, try to capitalize, and try to punctuate. For multi-line docstrings, add a blank line after the closing &quot;&quot;&quot;. Don't use a blank line before the closing quotes. &quot;&quot;&quot; not this &quot;&quot;&quot; &quot;&quot;&quot;This.&quot;&quot;&quot; &quot;&quot;&quot; Not this. &quot;&quot;&quot; &quot;&quot;&quot;This.&quot;&quot;&quot; This note is the least important. Focus on content first, but remember that consistent docs look good. Be sensible about the first line. Generally, one stand-alone summary line (per the Google guidelines) is good. Sometimes, it's better to cut directly to the args or an extended description. It's always acceptable to have a &quot;trailing&quot; citation. ","version":"Next","tagName":"h3"},{"title":"How can I ensemble Models at inference?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/ensemble_decoding","content":"How can I ensemble Models at inference? You can specify several models in the onmt_translate command line: -model model1_seed1 model2_seed2Bear in mind that your models must share the same target vocabulary.","keywords":"","version":"Next"},{"title":"How to use LoRa and 8bit loading to finetune a big model ?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/lora","content":"How to use LoRa and 8bit loading to finetune a big model ? Cf paper: LoRa LoRa is a mechanism that helps to finetune bigger model on a single GPU card by limiting the anmount of VRAM needed. The principle is to make only a few layers trainable (hence reducing the amount of required memory especially for the Adam optimizer) You need to train_from a model (for instance NLLB-200 3.3B) and use the following options: lora_layers: ['linear_values', 'linear_query'] these are the two layers of the Self-Attention module the paper recommend to make trainable.lora_rank: 2lora_dropout: 0.1 or any value you can testlora_alpha: 1 or any value you can testlora_embedding: true makes Embeddings LoRa compatible, hence trainable in the case you use update_vocab: true or if you want to finetune Embeddings as well. Bitsandbytes enables quantization of Linear layers. For more information: https://github.com/TimDettmers/bitsandbytesAlso you can read the blog post here: https://huggingface.co/blog/hf-bitsandbytes-integration You need to add the following option: quant_layers: ['up_proj', 'down_proj', 'linear_values', 'linear_query']quant_type: ['bnb_NF4'] You can for instane quantize the layers of the PositionWise Feed-Forward from the Encoder/Decoder and the key/query/values/final from the Multi-head attention. Choices for quantization are [&quot;bnb_8bit&quot;, &quot;bnb_FP4&quot;, &quot;bnb_NF4&quot;]","keywords":"","version":"Next"},{"title":"How to use gradient checkpointing when dealing with a big model ?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/gradient_checkpointing","content":"How to use gradient checkpointing when dealing with a big model ? use_ckpting: [&quot;ffn&quot;, &quot;mha&quot;, &quot;lora&quot;] Be carefull, the module that you use checkpointing needs to have gradients.","keywords":"","version":"Next"},{"title":"Performance tips","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/performance","content":"Performance tips use fp16use batch_size_multiple 8use vocab_size_multiple 8Depending on the number of GPU use num_workers 4 (for 1 GPU) or 2 (for multiple GPU)To avoid averaging checkpoints you can use the &quot;during training&quot; average decay system.for very fast inference convert your model to CTranslate2 format.","keywords":"","version":"Next"},{"title":"How to switch from OpenNMT-py to EOLE?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/opennmt","content":"","keywords":"","version":"Next"},{"title":"Configuration conversion​","type":1,"pageTitle":"How to switch from OpenNMT-py to EOLE?","url":"/eole/docs/FAQ/opennmt#configuration-conversion","content":" One of the main pillars of EOLE is the full revamping of the configuration structure and validation logic. That means OpenNMT-py configuration files are not supported by default. That being said, a conversion tool has been created to facilitate the transition: eole convert onmt_config  There are a few key things to know:  what was previous fully &quot;flat&quot; in OpenNMT-py configurations is now mostly nested in nested sections with specific scope such as training, model, transforms_configs;some parameters were renamed, removed, or replaced by other logics, which makes the conversion script not 100% exhaustive;the conversion script will log the remaining &quot;unmapped settings&quot;, to facilitate fixing the last issues manually.  ","version":"Next","tagName":"h2"},{"title":"Model conversion​","type":1,"pageTitle":"How to switch from OpenNMT-py to EOLE?","url":"/eole/docs/FAQ/opennmt#model-conversion","content":" Models trained with OpenNMT-py can technically be converted to be used with EOLE, but there is no automated tool for now. Feel free to get in touch via Issues or Discussions if that is a blocker. ","version":"Next","tagName":"h2"},{"title":"How do I use Pretrained embeddings (e.g. GloVe)?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/pretrained_embeddings","content":"","keywords":"","version":"Next"},{"title":"Example​","type":1,"pageTitle":"How do I use Pretrained embeddings (e.g. GloVe)?","url":"/eole/docs/FAQ/pretrained_embeddings#example","content":" Get GloVe files:  mkdir &quot;glove_dir&quot; wget http://nlp.stanford.edu/data/glove.6B.zip unzip glove.6B.zip -d &quot;glove_dir&quot;   Adapt the configuration:  # &lt;your_config&gt;.yaml &lt;Your data config...&gt; ... # this means embeddings will be used for both encoder and decoder sides both_embeddings: glove_dir/glove.6B.100d.txt # to set src and tgt embeddings separately: # src_embeddings: ... # tgt_embeddings: ... # supported types: GloVe, word2vec embeddings_type: &quot;GloVe&quot; # word_vec_size need to match with the pretrained embeddings dimensions word_vec_size: 100   Train:  eole train -config &lt;your_config&gt;.yaml   Notes:  the matched embeddings will be saved at &lt;save_data&gt;.enc_embeddings.pt and &lt;save_data&gt;.dec_embeddings.pt;additional flags freeze_word_vecs_enc and freeze_word_vecs_dec are available to freeze the embeddings. ","version":"Next","tagName":"h3"},{"title":"Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/position_encoding","content":"Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi The basic feature is absolute position encoding stemming from the original Transformer Paper. However, even with this, we can use SinusoidalInterleaved (default OpenNMT-py) or SinusoidalConcat (default Fairseq imported models) position_encoding_type: 'SinusoidalInterleaved'Do not forget to set also param_init_glorot: true If you prefer to use relative position encoding, we support 3 modes: &quot;Shaw&quot;: https://arxiv.org/abs/1803.02155 - you need to set position_encoding_type: 'Relative' and n_positions: N where N &gt; 1 (use 16, 20, 32) see paper.&quot;Rope&quot; Rotary Embeddings: https://arxiv.org/abs/2104.09864 - you need to set position_encoding_type: 'Rotary'&quot;Alibi&quot; (used by MPT-7B for example) https://arxiv.org/abs/2108.12409 - you need to set position_encoding_type: 'Alibi' In a nutshell, at the time if this writing (v3.1) absolute position encoding is managed in the Embeddings module, whereas the relative position encoding is managed directly in the multi-head self-attention module.","keywords":"","version":"Next"},{"title":"Compute dtype (precision) and storage dtype","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/precision","content":"","keywords":"","version":"Next"},{"title":"How to configure​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#how-to-configure","content":" It's important to note that compute precision does not necessarily reflect model parameters dtype. With this considered, compute precision can be configured by setting the compute_dtype field. From that, and other optimization settings (or specicic cases), the storage_dtype computed field is deduced. This is different from the specific quantization logic configured via quant_layers and quant_type. If such quantization is enabled, precision is still taken into account for non quantized components.  Note: the compute_dtype field can take both str and torch.dtype input types. An str input is validated to the corresponding torch.dtype via a custom mapping (see eole.config.common.RunningConfig.compute_dtype).  ","version":"Next","tagName":"h2"},{"title":"Available modes​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#available-modes","content":" ","version":"Next","tagName":"h2"},{"title":"Full precision​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#full-precision","content":" compute_dtype: {fp32, torch.float32}Standard float precision.  Note: flash attention is not compatible with float32 precision.  ","version":"Next","tagName":"h3"},{"title":"Half precision​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#half-precision","content":" compute_dtype: {fp16, torch.float16}  In most cases, the main model storage_dtype will be torch.float32, and some parameters will be automatically casted to torch.float16 with torch Automatic Mixed Precision.  Note: this means that checkpoints will be stored in torch.float32 in the amp case.  ","version":"Next","tagName":"h3"},{"title":"BFloat16​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#bfloat16","content":" compute_dtype: {bf16, torch.bfloat16}  See bfloat16 floating-point format for specificities.  When using the flag use_amp=True, behavior will be the same as above, ie torch AMP (mixed precision)  When using use_amp=False, we switch to torch-optimi which enables pure BF16 training using Kahan summation. see Optimi  ","version":"Next","tagName":"h3"},{"title":"Int8​","type":1,"pageTitle":"Compute dtype (precision) and storage dtype","url":"/eole/docs/FAQ/precision#int8","content":" compute_dtype: {int8, torch.int8}  This specific setting is only valid for CPU prediction, to enable Dynamic Quantization.  In that case, storage_dtype will initially be torch.float32, and the model will then be quantized to torch.qint8 with torch.quantization.quantize_dynamic. ","version":"Next","tagName":"h3"},{"title":"How can I apply on-the-fly tokenization and subword regularization when training?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/tokenization","content":"","keywords":"","version":"Next"},{"title":"Example​","type":1,"pageTitle":"How can I apply on-the-fly tokenization and subword regularization when training?","url":"/eole/docs/FAQ/tokenization#example","content":" This example applies sentencepiece tokenization with pyonmttok, with nbest=20 and alpha=0.1.  # &lt;your_config&gt;.yaml ... transforms_configs: onmt_tokenize: # Tokenization options src_subword_type: sentencepiece src_subword_model: examples/subword.spm.model tgt_subword_type: sentencepiece tgt_subword_model: examples/subword.spm.model # Number of candidates for SentencePiece sampling subword_nbest: 20 # Smoothing parameter for SentencePiece sampling subword_alpha: 0.1 # Specific arguments for pyonmttok src_onmttok_kwargs: &quot;{'mode': 'none', 'spacer_annotate': True}&quot; tgt_onmttok_kwargs: &quot;{'mode': 'none', 'spacer_annotate': True}&quot; # transforms: [onmt_tokenize] # if you don't need to specify at dataset level # Corpus opts: data: corpus_1: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt transforms: [onmt_tokenize] weight: 1 valid: path_src: toy-ende/src-val.txt path_tgt: toy-ende/tgt-val.txt transforms: [onmt_tokenize] ...   Other tokenization methods and transforms are readily available. See the dedicated docs for more details. ","version":"Next","tagName":"h3"},{"title":"How can I update a checkpoint's vocabulary?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/update_vocab","content":"How can I update a checkpoint's vocabulary? New vocabulary can be used to continue training from a checkpoint. Existing vocabulary embeddings will be mapped to the new vocabulary, and new vocabulary tokens will be initialized as usual. Run eole build_vocab as usual with the new dataset. New vocabulary files will be created. Training options to perform vocabulary update are: -update_vocab: set this option-reset_optim: set the value to &quot;states&quot;-train_from: checkpoint path","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/eole/docs/main","content":"","keywords":"","version":"Next"},{"title":"Installation​","type":1,"pageTitle":"Overview","url":"/eole/docs/main#installation","content":" Install OpenNMT-py from pip:  pip install OpenNMT-py   or from the sources:  git clone https://github.com/OpenNMT/OpenNMT-py.git cd OpenNMT-py python setup.py install   (Optional) some advanced features (e.g. working pretrained models or specific transforms) requires extra packages, you can install it with:  pip install -r requirements.opt.txt   And you are ready to go!  Take a look at the quickstart to familiarize yourself with the main training workflow.  ","version":"Next","tagName":"h2"},{"title":"Citation​","type":1,"pageTitle":"Overview","url":"/eole/docs/main#citation","content":" When using OpenNMT-py for research please cite ourOpenNMT technical report  @inproceedings{opennmt, author = {Guillaume Klein and Yoon Kim and Yuntian Deng and Jean Senellart and Alexander M. Rush}, title = {OpenNMT: Open-Source Toolkit for Neural Machine Translation}, booktitle = {Proc. ACL}, year = {2017}, url = {https://doi.org/10.18653/v1/P17-4012}, doi = {10.18653/v1/P17-4012} }   ","version":"Next","tagName":"h2"},{"title":"Additional resources​","type":1,"pageTitle":"Overview","url":"/eole/docs/main#additional-resources","content":" You can find additional help or tutorials in the following resources:  Forum Gitter channel ","version":"Next","tagName":"h2"},{"title":"Can I get word alignments while translating?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/word_alignments","content":"","keywords":"","version":"Next"},{"title":"Raw alignments from averaging Transformer attention heads​","type":1,"pageTitle":"Can I get word alignments while translating?","url":"/eole/docs/FAQ/word_alignments#raw-alignments-from-averaging-transformer-attention-heads","content":" Currently, we support producing word alignment while translating for Transformer based models. Using -report_align when calling translate.py will output the inferred alignments in Pharaoh format. Those alignments are computed from an argmax on the average of the attention heads of the second to last decoder layer. The resulting alignment src-tgt (Pharaoh) will be pasted to the translation sentence, separated by |||. Note: The second to last default behaviour was empirically determined. It is not the same as the paper (they take the penultimate layer), probably because of slight differences in the architecture.  alignments use the standard &quot;Pharaoh format&quot;, where a pair i-j indicates the ith word of source language is aligned to jth word of target language.Example: {'src': 'das stimmt nicht !'; 'output': 'that is not true ! ||| 0-0 0-1 1-2 2-3 1-4 1-5 3-6'}Using -tgt and -gold_align options when calling translate.py, we output alignments between the source and the gold target rather than the inferred target, assuming we're doing evaluation.To convert subword alignments to word alignments, or symetrize bidirectional alignments, please refer to the lilt scripts.  ","version":"Next","tagName":"h3"},{"title":"Supervised learning on a specific head​","type":1,"pageTitle":"Can I get word alignments while translating?","url":"/eole/docs/FAQ/word_alignments#supervised-learning-on-a-specific-head","content":" The quality of output alignments can be further improved by providing reference alignments while training. This will invoke multi-task learning on translation and alignment. This is an implementation based on the paper Jointly Learning to Align and Translate with Transformer Models.  The data need to be preprocessed with the reference alignments in order to learn the supervised task. The reference alignment file(s) can for instance be generated by GIZA++ or fast_align.  In order to learn the supervised task, you can set for each dataset the path of its alignment file in the YAML configuration file:  &lt;your_config&gt;.yaml ... # Corpus opts: data: corpus_1: path_src: toy-ende/src-train1.txt path_tgt: toy-ende/tgt-train1.txt # src - tgt alignments in pharaoh format path_align: toy-ende/src-tgt.align transforms: [] weight: 1 valid: path_src: toy-ende/src-val.txt path_tgt: toy-ende/tgt-val.txt transforms: [] ...   Notes:  Most of the transforms are for now incompatible with the joint alignment learning pipeline, because most of them make modifications at the token level, hence alignments would be made invalid.There should be no blank lines in the alignment files provided.  Training options to learn such alignments are:  -lambda_align: set the value &gt; 0.0 to enable joint align training, the paper suggests 0.05;-alignment_layer: indicate the index of the decoder layer;-alignment_heads: number of alignment heads for the alignment task - should be set to 1 for the supervised task, and preferably kept to default (or same as num_heads) for the average task;-full_context_alignment: do full context decoder pass (no future mask) when computing alignments. This will slow down the training (~12% in terms of tok/s) but will be beneficial to generate better alignment. ","version":"Next","tagName":"h3"},{"title":"What special tokens are used?","type":0,"sectionRef":"#","url":"/eole/docs/FAQ/special_tokens","content":"","keywords":"","version":"Next"},{"title":"Special tokens actually used​","type":1,"pageTitle":"What special tokens are used?","url":"/eole/docs/FAQ/special_tokens#special-tokens-actually-used","content":" Depending on the context, these tokens can take various values:  Default behaviour, training from scratch  Some default values are defined as constants for the project:  class DefaultTokens(object): PAD = &quot;&lt;blank&gt;&quot; BOS = &quot;&lt;s&gt;&quot; EOS = &quot;&lt;/s&gt;&quot; UNK = &quot;&lt;unk&gt;&quot;   Retrieving a pretrained model from HF  The special tokens will be retrieved and configured from the special_tokens_map.json configuration file from the HF model files.  Custom behaviour  In any case, these tokens can be overriden via the ad-hoc configuration settings:  bos_tokenpad_tokeneos_tokenunk_token  ","version":"Next","tagName":"h2"},{"title":"Special tokens behaviour in Eole​","type":1,"pageTitle":"What special tokens are used?","url":"/eole/docs/FAQ/special_tokens#special-tokens-behaviour-in-eole","content":" When we train a SEQ2SEQ model we use: SRC: srctok1 srctok2 srctok3 .... srctokn TGT: BOS tgttok1 tgttok2 ..... tgttokm EOS But when training a LM SRC: BOS srctok1 srctok2 srctok3 .... srctokn TGT: srctok1 srctok2 srctok3 .... srctokn EOS  Having said that, sometimes we need to finetune models (eg: NLLB-200, Llama, ...) with existing vocab and special tokens are not the same.  ex with NLLB-200 BOS id=0 PAD id=1 EOS id=2 UNK id=3 And the decoder start token is EOS () which means in fact that the BOS is never used. At training, TGT needs to start with EOS instead of BOS in the default OpenNMT-py config.  Example of Llama UNK id=0 BOS id=1 EOS id=2 There was no PAD but to avoid conflicts we forced PAD id=3 (which was token '&lt;0x00&gt;' in the original llama tokenizer) ","version":"Next","tagName":"h2"},{"title":"Recipes","type":0,"sectionRef":"#","url":"/eole/docs/recipes/","content":"","keywords":"","version":"Next"},{"title":"Available Recipes​","type":1,"pageTitle":"Recipes","url":"/eole/docs/recipes/#available-recipes","content":" Explore our collection of recipes designed to help you get started with various language models and tasks:  Llama2 Inference and Fine-tuning (Vicuna-style)Llama3 InferenceMMLU BenchmarkGPT-2 Style Language Model (WikiText-103)Transformer Translation Model (WMT17 en-de)  ","version":"Next","tagName":"h2"},{"title":"Contributing New Recipes​","type":1,"pageTitle":"Recipes","url":"/eole/docs/recipes/#contributing-new-recipes","content":" We welcome contributions to expand our library of recipes. Please follow these guidelines:  Respect Licenses: Ensure that you do not violate any licenses.Clear Documentation: Document your recipe thoroughly and ensure all scripts/commands are executable.Add Tests: Include tests if they are relevant to your recipe.Single Documentation File: Document your recipe in a single README.md file so it can be properly used in the documentation stack. ","version":"Next","tagName":"h2"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/eole/docs/quickstart","content":"","keywords":"","version":"Next"},{"title":"How to train a model from scratch​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#how-to-train-a-model-from-scratch","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Prepare the data​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-1-prepare-the-data","content":" To get started, we propose to download a toy English-German dataset for machine translation containing 10k tokenized sentences:  wget https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz tar xf toy-ende.tar.gz cd toy-ende   The data consists of parallel source (src) and target (tgt) data containing one sentence per line with tokens separated by a space:  src-train.txttgt-train.txtsrc-val.txttgt-val.txt  Validation files are used to evaluate the convergence of the training. It usually contains no more than 5k sentences.  $ head -n 2 toy-ende/src-train.txt It is not acceptable that , with the help of the national bureaucracies , Parliament &amp;apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance . Federal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .   We need to build a YAML configuration file to specify the data that will be used:  # toy_en_de.yaml ## Where the samples will be written save_data: toy-ende/run/example ## Where the vocab(s) will be written src_vocab: toy-ende/run/example.vocab.src tgt_vocab: toy-ende/run/example.vocab.tgt # Prevent overwriting existing files in the folder overwrite: False # Corpus opts: data: corpus_1: path_src: toy-ende/src-train.txt path_tgt: toy-ende/tgt-train.txt valid: path_src: toy-ende/src-val.txt path_tgt: toy-ende/tgt-val.txt   From this configuration, we can build the vocab(s), that will be necessary to train the model:  eole build_vocab -config toy_en_de.yaml -n_sample 10000   Notes:  -n_sample is advised here – it represents the number of lines sampled from each corpus to build the vocab.This configuration is the simplest possible, without any tokenization or other transforms. See recipes for more complex pipelines.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Train the model​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-2-train-the-model","content":" To train a model, we need to add the following to the YAML configuration file:  the vocabulary path(s) that will be used: can be that generated by eole build_vocab;training specific parameters.  # toy_en_de.yaml # Model architecture model: architecture: transformer # Train on a single GPU training: world_size: 1 gpu_ranks: [0] model_path: toy-ende/run/model save_checkpoint_steps: 500 train_steps: 1000 valid_steps: 500 # adapt dataloading defaults to very small dataset bucket_size: 1000   Then you can simply run:  eole train -config toy_en_de.yaml   This configuration will run a default transformer model. It will run on a single GPU (world_size 1 &amp; gpu_ranks [0]).  Before the training process actually starts, it is possible to generate transformed samples to simplify any potentially required visual inspection. The number of sample lines to dump per corpus is set with the -n_sample flag.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Translate​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-3-translate","content":" eole predict -model_path toy-ende/run/model -src toy-ende/src-test.txt -output toy-ende/pred_1000.txt -gpu 0 -verbose   Now you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into toy-ende/pred_1000.txt.  Note:  The predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets!  For example you can download millions of parallel sentences for translation or summarization.  ","version":"Next","tagName":"h3"},{"title":"How to generate with a pretrained LLM​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#how-to-generate-with-a-pretrained-llm","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Convert a model from Hugging Face Hub​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-1-convert-a-model-from-hugging-face-hub","content":" Several converters are provided for models 1) from Hugging Face hub: T5, Falcon, MPT, Openllama, Redpajama, Xgen or 2) the legacy Llama from Meta.  See here for conversion command line.  T5 (and variant Flan-T5), Llama and Openllama use Sentencepiece. Other models uses BPE, we had to reconstruct the BPE model and vocab file:  MPT bpe model  MPT vocab  Redpajama bpe model  Redpajama vocab  Falcon bpe model  Falcon vocab  The command line to convert a model to OpenNMT-py is:  Note: providing a HuggingFace repo id is supported in most conversion tools.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Prepare an inference.yaml config file​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-2-prepare-an-inferenceyaml-config-file","content":" Even though it is not mandatory, the best way to run inference is to use a config file; here is an example:  transforms: [sentencepiece] #### Subword src_subword_model: &quot;/path_to/llama7B/tokenizer.model&quot; tgt_subword_model: &quot;/path_to/llama7B/tokenizer.model&quot; # Model info model_path: &quot;/path_to/llama7B/llama7B-eole.pt&quot; # Inference seed: 42 max_length: 256 gpu: 0 batch_type: sents batch_size: 1 compute_dtype: fp16 #random_sampling_topk: 40 #random_sampling_topp: 0.75 #random_sampling_temp: 0.1 beam_size: 1 n_best: 1 report_time: true   or similarly for a model using BPE:  transforms: [onmt_tokenize] #### Subword src_subword_type: bpe src_subword_model: &quot;/path_to/mpt7B/mpt-model.bpe&quot; src_onmttok_kwargs: '{&quot;mode&quot;: &quot;conservative&quot;}' tgt_subword_type: bpe tgt_subword_model: &quot;/path_to/mpt7B/mpt-model.bpe&quot; tgt_onmttok_kwargs: '{&quot;mode&quot;: &quot;conservative&quot;}' gpt2_pretok: true # Model info model_path: &quot;/path_to/mpt7B/mpt-eole.pt&quot; # Inference seed: 42 max_length: 1 gpu: 0 batch_type: sents batch_size: 1 compute_dtype: fp16 #random_sampling_topk: 40 #random_sampling_topp: 0.75 #random_sampling_temp: 0.8 beam_size: 1 report_time: true src: None tgt: None   In this second example, we used max_length: 1 and src: None tgt: None which is typically the configuration to be used in a scoring script like MMLU where it expects only 1 token as the answer.  WARNINGFor inhomogeneous batches with many examples, the potentially high number of tokens inserted in the shortest examples leads to degraded results when attention layer quantization and flash attention are activated. In practice, in the inference configuration file, when batch_size is greater than 1, delete ‘linear_values’, ‘linear_query’, ‘linear_keys’, ‘final_linear’ from quant_layers and specify self_attn_type: scaled-dot.  You can run this script with the following command line:  eole tools run_mmlu --config myinference.yaml   ","version":"Next","tagName":"h3"},{"title":"Step 3: Generate text​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#step-3-generate-text","content":" Generating text is also easier with an inference config file (in which you can set max_length or ramdom sampling settings):  eole predict --config /path_to_config/llama7B/llama-inference.yaml --src /path_to_source/input.txt --output /path_to_target/output.txt   ","version":"Next","tagName":"h3"},{"title":"How to finetune a pretrained LLM​","type":1,"pageTitle":"Quickstart","url":"/eole/docs/quickstart#how-to-finetune-a-pretrained-llm","content":" See Llama2 recipe for an end-to-end example.  Note:  If you want to enable the “zero-out prompt loss” mechanism to ignore the prompt when calculating the loss, you can add the insert_mask_before_placeholder transform as well as the zero_out_prompt_loss flag:  transforms: [insert_mask_before_placeholder, sentencepiece, filtertoolong] zero_out_prompt_loss: true   The default value for the response response_pattern used to locate the end of the prompt is “Response : ｟newline｠”, but you can choose another to align it with your training data. ","version":"Next","tagName":"h2"},{"title":"fineweb10B","type":0,"sectionRef":"#","url":"/eole/docs/recipes/fineweb10B/","content":"fineweb10B This recipe is inspired from karpathy/llm.c#481 Disclaimer: it might not reproduce the results strictly, but should be a good starting point for whomever wanting to dive in such a topic.","keywords":"","version":"Next"},{"title":"CometKiwi","type":0,"sectionRef":"#","url":"/eole/docs/recipes/cometkiwi/","content":"","keywords":"","version":"Next"},{"title":"To make your life easier, run these commands from the recipe directory (here recipes/cometkiwi).​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#to-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipescometkiwi","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert the base model​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#download-and-convert-the-base-model","content":" eole convert HF --model_dir facebook/xlm-roberta-xxl --output $EOLE_MODEL_DIR/xlm-roberta-xxl-eole   NOTEThe facebook original model is stored in FP32 but we convert it to FP16 at conversion.  XXL is a 10.7G params model hence will save a 21.4GB file on disk (safetensors format) XL is a 3.5GB params model hence will save a 7.0GB file on disk  After conversion in $EOLE_MODEL_DIR you will get the following files: config.json model.00.safetensors sentencepiece.bpe.model vocab.json vocab.txt  The vocab.txt file contains 250000 entries (from sentencepiece) but the model was trained with extra tokens You need to concat the added_vocab.txt file to the vocab.txt file resulting in 250880 tokens  ","version":"Next","tagName":"h3"},{"title":"Training cometkiwi​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#training-cometkiwi","content":" Training will happen in two steps (see cometkiwi-xxl-eole.yaml file) FYI the trained model can be downloaded here: https://huggingface.co/eole-nlp/cometkiwi-xxl-eole/  STEP 1: We train from the converted xlm-roberta-xxl model but we keep everything frozen. We just add an Estimator layer that will be trained during 4000 steps To make sure we do not lose anything we will rename the pre-trained subfolder into step_4000_pretrainIn this step we do NOT use LoRA but we use 4bit quant to make things easier and fit in a smaller cards  STEP 2: We release the encoder to make weights trainable hence we need to use LoRA since the model is big and gradients would not fit in consumer grade cards. We train during 40000 steps  For the two steps above, the training command is the same but make sure the yaml file is modified according to the instructionseole train --config cometkiwi-xxl-eole.yaml  After this step we need to merge the LoRA weights into the original model with the following command:  eole model lora --base_model &quot;./cometkiwi-xxl-eole/step_4000_pretrain/&quot; --lora_weights &quot;./cometkiwi-xxl-eole/&quot; --output &quot;./cometkiwi-xxl-eole/merged&quot;  ","version":"Next","tagName":"h2"},{"title":"Cometkiwi Inference​","type":1,"pageTitle":"CometKiwi","url":"/eole/docs/recipes/cometkiwi/#cometkiwi-inference","content":" Format the source / target file you want to score in the Comet format:  ./combine.sh newstest2014.en newstest2014.de newstest2014.comb  Score the .comb file:  eole predict --config cometkiwi-xxl-inference.yaml --src newstest2014.comb --output newstest2014.scores --with_score  For now the scores are in the third column, so you can cut -f3 the output file ","version":"Next","tagName":"h2"},{"title":"OpenAI GPT2","type":0,"sectionRef":"#","url":"/eole/docs/recipes/gpt2/","content":"","keywords":"","version":"Next"},{"title":"Convert​","type":1,"pageTitle":"OpenAI GPT2","url":"/eole/docs/recipes/gpt2/#convert","content":" eole convert HF --model_dir openai-community/gpt2 --output $EOLE_MODEL_DIR/openai_gpt2 --token $HF_TOKEN   ","version":"Next","tagName":"h2"},{"title":"Infer​","type":1,"pageTitle":"OpenAI GPT2","url":"/eole/docs/recipes/gpt2/#infer","content":" echo -e &quot;The European Union was created in&quot; &gt; lm_input.txt eole predict -c inference.yaml   ","version":"Next","tagName":"h2"},{"title":"HellaSwag benchmark​","type":1,"pageTitle":"OpenAI GPT2","url":"/eole/docs/recipes/gpt2/#hellaswag-benchmark","content":" eole tools eval_hellaswag -c inference.yaml   Eole results, marginally different due to slight implementation differences (nn.Linear vs nn.Conv1D):  ... 10040 acc: 0.2865 acc_norm: 2959/10040=0.2947 10041 acc: 0.2864 acc_norm: 2959/10041=0.2947 10042 acc: 0.2864 acc_norm: 2960/10042=0.2948   Comparable results from llm.c script using official huggingface implementation:  ... 10040 acc: 0.2862 acc_norm: 2966/10040=0.2954 10041 acc: 0.2861 acc_norm: 2966/10041=0.2954 10042 acc: 0.2861 acc_norm: 2967/10042=0.2955  ","version":"Next","tagName":"h2"},{"title":"Serving Eurollm in a gradio interface with Eole","type":0,"sectionRef":"#","url":"/eole/docs/recipes/eurollm/","content":"","keywords":"","version":"Next"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#download-and-convert-model","content":" eole convert HF --model_dir utter-project/EuroLLM-9B-Instruct --output $EOLE_MODEL_DIR/EuroLLM-9B-Instruct --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Run server with the config file from this folder (you can add options according to your needs)​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#run-server-with-the-config-file-from-this-folder-you-can-add-options-according-to-your-needs","content":" eole serve -c serve.yaml   ","version":"Next","tagName":"h2"},{"title":"Start the gradio based translator​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#start-the-gradio-based-translator","content":" python eole-translator.py   You can access the Web based translator from the url given by Gradio (either local or from Gradio proxy with share=True turned on)  ","version":"Next","tagName":"h2"},{"title":"Alternatively you can also play with the API​","type":1,"pageTitle":"Serving Eurollm in a gradio interface with Eole","url":"/eole/docs/recipes/eurollm/#alternatively-you-can-also-play-with-the-api","content":" FastAPI exposes a swagger UI by default. It should be accessible via your browser at http://localhost:5000/docs. ","version":"Next","tagName":"h2"},{"title":"Llama2","type":0,"sectionRef":"#","url":"/eole/docs/recipes/llama2/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/llama2).​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama2","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#download-and-convert-model","content":" eole convert HF --model_dir meta-llama/Llama-2-7b-chat-hf --output $EOLE_MODEL_DIR/llama2-7b-chat-hf --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Write test prompt to text file​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#write-test-prompt-to-text-file","content":" echo -e &quot;&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt; You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. &lt;&lt;/SYS&gt;&gt; What are some nice places to visit in France? [/INST]&quot; | sed ':a;N;$!ba;s/\\n/｟newline｠/g' &gt; test_prompt.txt   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#run-inference","content":" Single GPU  eole predict -c llama-inference.yaml -src test_prompt.txt -output test_output.txt   Dual GPU (tensor parallelism)  eole predict -c llama-inference-tp-2gpu.yaml -src test_prompt.txt -output test_output.txt   ","version":"Next","tagName":"h3"},{"title":"Finetuning​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#finetuning","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve data​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#retrieve-data","content":" [ ! -d ./data ] &amp;&amp; mkdir ./data # Alpaca wget -P ./data https://opennmt-models.s3.amazonaws.com/llama/alpaca_clean.txt # Vicuna wget -P ./data https://opennmt-models.s3.amazonaws.com/llama/sharegpt.txt # Open Assisstant wget -P ./data https://opennmt-models.s3.amazonaws.com/llama/osst1.flattened.txt   ","version":"Next","tagName":"h3"},{"title":"Finetune​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#finetune","content":" eole train -c llama-finetune.yaml   ","version":"Next","tagName":"h3"},{"title":"Merge LoRa weights​","type":1,"pageTitle":"Llama2","url":"/eole/docs/recipes/llama2/#merge-lora-weights","content":" eole model lora --action merge --base_model ${EOLE_MODEL_DIR}/llama2-7b-chat-hf --lora_weights ./finetune/llama2-7b-chat-hf-finetune --output ./finetune/merged   Then you can just update your inference setup to use the newly finetuned &amp; merged model. ","version":"Next","tagName":"h3"},{"title":"Llama3","type":0,"sectionRef":"#","url":"/eole/docs/recipes/llama3/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/llama3).​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama3","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#download-and-convert-model","content":" eole convert HF --model_dir meta-llama/Meta-Llama-3-8B-Instruct --output $EOLE_MODEL_DIR/llama3-8b-instruct --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Write test prompt to text file​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#write-test-prompt-to-text-file","content":" echo -e &quot;What are some nice places to visit in France?&quot; | sed ':a;N;$!ba;s/\\n/｟newline｠/g' &gt; test_prompt.txt   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3/#run-inference","content":" eole predict -c llama-inference.yaml -src test_prompt.txt -output test_output.txt  ","version":"Next","tagName":"h3"},{"title":"Mixtral","type":0,"sectionRef":"#","url":"/eole/docs/recipes/mixtral/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/mixtral).​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesmixtral","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#download-and-convert-model","content":" eole convert HF --model_dir TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ --output ${EOLE_MODEL_DIR}/mixtral-8x7b-instruct-v0.1-awq --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Write test prompt to text file​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#write-test-prompt-to-text-file","content":" echo -e &quot;What are some nice places to visit in France?&quot; | sed ':a;N;$!ba;s/\\n/｟newline｠/g' &gt; test_prompt.txt   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Mixtral","url":"/eole/docs/recipes/mixtral/#run-inference","content":" eole predict -c mixtral-inference-awq.yaml -src test_prompt.txt -output test_output.txt  ","version":"Next","tagName":"h3"},{"title":"Mistral","type":0,"sectionRef":"#","url":"/eole/docs/recipes/mistral/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/mistral).​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesmistral","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#download-and-convert-model","content":" eole convert HF --model_dir mistralai/Mistral-7B-v0.3 --output ${EOLE_MODEL_DIR}/mistral-7b-v0.3 --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Write test prompt to text file​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#write-test-prompt-to-text-file","content":" echo -e &quot;What are some nice places to visit in France?&quot; | sed ':a;N;$!ba;s/\\n/｟newline｠/g' &gt; test_prompt.txt   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Mistral","url":"/eole/docs/recipes/mistral/#run-inference","content":" eole predict -c mistral-7b-awq-gemm-inference.yaml -src test_prompt.txt -output test_output.txt  ","version":"Next","tagName":"h3"},{"title":"Model Validator","type":0,"sectionRef":"#","url":"/eole/docs/recipes/model-validator/","content":"","keywords":"","version":"Next"},{"title":"1. Edit the models you want to run in the run.sh script:​","type":1,"pageTitle":"Model Validator","url":"/eole/docs/recipes/model-validator/#1-edit-the-models-you-want-to-run-in-the-runsh-script","content":" #!/bin/bash SCRIPT_DIR=$( cd -- &quot;$( dirname -- &quot;${BASH_SOURCE[0]}&quot; )&quot; &amp;&gt; /dev/null &amp;&amp; pwd ) # Define the models table models=( &quot;mistralai/Ministral-8B-Instruct-2410&quot; &quot;mistralai/Mistral-7B-v0.3&quot; &quot;mistralai/Mathstral-7B-v0.1&quot; &quot;meta-llama/Llama-3.2-1B&quot; ... ) ...   ","version":"Next","tagName":"h2"},{"title":"2. Run the script​","type":1,"pageTitle":"Model Validator","url":"/eole/docs/recipes/model-validator/#2-run-the-script","content":" nohup ./run.sh &gt; &lt;custom_logfile_name&gt;.log  ","version":"Next","tagName":"h2"},{"title":"Evaluation with MMLU dataset","type":0,"sectionRef":"#","url":"/eole/docs/recipes/mmlu/","content":"","keywords":"","version":"Next"},{"title":"How to run​","type":1,"pageTitle":"Evaluation with MMLU dataset","url":"/eole/docs/recipes/mmlu/#how-to-run","content":" To run the MMLU benchmark with a specific model, execute the following command from the recipe directory (recipes/mmlu):  eole tools run_mmlu -c &lt;model_inference_config&gt;   For instance, following the llama3 recipe:  eole tools run_mmlu -c ../llama3/llama-mmlu.yaml   ","version":"Next","tagName":"h2"},{"title":"Results​","type":1,"pageTitle":"Evaluation with MMLU dataset","url":"/eole/docs/recipes/mmlu/#results","content":" Note: Below are the legacy OpenNMT-py results. We might re-run and update everything at some point.  All evaluations below have been computed with the OpenNMT-py converted models.  The evaluation script is taken from the https://github.com/FranxYao/chain-of-thought-hub repo and modified to use the OpenNMT-py models  There is a difference compared to the original MMLU Hendrycks script.  We do not compare the logprobs of A, B, C, D to determine the answer, we actually decode the next token after the prompt.  When the model is Sentencepiece based the next token can be 'A', 'B', 'C', 'D' or any other token. When the model is BPE based the tokens will be ' A', ' B', ' C', ' D' because the leading space is encoded with the letter, We strip that space to compute the metric.  For 7B params models:  Llama7B score (35.25) matches both the Llama paper and the score reported by chain-of-thought-hub Falcon7B is a little higher then the score reported by chain-of-thought-hub (0.2641) I ran MPT7B with chain-of-thought-hub and found 28.46, again ours is a little higher. There are major discrepancies between those scores and Open LLM leaderboard of HF for MPT, Falcon, Redpajama that are way higher on the leaderboard.  For 13B, 33B, 40B models, we score with the 4-bit loading option, hence for Llama13B a score slightly under the paper (46.9), same for 33B (paper is 57.8)  \tMPT7B\tRedpajama7B\tOpen Llama7B\tFalcon7B\txgen7B\tFlan-T5-3B\tLlama7B\tLlama-2-7B\tLlama-2-chat-7B\tOpen Llama13B\tLlama13B\tLlama-2-13B\tLlama-2-chat-13B\tFalcon40B\tLlama33B\tLlama-2-70BACC-all\t0.2958\t0.2745\t0.3007\t0.2765\t0.3468\t0.4929\t0.3525\t0.4587\t0.4569\t0.4148\t0.4472\t0.5429\t0.5217\t0.5499\t0.5701\t0.6875 ACC-abstract_algebra\t0.2200\t0.2500\t0.3000\t0.2400\t0.2900\t0.2700\t0.2500\t0.3000\t0.3100\t0.3200\t0.2800\t0.3100\t0.3500\t0.3200\t0.3700\t0.3900 ACC-anatomy\t0.2963\t0.2667\t0.3333\t0.2444\t0.3185\t0.4296\t0.3852\t0.4815\t0.4222\t0.4667\t0.4889\t0.5037\t0.5037\t0.5111\t0.5185\t0.6296 ACC-astronomy\t0.2961\t0.2763\t0.2500\t0.2434\t0.3355\t0.4737\t0.3487\t0.4079\t0.4803\t0.4737\t0.4671\t0.5263\t0.5461\t0.5658\t0.6118\t0.7895 ACC-business_ethics\t0.2900\t0.2900\t0.3200\t0.1900\t0.3200\t0.6800\t0.4100\t0.5300\t0.4200\t0.4100\t0.4300\t0.5500\t0.5000\t0.5500\t0.5800\t0.6900 ACC-clinical_knowledge\t0.2943\t0.3208\t0.3887\t0.3019\t0.3057\t0.5245\t0.3585\t0.4604\t0.5208\t0.4113\t0.4189\t0.5811\t0.5698\t0.6113\t0.5547\t0.7019 ACC-college_biology\t0.3056\t0.3125\t0.3264\t0.2153\t0.3958\t0.4444\t0.3819\t0.4722\t0.5417\t0.4167\t0.4722\t0.5694\t0.5347\t0.6319\t0.5833\t0.8333 ACC-college_chemistry\t0.2800\t0.2700\t0.2400\t0.2300\t0.2500\t0.3400\t0.2900\t0.3400\t0.2500\t0.2800\t0.2400\t0.3900\t0.3600\t0.4100\t0.3800\t0.5200 ACC-college_computer_science\t0.3100\t0.3100\t0.3100\t0.3000\t0.3300\t0.3600\t0.2900\t0.3400\t0.3600\t0.4000\t0.3700\t0.4600\t0.5100\t0.4700\t0.4400\t0.6000 ACC-college_mathematics\t0.2900\t0.2500\t0.2800\t0.2900\t0.3200\t0.2900\t0.3400\t0.3800\t0.3400\t0.3200\t0.2500\t0.3000\t0.2800\t0.3500\t0.3600\t0.3700 ACC-college_medicine\t0.2890\t0.2659\t0.3179\t0.2659\t0.3410\t0.4277\t0.3237\t0.4220\t0.4104\t0.3699\t0.4220\t0.5318\t0.4451\t0.4798\t0.5376\t0.6532 ACC-college_physics\t0.2157\t0.2451\t0.1863\t0.2157\t0.2353\t0.2941\t0.2451\t0.2255\t0.2451\t0.2549\t0.1863\t0.2647\t0.3137\t0.3333\t0.3137\t0.3333 ACC-computer_security\t0.3100\t0.3600\t0.3800\t0.2800\t0.3900\t0.6400\t0.4500\t0.6200\t0.5400\t0.5400\t0.6300\t0.6900\t0.6700\t0.6500\t0.6800\t0.8100 ACC-conceptual_physics\t0.3362\t0.2723\t0.3064\t0.3149\t0.3489\t0.4085\t0.3702\t0.4170\t0.3872\t0.3574\t0.3915\t0.4511\t0.3787\t0.4170\t0.4723\t0.6723 ACC-econometrics\t0.2895\t0.2368\t0.2895\t0.2632\t0.2632\t0.2807\t0.2632\t0.2632\t0.3333\t0.3070\t0.2719\t0.2895\t0.3158\t0.3246\t0.3333\t0.4123 ACC-electrical_engineering\t0.2897\t0.3034\t0.3034\t0.2828\t0.3862\t0.4552\t0.2483\t0.4759\t0.4345\t0.4966\t0.3862\t0.5172\t0.5103\t0.5034\t0.4690\t0.6276 ACC-elementary_mathematics\t0.2698\t0.2646\t0.2698\t0.2593\t0.2725\t0.3148\t0.2646\t0.2672\t0.2857\t0.2487\t0.2487\t0.3360\t0.3333\t0.3413\t0.3413\t0.4180 ACC-formal_logic\t0.2540\t0.4048\t0.2381\t0.1905\t0.2619\t0.3333\t0.2619\t0.2698\t0.2381\t0.3016\t0.3889\t0.3492\t0.2857\t0.3413\t0.3571\t0.5000 ACC-global_facts\t0.2700\t0.3200\t0.3200\t0.3100\t0.3300\t0.3600\t0.3000\t0.3200\t0.3100\t0.2900\t0.3400\t0.3200\t0.2900\t0.3300\t0.3900\t0.4500 ACC-high_school_biology\t0.3097\t0.2484\t0.2968\t0.2645\t0.3290\t0.5645\t0.3387\t0.5065\t0.5258\t0.4290\t0.5065\t0.6742\t0.6194\t0.6516\t0.6419\t0.8194 ACC-high_school_chemistry\t0.2020\t0.2660\t0.2512\t0.2512\t0.2611\t0.3300\t0.2956\t0.3744\t0.3547\t0.3350\t0.2660\t0.4286\t0.4138\t0.4187\t0.3793\t0.5468 ACC-high_school_computer_science\t0.3400\t0.2700\t0.2800\t0.3200\t0.3200\t0.5100\t0.3300\t0.4000\t0.4500\t0.2700\t0.4500\t0.5500\t0.5800\t0.6000\t0.5800\t0.7700 ACC-high_school_european_history\t0.3455\t0.2848\t0.3455\t0.2909\t0.3879\t0.7333\t0.4667\t0.6121\t0.5818\t0.4727\t0.6121\t0.6545\t0.6667\t0.6667\t0.7152\t0.8121 ACC-high_school_geography\t0.3737\t0.3283\t0.3333\t0.1667\t0.3636\t0.6414\t0.3333\t0.4899\t0.5960\t0.4899\t0.5000\t0.6616\t0.6616\t0.7121\t0.7273\t0.8636 ACC-high_school_government_and_politics\t0.3782\t0.2124\t0.3575\t0.2591\t0.4352\t0.6632\t0.4611\t0.6736\t0.6632\t0.5959\t0.6425\t0.8135\t0.7617\t0.7927\t0.8187\t0.9430 ACC-high_school_macroeconomics\t0.3821\t0.2718\t0.3564\t0.2615\t0.3359\t0.5359\t0.3410\t0.4513\t0.4103\t0.4282\t0.4256\t0.4923\t0.4744\t0.5641\t0.5590\t0.7308 ACC-high_school_mathematics\t0.2778\t0.2667\t0.2407\t0.2481\t0.2333\t0.3074\t0.2630\t0.2963\t0.2556\t0.2667\t0.2593\t0.2889\t0.3037\t0.3111\t0.2741\t0.3630 ACC-high_school_microeconomics\t0.2941\t0.3067\t0.2941\t0.2899\t0.3697\t0.5168\t0.3319\t0.4412\t0.4328\t0.4370\t0.4454\t0.5630\t0.5042\t0.5504\t0.5588\t0.7605 ACC-high_school_physics\t0.2583\t0.2649\t0.2517\t0.3179\t0.2450\t0.2980\t0.2649\t0.3179\t0.3046\t0.2980\t0.2517\t0.3444\t0.3245\t0.2914\t0.3311\t0.3907 ACC-high_school_psychology\t0.2844\t0.3229\t0.3505\t0.2440\t0.4752\t0.6771\t0.4789\t0.6312\t0.6477\t0.5486\t0.5835\t0.7413\t0.7229\t0.7541\t0.7596\t0.8752 ACC-high_school_statistics\t0.4028\t0.2454\t0.3981\t0.1852\t0.1620\t0.3657\t0.3241\t0.2778\t0.3241\t0.2546\t0.2685\t0.4722\t0.3611\t0.4630\t0.4676\t0.6157 ACC-high_school_us_history\t0.2892\t0.2255\t0.3137\t0.2892\t0.4167\t0.6863\t0.3284\t0.5245\t0.6765\t0.5490\t0.5343\t0.7108\t0.6863\t0.7108\t0.7696\t0.9069 ACC-high_school_world_history\t0.2489\t0.2785\t0.2869\t0.2996\t0.3966\t0.6667\t0.4262\t0.6245\t0.6667\t0.5105\t0.6287\t0.7089\t0.7215\t0.6835\t0.7637\t0.8608 ACC-human_aging\t0.3274\t0.1659\t0.2870\t0.4215\t0.4260\t0.5650\t0.3991\t0.5695\t0.5695\t0.5157\t0.5112\t0.6502\t0.6816\t0.7130\t0.6861\t0.7848 ACC-human_sexuality\t0.3511\t0.2519\t0.2748\t0.2901\t0.3359\t0.5802\t0.3435\t0.5649\t0.4885\t0.4962\t0.5649\t0.6031\t0.5878\t0.6794\t0.6718\t0.8550 ACC-international_law\t0.3802\t0.2231\t0.3636\t0.2479\t0.5041\t0.6860\t0.5207\t0.6529\t0.5620\t0.5207\t0.6860\t0.6860\t0.7851\t0.6612\t0.7603\t0.8595 ACC-jurisprudence\t0.3704\t0.2315\t0.3426\t0.3426\t0.4074\t0.6204\t0.4167\t0.5370\t0.5833\t0.4444\t0.4722\t0.6852\t0.7037\t0.6667\t0.6574\t0.8148 ACC-logical_fallacies\t0.2945\t0.2638\t0.2883\t0.2638\t0.3558\t0.6319\t0.4172\t0.5092\t0.5399\t0.4847\t0.5031\t0.6564\t0.6319\t0.6503\t0.6994\t0.7975 ACC-machine_learning\t0.3125\t0.2232\t0.2321\t0.3750\t0.2589\t0.3571\t0.2768\t0.3839\t0.3393\t0.3571\t0.3304\t0.3036\t0.3482\t0.3036\t0.3750\t0.5089 ACC-management\t0.3301\t0.2816\t0.2524\t0.2816\t0.3010\t0.6796\t0.3301\t0.5631\t0.6699\t0.5243\t0.6311\t0.7379\t0.7184\t0.7184\t0.7573\t0.8252 ACC-marketing\t0.3120\t0.2735\t0.3761\t0.2949\t0.5385\t0.7906\t0.4615\t0.6795\t0.7265\t0.5897\t0.7094\t0.8077\t0.7821\t0.7949\t0.8333\t0.8932 ACC-medical_genetics\t0.3100\t0.2400\t0.2700\t0.2800\t0.3600\t0.4800\t0.3700\t0.5500\t0.5000\t0.5100\t0.5100\t0.5500\t0.5700\t0.6200\t0.6100\t0.7400 ACC-miscellaneous\t0.3001\t0.2899\t0.3678\t0.2976\t0.5326\t0.6782\t0.4278\t0.6450\t0.6692\t0.5900\t0.6296\t0.7407\t0.7458\t0.7471\t0.7752\t0.8557 ACC-moral_disputes\t0.2977\t0.2659\t0.3295\t0.3092\t0.3613\t0.5983\t0.4133\t0.5116\t0.5145\t0.4798\t0.4566\t0.6272\t0.5809\t0.6503\t0.6503\t0.7572 ACC-moral_scenarios\t0.2436\t0.2469\t0.2469\t0.2492\t0.2425\t0.2436\t0.2425\t0.2380\t0.2145\t0.2715\t0.2480\t0.3464\t0.2927\t0.2615\t0.3855\t0.4413 ACC-nutrition\t0.2810\t0.2908\t0.3301\t0.2582\t0.3431\t0.4804\t0.3922\t0.4902\t0.5098\t0.3758\t0.5163\t0.6144\t0.5980\t0.6405\t0.6471\t0.7778 ACC-philosophy\t0.3183\t0.2830\t0.2830\t0.2830\t0.3151\t0.5177\t0.4051\t0.6013\t0.5659\t0.4662\t0.5145\t0.6656\t0.6077\t0.6399\t0.6656\t0.7781 ACC-prehistory\t0.3056\t0.3210\t0.3210\t0.3117\t0.3488\t0.5216\t0.3519\t0.4907\t0.5679\t0.5216\t0.5093\t0.6451\t0.5926\t0.5988\t0.6667\t0.8272 ACC-professional_accounting\t0.2447\t0.2872\t0.2553\t0.2979\t0.3050\t0.3723\t0.2730\t0.3582\t0.3475\t0.3050\t0.3227\t0.3830\t0.3759\t0.4255\t0.4326\t0.5780 ACC-professional_law\t0.2784\t0.2705\t0.2523\t0.2497\t0.2647\t0.3990\t0.2973\t0.3553\t0.3266\t0.3064\t0.3566\t0.4068\t0.3722\t0.4296\t0.4342\t0.5404 ACC-professional_medicine\t0.2206\t0.2059\t0.2500\t0.3125\t0.4375\t0.4412\t0.4265\t0.5184\t0.3529\t0.3860\t0.5000\t0.5221\t0.4706\t0.6176\t0.5441\t0.7390 ACC-professional_psychology\t0.2876\t0.2925\t0.2696\t0.2647\t0.3203\t0.4526\t0.3546\t0.4428\t0.4739\t0.3693\t0.4575\t0.5392\t0.5065\t0.5539\t0.6144\t0.7500 ACC-public_relations\t0.3455\t0.3182\t0.4091\t0.3364\t0.4182\t0.5909\t0.4091\t0.5273\t0.5182\t0.5273\t0.5545\t0.6364\t0.6091\t0.6364\t0.6818\t0.7273 ACC-security_studies\t0.3796\t0.2816\t0.2939\t0.3102\t0.2531\t0.6531\t0.3306\t0.4980\t0.4571\t0.4245\t0.5224\t0.6122\t0.6531\t0.6735\t0.6367\t0.8082 ACC-sociology\t0.2239\t0.2587\t0.2488\t0.3532\t0.4826\t0.7363\t0.4726\t0.6318\t0.5771\t0.5473\t0.6418\t0.7264\t0.7214\t0.7761\t0.7761\t0.8955 ACC-us_foreign_policy\t0.3500\t0.3200\t0.3900\t0.4200\t0.5100\t0.6600\t0.4300\t0.6500\t0.6700\t0.6100\t0.7200\t0.8500\t0.7700\t0.8000\t0.8300\t0.9100 ACC-virology\t0.3494\t0.2530\t0.3494\t0.3554\t0.3735\t0.4819\t0.3253\t0.4217\t0.4277\t0.4398\t0.4096\t0.4458\t0.4940\t0.4639\t0.5000\t0.5361 ACC-world_religions\t0.3158\t0.3041\t0.4035\t0.3333\t0.6140\t0.5614\t0.4912\t0.6842\t0.6842\t0.6550\t0.6491\t0.7602\t0.7427\t0.7719\t0.7953\t0.8538 0.3022\t0.2747\t0.3053\t0.2818\t0.3515\t0.5018\t0.3569\t0.4682\t0.4662\t0.4258\t0.4559\t0.5482\t0.5340\t0.5580\t0.5741\t0.6932 ","version":"Next","tagName":"h2"},{"title":"Example of using Hugging Face streaming dataset","type":0,"sectionRef":"#","url":"/eole/docs/recipes/NewsPalm-synthetic/","content":"","keywords":"","version":"Next"},{"title":"Based on: https://arxiv.org/pdf/2408.06537​","type":1,"pageTitle":"Example of using Hugging Face streaming dataset","url":"/eole/docs/recipes/NewsPalm-synthetic/#based-on-httpsarxivorgpdf240806537","content":" Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality Parallel Data Outperforms Traditional Web-Crawled Data  ","version":"Next","tagName":"h2"},{"title":"Get the vocab and BPE model on HF​","type":1,"pageTitle":"Example of using Hugging Face streaming dataset","url":"/eole/docs/recipes/NewsPalm-synthetic/#get-the-vocab-and-bpe-model-on-hf","content":" https://huggingface.co/eole-nlp/NewsPalmSynthetic-ENDE  copy files:  ende.vocab2subwords.en_de.bpe  ","version":"Next","tagName":"h3"},{"title":"Optionally you can get the trained model to test it.​","type":1,"pageTitle":"Example of using Hugging Face streaming dataset","url":"/eole/docs/recipes/NewsPalm-synthetic/#optionally-you-can-get-the-trained-model-to-test-it","content":" config.jsonvocab.jsonmodel.00.safetensors  ","version":"Next","tagName":"h3"},{"title":"Train with the yaml config file​","type":1,"pageTitle":"Example of using Hugging Face streaming dataset","url":"/eole/docs/recipes/NewsPalm-synthetic/#train-with-the-yaml-config-file","content":" eole train -c newspalm-synthetic-hfstreaming.yaml   ","version":"Next","tagName":"h2"},{"title":"Run inference​","type":1,"pageTitle":"Example of using Hugging Face streaming dataset","url":"/eole/docs/recipes/NewsPalm-synthetic/#run-inference","content":" eole predict -c inference.yaml --src newstest2023-src.en --output newstest2023-hyp.de   Then you can score with sacrebleu and/or comet  Scoring with Unbabel/wmt22-comet-da gives: 81.90  You can compare to table 5 lines 2a) to 2d) of the paper https://arxiv.org/pdf/2408.06537 ","version":"Next","tagName":"h2"},{"title":"NLLB","type":0,"sectionRef":"#","url":"/eole/docs/recipes/nllb/","content":"","keywords":"","version":"Next"},{"title":"Conversion​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#conversion","content":" ","version":"Next","tagName":"h2"},{"title":"1. Sentencepiece with OpenNMT Tokenizer​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#1-sentencepiece-with-opennmt-tokenizer","content":" eole convert HF --model_dir facebook/nllb-200-1.3B --output ./nllb-1.3b --token $HF_TOKEN --tokenizer onmt   ","version":"Next","tagName":"h3"},{"title":"2. HuggingFace Tokenizer​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#2-huggingface-tokenizer","content":" eole convert HF --model_dir facebook/nllb-200-1.3B --output ./nllb-1.3b --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#inference","content":" echo &quot;What is the weather like in Tahiti?&quot; &gt; test.en   ","version":"Next","tagName":"h2"},{"title":"1. Sentencepiece with OpenNMT Tokenizer​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#1-sentencepiece-with-opennmt-tokenizer-1","content":" eole predict -c inference-pyonmttok.yaml   ","version":"Next","tagName":"h3"},{"title":"2. HuggingFace Tokenizer​","type":1,"pageTitle":"NLLB","url":"/eole/docs/recipes/nllb/#2-huggingface-tokenizer-1","content":" eole predict -c inference-hf.yaml  ","version":"Next","tagName":"h3"},{"title":"Pixtral","type":0,"sectionRef":"#","url":"/eole/docs/recipes/pixtral/","content":"","keywords":"","version":"Next"},{"title":"Convert the model​","type":1,"pageTitle":"Pixtral","url":"/eole/docs/recipes/pixtral/#convert-the-model","content":" eole convert HF --model_dir mistral-community/pixtral-12b --output ./pixtral-12b --token $HF_TOKEN   ","version":"Next","tagName":"h2"},{"title":"Run the test script​","type":1,"pageTitle":"Pixtral","url":"/eole/docs/recipes/pixtral/#run-the-test-script","content":" python3 test_inference.py   There are several examples in the test script (taken from pixtral blog posts). A single one is activated by default, but you can uncomment the others to test the various cases.  ","version":"Next","tagName":"h2"},{"title":"Finetuning​","type":1,"pageTitle":"Pixtral","url":"/eole/docs/recipes/pixtral/#finetuning","content":" Finetuning is untested for now. Feel free to try it out and fix any arising issues. ","version":"Next","tagName":"h2"},{"title":"Llama3","type":0,"sectionRef":"#","url":"/eole/docs/recipes/llama3.1/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/llama3.1).​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipesllama31","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#download-and-convert-model","content":" eole convert HF --model_dir meta-llama/Meta-Llama-3.1-8B --output $EOLE_MODEL_DIR/llama3.1-8b --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Write test prompt to text file​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#write-test-prompt-to-text-file","content":" (Example prompt inspired from this HF PR: https://github.com/huggingface/transformers/pull/24653)  echo -e &quot;You are given this machine learning research paper, please read it carefully and answer the follow up question. === BEGIN === 2306.15595v2 [cs.CL] 28 Jun 2023 arXiv EXTENDING CONTEXT WINDOW OF LARGE LAN- GUAGE MODELS VIA POSITION INTERPOLATION Shouyuan Chen Sherman Wong Liangjian Chen Yuandong Tian Meta Platforms Inc. {chenshouyuan, shermanwong, cli, yuandong}@meta . com 1 INTRODUCTION Large language models (LLMs) typically come with a pre-defined context window size. For exam- ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set context window limit is frequently exceeded in applications such as conducting long conversations, summarizing long documents, or executing long-term planning. For these applications, LLMs with longer context windows are preferred. However, training an LLM from scratch with long context windows requires significant investments. This naturally leads to a question: Can we extend the context window of an existing pre-trained LLM? One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con- text window. However, empirically, we found that models trained this way adapt to long context windows very slowly. After training for more than 10000 batches, the effective context window saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is inefficient for extending to substantially longer context windows. While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length extrapolation of Transformers, i.e. train on short context windows and inference on longer ones, many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability of these techniques for extending the context window sizes of such LLMs remains limited. In this work, we introduce Position Interpolation to enable context window extensions for certain existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly down-scale the position indices so that the maximum position index matches the previous context window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom- modate more input tokens, we interpolate the position encodings at neighboring integer positions, utilizing the fact that position encodings can be applied on non-integer positions, as opposed to extrapolating outside the trained positions, which may lead to catastrophic values. We verify our approach theoretically, by showing that the interpolated attention score has a much smaller upper bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more stable. Therefore, interpolated position encodings are easier for the model to adapt. Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a very short period of fine-tuning for the model to fully adapt to greatly extended context windows. We present experimental results for extending the context window to up to 32768 from the initial 2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that 1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality. The cost of fine-tuning is negligible compared to the pre-training costs. This confirms our hypothesis that it is relatively easy for the models to adapt to interpolated position encodings. 2. Position Interpolation generates strong models that can effectively make use of much ex- tended context window. We show that models extended by Position Interpolation enjoy significant perplexity gains from greatly extended context windows for text modeling, and we show that the perplexity reduces graceful with the enlargement of context windows. We also applied Position Interpolation in a long text summarization task, and demonstrate competitive performances. 3. Position Interpolation preserves model quality relatively well for tasks within its original context window sizes. We present a variety of evaluation results for the extended LLaMA models on the original LLaMA benchmark. Compared with original LLaMA models, the extended LLLaM A models saw a minor degradation on several standard benchmarks within a 2048 token limit. Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex- trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct extrapolation of positional encodings and it can be largely mitigated by interpolating position en- codings instead. Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super- HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context window from 2K to 8K. Recently, open source community picks it up in Reddit post ! and Github Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we also give theoretical explanations why interpolation achieves much more stable results than extrap- olation, by showing that the upper bound of interplated attention score is much lower than that of extrapolated ones. 2 METHOD 2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE) Transformer models require explicit positional information to be injected, typically in the form of positional encodings, to represent the order of inputs. We consider Rotary Position Embedding (ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al., 2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as follows Using RoPE, the self-attention score is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the query and key vector for a specific attention head. At each layer, RoPE is applied on both query and key embeddings for computing attention scores. 2.2 DIRECT EXTRAPOLATION While the attention score in RoPE only depends on the relative positions, which is what we want, its extrapolation performance is not great . In particular, when directly extending to larger context windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., &gt; 10%), comparable to untrained models. Ideally, we want to see the model trained on a context window of size L = 2048 to still work reasonably well on longer context window, but may not have the capability to leverage information that appears beyond L. For example, to answer a question located at 3000, the model trained on maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are located at location 2900. What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3 of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and think about Eqn. 2 as basis expansion as the following: where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1) are complex coefficients depending on q and k (here the definition of h; is exactly the same as the definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but much larger in regions beyond. 2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI) In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s &gt; L, how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window? Formally, we replace RoPE f by {’ defined as follows We call this transformation on the position encoding Position Interpolation. In this step, we reduce position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE. Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been reduced from I’ to L. Since we align the ranges of position indices and relative distances before and after extension, we mitigate the effect on attention score computation due to context window extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the following theorem, we show that the interpolated attention score is well-behaved: While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d (check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x smaller than the extrapolation bound, and thus the interpolated attention score is much more stable than extrapolated one. Notably, our method of rescaling of position indices does not introduce extra weight, or modify the model architecture in any way. This makes it attractive in practical applications, since most infrastructure and optimization for the original model can be reused after the extension. Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task with interpolated position encodings on the extended context window size using a pre-training cor- pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning is not sensitive to the choice of examples. The reason may be that the model is only adapting to the new context window during the fine-tuning phase, starting from a good initialization, as opposed to acquiring new knowledge. Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola- tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal magnitude of query/key products. If we enforce a regularization on || during LLM training, it is possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo- lated a(s) when s &gt; L can be comparable to that within [0, L]. To our knowledge, we are not aware of existing LLM pre-training techniques that leverage this regularization and will leave it for future work. 3 EXPERIMENTS We show Position Interpolation can effectively extend context window up to 32 times of the original size, and such extension can be done with only several hundreds of training steps. We show the resulting models are strong LLMs with fully effective long context windows. We demonstrate its performance in a number of tasks including language modeling, passkey retrieval, and long doc- ument summarization. We also present benchmark results of the extended models on the original LLaMA evaluation benchmarks. 3.1 SETUP Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or Position Interpoloation method. Except for rescaling the position indices for models extended with Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any ways. Training Procedure. We fine-tune all model variants using the next token prediction objective. We use AdamW (Loshchilov &amp; Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models, we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and 128 global batch size. We note that the main need of using more GPUs is memory limitation during fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch (Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao et al., 2022). If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000 steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the RedPajama dataset (Computer, 2023). 3.2 LONG SEQUENCE LANGUAGE MODELING We evaluate the long sequence language modeling performance of our extended models and base- lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile dataset (Azerbayev et al., 2022). We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19, we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random subsample of 128 documents with at least 32768 SentencePiece (Kudo &amp; Richardson, 2018) tokens and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various context window size by using a sliding window approach following Press et al. (2022) with stride S = 256. In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets. From the results, we found that models extended with our method enjoy a significantly improved perplexity from longer context window sizes. By increasing the context window size from 2048 to 16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42 reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and -0.3 reductions of perplexity by extending to the 8192 context window size. In general, we observed a consistent trend of our models achieving better perplexity with longer context windows. This indicates our models can effectively make use of the longer context windows to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to 32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This indicates that our method may enable extension to even longer context windows. In contrast, we observed that models extended via the direct fine-tuning method has shown regres- sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows. This indicates that models extended this way have limited capability of making use of context win- dows longer than their pre-trained settings. We saw a minor degradation of the perplexity on the original context window of 2048 for our ex- tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation of performance within original evaluation context window is expected since Position Interpolation forces position encodings in original context window to reside in a much narrower region, which may negatively affect the language model’s performance. We present more benchmark results on the original context window size in Section 3.4. In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit certain language modeling capability, as indicated by &lt; 20 perplexity for extending to 8192 context window (in contrast, the direct extrapolation method leads to &gt; 10% perplexity). With fine-tuning, we observed that the perplexity improves quickly. At 200 steps the models surpassed the original model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can see the models have improved steadily and achieve a significantly better perplexity. 3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL We study the effective context window size, i.e. the maximum distance of a token can effectively attend to during inference, of our models after extension. To measure this, we follow a synthetic evaluation task of passkey retrieval proposed by Mohtashami &amp; Jaggi (2023). In this task, the models are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of the document. Given a language model, we estimate the upper and lower bounds of effective context windows as follows. Suppose the random passkey is k tokens away from the end of the input. When a model persistently fails to retrieve the correct passkey value across several independent attempts, it suggests that the effective context window size of the model is less than k. Conversely, if a model consistently succeeds in retrieving the correct passkey value, we deduce that the effective context window size of the model is at least k. We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or direct fine-tuning. For each model, we use 32 different &amp;£ uniformly spaced in the targeted context window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5 random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps, We can see that models extended via Position Interpolation all successfully attain their desired ex- tension objectives in terms of effective context window sizes, indicating by the effective context window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models that are extended via direct fine-tuning only saw a minimal increase of the effective context win- dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear indication of an acceleration in the increase of window size. 3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE We evaluate the models extended by Position Interpolation on several standard benchmark tasks within the original context window size of 2048. The evaluation results are listed in Table 5. From the results, we saw that models extended to 8192 produce comparable results on the original bench- mark which is designed for a much smaller context window, with a degradation of up to 2% on the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win- dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark performances, which may be due to the limited number of fine-tuning steps used in our method. The regression on benchmark tasks is consistent with our observation on perplexity regression in Section 3.2. 3.5 LONG DOCUMENT SUMMARIZATION In this task, we evaluate our models’ performance on the long document summarization task. In particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents for training and 972 documents for evaluation. Each document comes with a human generated summary. We truncate all input documents to their first 15000 tokens. We fine-tune the LL.aMA models extended with Position Interpolation with a context window of 16384. Note the rescaling of position indices are still required during this fine-tuning step. We first Model Size Context Window Fine-tune on BoolQ PIQA Race-M Race-H WinoGrande format the raw document using the prompt template in Figure 4, and then concatenate the prompt with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine- tune the model using the next token prediction task with the above setup for 10 epochs. The losses from the input prompt proportion of training examples are excluded during our fine-tuning. We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a summarization of each document in the test set. The final output is truncated at 1000 tokens. We used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate the models’ outputs vs the ground-truth summaries. In Table 6 we report our evaluation results. We have also included results from two baselines in existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This result suggests our models with 16384 context window can effectively handle the long document summarization task. === END OF FILE === Question: What is the paper about? Answer: &quot; | sed ':a;N;$!ba;s/\\n/｟newline｠/g' &gt; test_prompt.txt   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/llama3.1/#run-inference","content":" eole predict -c llama-inference.yaml -src test_prompt.txt -output test_output.txt  ","version":"Next","tagName":"h3"},{"title":"Serving models with Eole","type":0,"sectionRef":"#","url":"/eole/docs/recipes/server/","content":"","keywords":"","version":"Next"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Option 1 - Download and convert model​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#option-1---download-and-convert-model","content":" The first example &quot;llama3-8b-instruct&quot; requires you to manually convert the model in your desired $EOLE_MODEL_DIR.  eole convert HF --model_dir meta-llama/Meta-Llama-3-8B-Instruct --output $EOLE_MODEL_DIR/llama3-8b-instruct --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Option 2 - Retrieve an already converted model from HF​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#option-2---retrieve-an-already-converted-model-from-hf","content":" The second example &quot;llama3-8b-instruct-hf&quot; downloads a model that has already been converted, for the sake of this example.  ","version":"Next","tagName":"h3"},{"title":"Run server​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#run-server","content":" eole serve -c serve.example.yaml   ","version":"Next","tagName":"h2"},{"title":"Play with the API​","type":1,"pageTitle":"Serving models with Eole","url":"/eole/docs/recipes/server/#play-with-the-api","content":" FastAPI exposes a swagger UI by default. It should be accessible via your browser at http://localhost:5000/docs. ","version":"Next","tagName":"h2"},{"title":"Language Model Wiki-103","type":0,"sectionRef":"#","url":"/eole/docs/recipes/wiki_103/","content":"","keywords":"","version":"Next"},{"title":"Step 1: Download and clean the data, prepare subword model​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#step-1-download-and-clean-the-data-prepare-subword-model","content":" Preliminary steps are defined in the prepare_wikitext-103_data.sh script.  The following command will download the WikiText103 dataset, remove empty lines and shuffle the training corpus. It will also call the learn_bpe.py script to train a bpe of 40000 symbols on the training dataset using pyonmttok. The bpe model will be stored in data/wikitext/wikitext-103-raw-v1/subwords.bpe.  chmod u+x prepare_wikitext-103_data.sh ./prepare_wikitext-103_data.sh   ","version":"Next","tagName":"h2"},{"title":"Step 2: Build the vocabulary​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#step-2-build-the-vocabulary","content":" An example of yaml configuration for language modeling task is available in wiki_103.yaml. This configuration will be used for building the vocabulary and training the model. BPE and language modeling specificities are explained in the following sections.  ","version":"Next","tagName":"h2"},{"title":"Language Model specificities​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#language-model-specificities","content":" In LM tasks we expect a single source, therefore path_tgt is not required for LM tasks.  data: corpus_1: path_src: data/wikitext-103-raw/wiki.train.raw   ","version":"Next","tagName":"h3"},{"title":"BPE specificities​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#bpe-specificities","content":" To use BPE tokenization on the fly, the following parameters must be in the config file. Slight differences between on the fly tokenization and outputed tokenized files from step 1 can be observed.  transforms: [onmt_tokenize, filtertoolong] transforms_configs: onmt_tokenize: src_subword_type: bpe src_subword_model: data/wikitext-103-raw/subwords.bpe src_onmttok_kwargs: {&quot;mode&quot;: &quot;aggressive&quot;, &quot;joiner_annotate&quot;: True, &quot;preserve_placeholders&quot;: True, &quot;case_markup&quot;: True, &quot;soft_case_regions&quot;: True, &quot;preserve_segmented_tokens&quot;: True}   ","version":"Next","tagName":"h3"},{"title":"Build vocabulary command​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#build-vocabulary-command","content":" The vocabulary is built using:  eole build_vocab -config wiki_103.yaml -n_sample -1 # -num_threads 4   ","version":"Next","tagName":"h3"},{"title":"Step 3: Train the model​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#step-3-train-the-model","content":" The training is launched using:  eole train -config wiki_103.yaml   Tensorboard can be used to monitor the training.  Expected results: perplexity of 20-22 on the validation set.  ","version":"Next","tagName":"h2"},{"title":"Step 4: Generate output​","type":1,"pageTitle":"Language Model Wiki-103","url":"/eole/docs/recipes/wiki_103/#step-4-generate-output","content":" Options contained in the loaded model will trigger language modeling specific inference.  input.txt must contain already tokenized examples, with the same method as the training data. Here, part of validation data will be used:  head data/wikitext-103-raw/wiki.valid.bpe | cut -d&quot; &quot; -f-15 &gt; data/wikitext/wikitext-103-raw-v1/test_input.txt   To proceed with LM inference, sampling methods such as top-k sampling or nucleus sampling are usually applied. Details and options about inference methods can be found in eole/config/inference.py.  The following command will provide inference with nucleus sampling of p=0.9 and return the 3 sequences with the lowest perplexity out of the 10 generated sequences (see inference.yaml):  eole predict -config inference.yaml -model_path data/wikitext/wikitext-103-raw-v1/run/model-lm/step_1000000 -src data/wikitext/wikitext-103-raw-v1/test_input.txt -output data/wikitext/wikitext-103-raw-v1/test_pred.txt   Note: main transform-related settings are now stored within the model and its configuration, which makes the (rather complex) inference.yaml config not strictly necessary anymore. The above command can be converted to a simple command line with the desired settings:  eole predict -model_path data/wikitext/wikitext-103-raw-v1/run/model-lm/step_1000000 -src data/wikitext/wikitext-103-raw-v1/test_input.txt -output data/wikitext/wikitext-103-raw-v1/test_pred.txt -world_size 1 -gpu_ranks 0 -n_best 3 -top_p 0.9 -beam_size 10  ","version":"Next","tagName":"h2"},{"title":"Translation WMT17 en-de","type":0,"sectionRef":"#","url":"/eole/docs/recipes/wmt17/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/wmt17).​","type":1,"pageTitle":"Translation WMT17 en-de","url":"/eole/docs/recipes/wmt17/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipeswmt17","content":" ","version":"Next","tagName":"h2"},{"title":"Tokenization methods​","type":1,"pageTitle":"Translation WMT17 en-de","url":"/eole/docs/recipes/wmt17/#tokenization-methods","content":" The following configurations as provided as example:  wmt17_ende_yaml: &quot;legacy&quot; configuration, using already tokenized data;wmt17_ende_bpe.yaml: on-the-fly bpe tokenization, using the &quot;official&quot; subword-nmt based bpe transform;wmt17_ende_bpe_onmt_tokenize.yaml: on-the-fly bpe tokenization, using the pyonmttok based onmt_tokenize transform;wmt17_ende_spm.yaml: on-the-fly sentencepiece tokenization, using the official sentencepiece based sentencepiece transform;wmt17_ende_spm_onmt_tokenize.yaml: on-the-fly sentencepiece tokenization, using the pyonmttok based onmt_tokenize transform;  ","version":"Next","tagName":"h3"},{"title":"Get Data and prepare​","type":1,"pageTitle":"Translation WMT17 en-de","url":"/eole/docs/recipes/wmt17/#get-data-and-prepare","content":" WMT17 English-German data set:  cd recipes/wmt17 bash prepare_wmt_ende_data.sh   Options:  --method: bpe/sentencepiece (subwords method to use)--encode: true/false (tokenize all datasets, not necessary if using on the fly transforms)  If you want to use one of the aforementioned configurations with on-the-fly transforms, set --encode false, and either of --method bpe/--method sentecepiece.  ","version":"Next","tagName":"h3"},{"title":"Train​","type":1,"pageTitle":"Translation WMT17 en-de","url":"/eole/docs/recipes/wmt17/#train","content":" Choose the config you want to run:  export CONFIG=&quot;wmt17_ende_bpe.yaml&quot;   Training the following big transformer for 50K steps takes less than 10 hours on a single RTX 4090  eole build_vocab --config $CONFIG --n_sample -1 # --num_threads 4 eole train --config $CONFIG   Note: if you need to perform some visual checks on the &quot;transformed&quot; data, you can enable the dump_samples flag at the build_vocab stage (and specify a smaller -n_sample for efficiency).  Translate test sets with various settings on local GPU and CPUs.  Notes:  the exact model path depends on the config you chose. You can check your logs for the exact path.the &quot;root&quot; model links to the last saved step, but you can choose any step subfolder if needed (e.g. --model_path wmt17_en_de/transformer_big_bpe/step_10000)  eole predict --src wmt17_en_de/test.src.bpe --model_path wmt17_en_de/transformer_big_bpe --beam_size 5 --batch_size 4096 --batch_type tokens --output wmt17_en_de/pred.trg.bpe --gpu 0 sed -re 's/@@( |$)//g' &lt; wmt17_en_de/pred.trg.bpe &gt; wmt17_en_de/pred.trg.tok sacrebleu -tok none wmt17_en_de/test.trg &lt; wmt17_en_de/pred.trg.tok   BLEU scored at 40K, 45K, 50K steps on the test set (Newstest2016)  { &quot;name&quot;: &quot;BLEU&quot;, &quot;score&quot;: 35.4, &quot;signature&quot;: &quot;nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.0.0&quot;, &quot;verbose_score&quot;: &quot;66.2/41.3/28.5/20.3 (BP = 0.998 ratio = 0.998 hyp_len = 64244 ref_len = 64379)&quot;, &quot;nrefs&quot;: &quot;1&quot;, &quot;case&quot;: &quot;mixed&quot;, &quot;eff&quot;: &quot;no&quot;, &quot;tok&quot;: &quot;none&quot;, &quot;smooth&quot;: &quot;exp&quot;, &quot;version&quot;: &quot;2.0.0&quot; } { &quot;name&quot;: &quot;BLEU&quot;, &quot;score&quot;: 35.2, &quot;signature&quot;: &quot;nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.0.0&quot;, &quot;verbose_score&quot;: &quot;65.9/41.0/28.3/20.2 (BP = 1.000 ratio = 1.000 hyp_len = 64357 ref_len = 64379)&quot;, &quot;nrefs&quot;: &quot;1&quot;, &quot;case&quot;: &quot;mixed&quot;, &quot;eff&quot;: &quot;no&quot;, &quot;tok&quot;: &quot;none&quot;, &quot;smooth&quot;: &quot;exp&quot;, &quot;version&quot;: &quot;2.0.0&quot; } { &quot;name&quot;: &quot;BLEU&quot;, &quot;score&quot;: 35.1, &quot;signature&quot;: &quot;nrefs:1|case:mixed|eff:no|tok:none|smooth:exp|version:2.0.0&quot;, &quot;verbose_score&quot;: &quot;66.2/41.2/28.4/20.3 (BP = 0.992 ratio = 0.992 hyp_len = 63885 ref_len = 64379)&quot;, &quot;nrefs&quot;: &quot;1&quot;, &quot;case&quot;: &quot;mixed&quot;, &quot;eff&quot;: &quot;no&quot;, &quot;tok&quot;: &quot;none&quot;, &quot;smooth&quot;: &quot;exp&quot;, &quot;version&quot;: &quot;2.0.0&quot; }  ","version":"Next","tagName":"h3"},{"title":"TowerInstruct (Mistral)","type":0,"sectionRef":"#","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/wmt22_with_TowerInstruct-Mistral).​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipeswmt22_with_towerinstruct-mistral","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#download-and-convert-model","content":" eole convert HF --model_dir Unbabel/TowerInstruct-Mistral-7B-v0.2 --output $EOLE_MODEL_DIR/TowerInstruct-Mistral-7b-v0.2 --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Build the prompt for translation of newstest2022-src.en​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#build-the-prompt-for-translation-of-newstest2022-srcen","content":" python promptize_mistral.py   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"TowerInstruct (Mistral)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-Mistral/#run-inference","content":" eole predict -c tower-inference.yaml -src newstest2022-src-prompt.en -output newstest2022-hyp.de   Then you can score newstest2022-hyp.de against newstest2022-ref.de with a scorer (sacrebleu or comet) or just use cometkiwi for reference-less score. ","version":"Next","tagName":"h3"},{"title":"TowerInstruct-(Llama2)","type":0,"sectionRef":"#","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/wmt22_with_TowerInstruct-llama2).​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipeswmt22_with_towerinstruct-llama2","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#download-and-convert-model","content":" eole convert HF --model_dir Unbabel/TowerInstruct-7B-v0.2 --output $EOLE_MODEL_DIR/TowerInstruct-7b-v0.2 --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Build the prompt for translation of newstest2022-src.en​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#build-the-prompt-for-translation-of-newstest2022-srcen","content":" python promptize_llama2.py   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"TowerInstruct-(Llama2)","url":"/eole/docs/recipes/wmt22_with_TowerInstruct-llama2/#run-inference","content":" eole predict -c tower-inference.yaml -src newstest2022-src-prompt.en -output newstest2022-hyp.de   Then you can score newstest2022-hyp.de against newstest2022-ref.de with a scorer (sacrebleu or comet) or just use cometkiwi for reference-less score. ","version":"Next","tagName":"h3"},{"title":"Bibliography","type":0,"sectionRef":"#","url":"/eole/docs/reference/bibliography","content":"Bibliography Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation By Jointly Learning To Align and Translate.” In ICLR, 1–15.https://doi.org/10.1146/annurev.neuro.26.041002.131047. Bowman, Samuel R., Luke Vilnis, Oriol Vinyals, Andrew M. Dai, Rafal Józefowicz, and Samy Bengio. 2016. “Generating Sentences from a Continuous Space.” In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, CoNLL 2016, Berlin, Germany, August 11-12, 2016, 10–21.http://aclweb.org/anthology/K/K16/K16-1002.pdf. Bulte, Bram, and Arda Tezcan. 2019. “Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 1800–1809. Florence, Italy: Association for Computational Linguistics.https://doi.org/10.18653/v1/P19-1175. Chan, William, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals. 2015. “Listen, Attend and Spell.” CoRR abs/1508.01211.http://arxiv.org/abs/1508.01211. Cho, Kyunghyun, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation.” In Proc of EMNLP. Chopra, Sumit, Michael Auli, Alexander M Rush, and SEAS Harvard. 2016. “Abstractive Sentence Summarization with Attentive Recurrent Neural Networks.” Proceedings of NAACL-HLT16, 93–98. Chung, Junyoung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.” arXiv Preprint arXiv:1412.3555. Crego, Josep, Jungi Kim, and Jean Senellart. 2016. “SYSTRAN’s Pure Neural Machine Translation System.” arXiv Preprint arXiv:1602.06023. Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, et al. 2012. “Large Scale Distributed Deep Networks.” In Advances in Neural Information Processing Systems, 1223–31. Deng, Yuntian, Anssi Kanervisto, and Alexander M. Rush. 2016. “What You Get Is What You See: A Visual Markup Decompiler.” CoRR abs/1609.04938.http://arxiv.org/abs/1609.04938. Dyer, Chris, Jonathan Weese, Hendra Setiawan, Adam Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitkevitch, Phil Blunsom, and Philip Resnik. 2010. “Cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models.” In Proc ACL, 7–12. Association for Computational Linguistics. Garg, Sarthak, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik. 2019. “Jointly Learning to Align and Translate with Transformer Models.” In Conference on Empirical Methods in Natural Language Processing (EMNLP). Hong Kong. https://arxiv.org/abs/1909.02074. Gehring, Jonas, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. “Convolutional Sequence to Sequence Learning.” CoRRabs/1705.03122. http://arxiv.org/abs/1705.03122. Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80. Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, et al. 2007. “Moses: Open Source Toolkit for Statistical Machine Translation.” In Proc ACL, 177–80. Association for Computational Linguistics. Lei, Tao, Yu Zhang, and Yoav Artzi. 2017. “Training RNNs as Fast as CNNs.” CoRR abs/1709.02755. http://arxiv.org/abs/1709.02755. Léonard, Nicholas, Sagar Waghmare, Yang Wang, and Jin-Hwa Kim. 2015. “Rnn : Recurrent Library for Torch.” CoRR abs/1511.07889.http://arxiv.org/abs/1511.07889. Li, Yujia, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. “Gated Graph Sequence Neural Networks.” In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. http://arxiv.org/abs/1511.05493. Liu, Yang, and Mirella Lapata. 2017. “Learning Structured Text Representations.” CoRR abs/1705.09207.http://arxiv.org/abs/1705.09207. Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” InProc of EMNLP. Luong, Minh-Thang, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba. 2015. “Addressing the Rare Word Problem in Neural Machine Translation.” In Proc of ACL. Martins, André F. T., and Ramón Fernández Astudillo. 2016. “From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification.” CoRR abs/1602.02068.http://arxiv.org/abs/1602.02068. Martins, André FT, and Ramón Fernandez Astudillo. 2016. “From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification.”arXiv Preprint arXiv:1602.02068. Neubig, G. 2017. “Neural Machine Translation and Sequence-to-sequence Models: A Tutorial.” ArXiv e-Prints, March. https://arxiv.org/abs/1703.01619. Neubig, Graham. 2013. “Travatar: A Forest-to-String Machine Translation Engine Based on Tree Transducers.” In Proc ACL. Sofia, Bulgaria. See, Abigail, Peter J. Liu, and Christopher D. Manning. 2017. “Get to the Point: Summarization with Pointer-Generator Networks.” CoRRabs/1704.04368. http://arxiv.org/abs/1704.04368. Sennrich, Rico, and Barry Haddow. 2016. “Linguistic Input Features Improve Neural Machine Translation.” arXiv Preprint arXiv:1606.02892. Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2015. “Neural Machine Translation of Rare Words with Subword Units.” CoRR abs/1508.07909.http://arxiv.org/abs/1508.07909. Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In NIPS, 9. http://arxiv.org/abs/1409.3215. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” CoRR abs/1706.03762.http://arxiv.org/abs/1706.03762. Vinyals, Oriol, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. 2016. “Matching Networks for One Shot Learning.” InAdvances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, 3630–38.http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning. Vinyals, Oriol, and Quoc Le. 2015. “A Neural Conversational Model.”arXiv Preprint arXiv:1506.05869. Wang, Qiang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. “Learning Deep Transformer Models for Machine Translation.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 1810–22. Florence, Italy: Association for Computational Linguistics.https://doi.org/10.18653/v1/P19-1176. Wang, Xinyi, Hieu Pham, Zihang Dai, and Graham Neubig. 2018. “SwitchOut: An Efficient Data Augmentation Algorithm for Neural Machine Translation.” CoRR abs/1808.07512. http://arxiv.org/abs/1808.07512. Weston, Jason, Sumit Chopra, and Antoine Bordes. 2014. “Memory Networks.” CoRR abs/1410.3916. http://arxiv.org/abs/1410.3916. Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, et al. 2016. “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation.” arXiv Preprint arXiv:1609.08144. Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015a. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.”CoRR abs/1502.03044. http://arxiv.org/abs/1502.03044. Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015b. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” ICML, February.http://arxiv.org/abs/1502.03044. Yang, Zichao, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. “Hierarchical Attention Networks for Document Classification.” In Proc ACL. Zhang, Biao, Deyi Xiong, and Jinsong Su. 2018. “Accelerating Neural Transformer via an Average Attention Network.” CoRR abs/1805.00631.http://arxiv.org/abs/1805.00631.","keywords":"","version":"Next"},{"title":"Llama3","type":0,"sectionRef":"#","url":"/eole/docs/recipes/wmt22_with_llama3.1/","content":"","keywords":"","version":"Next"},{"title":"NOTETo make your life easier, run these commands from the recipe directory (here recipes/wmt22_with_llama3.1).​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#noteto-make-your-life-easier-run-these-commands-from-the-recipe-directory-here-recipeswmt22_with_llama31","content":" ","version":"Next","tagName":"h2"},{"title":"Retrieve and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#retrieve-and-convert-model","content":" ","version":"Next","tagName":"h2"},{"title":"Set environment variables​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#set-environment-variables","content":" export EOLE_MODEL_DIR=&lt;where_to_store_models&gt; export HF_TOKEN=&lt;your_hf_token&gt;   ","version":"Next","tagName":"h3"},{"title":"Download and convert model​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#download-and-convert-model","content":" eole convert HF --model_dir meta-llama/Meta-Llama-3.1-8B --output $EOLE_MODEL_DIR/llama3.1-8b --token $HF_TOKEN   ","version":"Next","tagName":"h3"},{"title":"Inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#inference","content":" ","version":"Next","tagName":"h2"},{"title":"Build the prompt for translation of newstest2022-src.en​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#build-the-prompt-for-translation-of-newstest2022-srcen","content":" python promptize_llama3.py   ","version":"Next","tagName":"h3"},{"title":"Run inference​","type":1,"pageTitle":"Llama3","url":"/eole/docs/recipes/wmt22_with_llama3.1/#run-inference","content":" eole predict -c llama-instruct-inference.yaml -src newstest2022-src-prompt.en -output newstest2022-hyp.de   Then you can score newstest2022-hyp.de against newstest2022-ref.de with a scorer (sacrebleu or comet) or just use cometkiwi for reference-less score. ","version":"Next","tagName":"h3"},{"title":"Data","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/data","content":"Data","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/","content":"","keywords":"","version":"Next"},{"title":"TODO: add some example yaml configs here​","type":1,"pageTitle":"Configuration","url":"/eole/docs/reference/Config/#todo-add-some-example-yaml-configs-here","content":"","version":"Next","tagName":"h2"},{"title":"Inference","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/inference","content":"Inference","keywords":"","version":"Next"},{"title":"Transforms","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/transforms","content":"Transforms","keywords":"","version":"Next"},{"title":"Training","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/training","content":"Training","keywords":"","version":"Next"},{"title":"Framework","type":0,"sectionRef":"#","url":"/eole/docs/reference/Core API/core","content":"","keywords":"","version":"Next"},{"title":"Model​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#model","content":" ","version":"Next","tagName":"h2"},{"title":"Trainer​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#trainer","content":" ","version":"Next","tagName":"h2"},{"title":"Loss​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#loss","content":" ","version":"Next","tagName":"h2"},{"title":"Optimizer​","type":1,"pageTitle":"Framework","url":"/eole/docs/reference/Core API/core#optimizer","content":"","version":"Next","tagName":"h2"},{"title":"Main Entrypoints","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/run","content":"","keywords":"","version":"Next"},{"title":"Common Base Config​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#common-base-config","content":" ","version":"Next","tagName":"h2"},{"title":"Run Config​","type":1,"pageTitle":"Main Entrypoints","url":"/eole/docs/reference/Config/run#run-config","content":"","version":"Next","tagName":"h2"},{"title":"Data Loaders","type":0,"sectionRef":"#","url":"/eole/docs/reference/Core API/dataloaders","content":"","keywords":"","version":"Next"},{"title":"Data Iterator​","type":1,"pageTitle":"Data Loaders","url":"/eole/docs/reference/Core API/dataloaders#data-iterator","content":" ","version":"Next","tagName":"h2"},{"title":"Dataset​","type":1,"pageTitle":"Data Loaders","url":"/eole/docs/reference/Core API/dataloaders#dataset","content":"","version":"Next","tagName":"h2"},{"title":"Models","type":0,"sectionRef":"#","url":"/eole/docs/reference/Config/models","content":"","keywords":"","version":"Next"},{"title":"Base Configs​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#base-configs","content":" ","version":"Next","tagName":"h2"},{"title":"Transformer​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#transformer","content":" ","version":"Next","tagName":"h2"},{"title":"RNN​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#rnn","content":" ","version":"Next","tagName":"h2"},{"title":"CNN​","type":1,"pageTitle":"Models","url":"/eole/docs/reference/Config/models#cnn","content":"","version":"Next","tagName":"h2"},{"title":"Prediction","type":0,"sectionRef":"#","url":"/eole/docs/reference/Core API/inference","content":"","keywords":"","version":"Next"},{"title":"Predictions​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#predictions","content":" ","version":"Next","tagName":"h2"},{"title":"Predictor Classes​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#predictor-classes","content":" ","version":"Next","tagName":"h2"},{"title":"Decoding Strategies​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#decoding-strategies","content":" ","version":"Next","tagName":"h2"},{"title":"eole.predict.greedy_search.sample_with_temperature(logits, temperature, top_k, top_p)​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#eolepredictgreedy_searchsample_with_temperaturelogits-temperature-top_k-top_p","content":" Select next tokens randomly from the top k possible next tokens.  Samples from a categorical distribution over the top_k words using the category probabilities logits / temperature.  Parameters: logits (FloatTensor) – Shaped (batch_size, vocab_size). These can be logits ((-inf, inf)) or log-probs ((-inf, 0]). (The distribution actually uses the log-probabilitieslogits - logits.logsumexp(-1), which equals the logits if they are log-probabilities summing to 1.)temperature (float) – Used to scale down logits. The higher the value, the more likely it is that a non-max word will be sampled.top_k (int) – This many words could potentially be chosen. The other logits are set to have probability 0.top_p (float) – Keep most likely words until the cumulated probability is greater than p. If used with top_k: both conditions will be applied Returns: topk_ids: Shaped (batch_size, 1). These are the sampled word indices in the output vocab.topk_scores: Shaped (batch_size, 1). These are essentially (logits / temperature)[topk_ids]. Return type:(LongTensor, FloatTensor)  ","version":"Next","tagName":"h3"},{"title":"Eole Core API","type":0,"sectionRef":"#","url":"/eole/docs/reference/index","content":"Eole Core API Note : These sections are built via sphinx and converted into Markdown. Some layout or links might be shaky.","keywords":"","version":"Next"},{"title":"Scoring​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#scoring","content":" ","version":"Next","tagName":"h2"},{"title":"class eole.predict.penalties.PenaltyBuilder(cov_pen, length_pen)​","type":1,"pageTitle":"Prediction","url":"/eole/docs/reference/Core API/inference#class-eolepredictpenaltiespenaltybuildercov_pen-length_pen","content":" Bases: object  Returns the Length and Coverage Penalty function for Beam Search.  Parameters: length_pen (str) – option name of length pencov_pen (str) – option name of cov pen Variables: has_cov_pen (bool) – Whether coverage penalty is None (applying it is a no-op). Note that the converse isn’t true. Setting beta to 0 should force coverage length to be a no-op.has_len_pen (bool) – Whether length penalty is None (applying it is a no-op). Note that the converse isn’t true. Setting alpha to 1 should force length penalty to be a no-op.coverage_penalty (callable [ *[*FloatTensor , float ] , FloatTensor ]) – Calculates the coverage penalty.length_penalty (callable [ *[*int , float ] , float ]) – Calculates the length penalty.  coverage_none(cov, beta=0.0)​  Returns zero as penalty  coverage_summary(cov, beta=0.0)​  Our summary penalty.  coverage_wu(cov, beta=0.0)​  GNMT coverage re-ranking score.  See “Google’s Neural Machine Translation System” [].cov is expected to be sized (*, seq_len), where * is probably batch_size x beam_size but could be several dimensions like (batch_size, beam_size). If cov is attention, then the seq_len axis probably sums to (almost) 1.  length_average(cur_len, alpha=1.0)​  Returns the current sequence length.  length_none(cur_len, alpha=0.0)​  Returns unmodified scores.  length_wu(cur_len, alpha=0.0)​  GNMT length re-ranking score.  See “Google’s Neural Machine Translation System” []. ","version":"Next","tagName":"h3"},{"title":"Modules","type":0,"sectionRef":"#","url":"/eole/docs/reference/Core API/modules","content":"","keywords":"","version":"Next"},{"title":"Embeddings​","type":1,"pageTitle":"Modules","url":"/eole/docs/reference/Core API/modules#embeddings","content":" ","version":"Next","tagName":"h2"},{"title":"Encoders​","type":1,"pageTitle":"Modules","url":"/eole/docs/reference/Core API/modules#encoders","content":" ","version":"Next","tagName":"h2"},{"title":"Decoders​","type":1,"pageTitle":"Modules","url":"/eole/docs/reference/Core API/modules#decoders","content":" ","version":"Next","tagName":"h2"},{"title":"Attention​","type":1,"pageTitle":"Modules","url":"/eole/docs/reference/Core API/modules#attention","content":"","version":"Next","tagName":"h2"}],"options":{"languages":["en"],"id":"default"}}